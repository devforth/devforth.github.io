"use strict";(self.webpackChunkadminforth=self.webpackChunkadminforth||[]).push([[8749],{91895:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"k3s-ec2-deployment","metadata":{"permalink":"/blog/k3s-ec2-deployment","source":"@site/blog/2025-11-04-k3s-ec2-deployment/index.md","title":"IaC Simplified: K3s on EC2 Deployments with Terraform, Helm & Amazon ECR","description":"The ultimate step-by-step guide to cost-effective, build-time-efficient, and easy managable EC2 deployments using K3s, Terraform, Docker, and a Amazon ECR registry.","date":"2025-11-04T00:00:00.000Z","tags":[{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"Amazon Web Services (AWS) is a cloud computing platform that provides a wide range of services for building and deploying applications."},{"inline":false,"label":"Terraform","permalink":"/blog/tags/terraform","description":"Terraform is an open-source infrastructure as code software tool created by HashiCorp that enables users to define and provision data center infrastructure using a declarative configuration language."},{"inline":false,"label":"Helm","permalink":"/blog/tags/helm","description":"The package manager for Kubernetes"},{"inline":false,"label":"k3s","permalink":"/blog/tags/k3s","description":"k3s is a lightweight version of k8s (kubernetes) that is also used for container orchestration but uses fewer resources."}],"readingTime":13.825,"hasTruncateMarker":true,"authors":[{"name":"Kyrylo Doropii","title":"DevOps Engineer of AdminForth","url":"https://github.com/kirilldorr","imageURL":"https://avatars.githubusercontent.com/u/181721742?s=96&v=4","key":"kirilldorr","page":null}],"frontMatter":{"slug":"k3s-ec2-deployment","title":"IaC Simplified: K3s on EC2 Deployments with Terraform, Helm & Amazon ECR","authors":"kirilldorr","tags":["aws","terraform","helm","k3s"],"description":"The ultimate step-by-step guide to cost-effective, build-time-efficient, and easy managable EC2 deployments using K3s, Terraform, Docker, and a Amazon ECR registry."},"unlisted":false,"nextItem":{"title":"How to set up Context7 MCP in Visual Studio Code","permalink":"/blog/context7-setup-vscode"}},"content":"This guide shows how to deploy own Docker apps (with AdminForth as example) to Amazon EC2 instance with K3s and Terraform involving pushing images into Amazon ECR.\\n\\nNeeded resources:\\n- AWS account where we will auto-spawn EC2 instance. We will use `t3a.small` instance (2 vCPUs, 2GB RAM) which costs `~14$` per month in `us-west-2` region (cheapest region). Also it will take `$2` per month for EBS gp2 storage (20GB) for EC2 instance. \\n- Also AWS ECR will charge for `$0.09` per GB of data egress traffic (from EC2 to the internet) - this needed to load docker build cache.\\n\\nThe setup shape:\\n- Build is done using IaaC approach with HashiCorp Terraform, so almoast no manual actions are needed from you. Every resource including EC2 server instance is described in code which is commited to repo.\\n- Docker build process is done on GitHub actions server, so EC2 server is not overloaded with builds\\n- Docker images and build cache are stored on Amazon ECR\\n- Total build time for average commit to AdminForth app (with Vite rebuilds) is around 3 minutes.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Why exactly K3s?\\n\\nPreviously, our blog featured posts about different types of application deployment, but without the use of Kubernetes. This post will look at the cheapest option for deploying an application using k3s (a lightweight version of k8s Kubernetes). This option is more interesting than most of the alternatives, primarily because of its automation and scalability (it is, of course, inferior to the \u201colder\u201d K8s, but it also requires significantly fewer resources). \\n\\n## How we will store containers?\\n\\nThe ECR repository will be used for storage. Since we are working with AWS, this is the most reliable option. The image must be assembled from local files on the machine and then sent to the Amazon server. All instructions for performing these actions will be provided below.\\n\\n# Prerequisites\\n\\nI will assume you run Ubuntu (Native or WSL2).\\n\\nYou should have terraform, here is official repository: \\n\\n```\\nwget -O - https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\\necho \\"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\\nsudo apt update && sudo apt install terraform\\n```\\n\\n\\nAWS CLI:\\n\\n```bash\\nsudo snap install aws-cli --classic\\n```\\n\\nHELM:\\n\\n```bash\\ncurl https://baltocdn.com/helm/signing.asc | sudo tee /etc/apt/trusted.gpg.d/helm.asc\\nsudo apt-get install apt-transport-https --yes\\necho \\"deb https://baltocdn.com/helm/stable/debian/ all main\\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\\nsudo apt-get install helm\\n```\\n\\nAlso you need Doker Daemon running. We recommend Docker Desktop running. ON WSL2 make sure you have Docker Desktop WSL2 integration enabled.\\n\\n```bash\\ndocker version\\n```\\n\\nIt is also worth having `kubectl` locally on your machine for more convenient interaction with nodes and pods, but this is not mandatory.\\n\\n```bash\\ncurl -LO \\"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\\"\\n```\\n\\n# Practice - deploy setup\\n\\nAssume you have your AdminForth project in `myadmin`.\\n\\n## Step 1 - create a SSH keypair\\n\\nMake sure you are still in `deploy` folder, run next command:\\n\\n```bash title=\\"deploy\\"\\nmkdir .keys && ssh-keygen -f .keys/id_rsa -N \\"\\"\\n```\\n\\nNow it should create `deploy/.keys/id_rsa` and `deploy/.keys/id_rsa.pub` files with your SSH keypair. Terraform script will put the public key to the EC2 instance and will use private key to connect to the instance. Also you will be able to use it to connect to the instance manually.\\n\\n## Step 2 - create key pairs\\n\\nI recommend doing this on the official AWS website. Go to EC2>Key pairs>Create key pair, name the new pair k3s-keys, and leave the default settings. Then move the downloaded .pem file to the myadmin/deploy/.keys directory.\\n\\n## Step 3 - .gitignore file\\n\\nCreate `deploy/.gitignore` file with next content:\\n\\n```bash\\n.terraform/\\n.keys/\\n*.tfstate\\n*.tfstate.*\\n*.tfvars\\ntfplan\\n.env.secrets.prod\\nsa-dash.yaml\\nsession-manager-plugin.deb\\n.env.secrets.prod\\n.terraform.lock.hcl\\nk3s.yaml\\n```\\n\\n## Step 4 - file with secrets for local deploy\\n\\nCreate file `deploy/.env.secrets.prod`\\n\\n```bash\\nADMINFORTH_SECRET=<your_secret>\\n```\\n\\n\\n## Step 5 - Terraform folder\\n\\nFirst of all install Terraform as described here [terraform installation](https://developer.hashicorp.com/terraform/install#linux).\\n\\nAfter this create folder ../deploy/terraform\\n\\nCreate file `main.tf` in `deploy` folder:\\n\\n```hcl title=\\"deploy/terraform/main.tf\\"\\n\\nterraform {\\n  required_providers {\\n    aws = {\\n      source  = \\"hashicorp/aws\\"\\n      version = \\"~> 5.0\\"\\n    }\\n  }\\n\\n}\\n\\nlocals {\\n  aws_region           = \\"us-west-2\\"\\n  vpc_cidr             = \\"10.0.0.0/16\\"\\n  subnet_a_cidr        = \\"10.0.10.0/24\\"\\n  subnet_b_cidr        = \\"10.0.11.0/24\\"\\n  az_a                 = \\"us-west-2a\\"\\n  az_b                 = \\"us-west-2b\\"\\n  cluster_name         = \\"myappk3s\\"\\n  app_name             = <your_app_name>\\n  app_source_code_path = \\"../../\\"\\n\\n  app_container_port = 3500\\n  service_port       = 80\\n  admin_secret       = \\"your_secret\\"\\n\\n  ingress_ports = [\\n    { from = 22, to = 22, protocol = \\"tcp\\", desc = \\"SSH\\" },\\n    { from = 80, to = 80, protocol = \\"tcp\\", desc = \\"App HTTP (Traefik)\\" },\\n    { from = 443, to = 443, protocol = \\"tcp\\", desc = \\"App HTTPS (Traefik)\\" },\\n    { from = 6443, to = 6443, protocol = \\"tcp\\", desc = \\"Kubernetes API\\" }\\n  ]\\n}\\n\\nprovider \\"aws\\" {\\n  region = local.aws_region\\n}\\n\\nprovider \\"kubernetes\\" {\\n  config_path = \\"../k3s.yaml\\"\\n}\\n\\ndata \\"aws_ami\\" \\"ubuntu_22_04\\" {\\n  most_recent = true\\n  owners      = [\\"099720109477\\"] # Canonical ubuntu account ID\\n\\n  filter {\\n    name   = \\"name\\"\\n    values = [\\"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\\"]\\n  }\\n}\\n\\nresource \\"aws_instance\\" \\"k3s_server\\" {\\n  instance_type = \\"t3a.small\\"\\n  ami           = data.aws_ami.ubuntu_22_04.id\\n\\n  iam_instance_profile = aws_iam_instance_profile.k3s_instance_profile.name\\n\\n  subnet_id                   = aws_subnet.public_a.id\\n  vpc_security_group_ids      = [aws_security_group.app_sg.id]\\n  associate_public_ip_address = true\\n  key_name                    = \\"k3s-keys\\"\\n\\n  tags = {\\n    Name = local.cluster_name\\n  }\\n\\n  depends_on = [\\n    null_resource.docker_build_and_push\\n  ]\\n\\n  user_data = templatefile(\\"../user_data.sh.tpl\\", {\\n    app_name           = local.app_name\\n    aws_region         = local.aws_region\\n    admin_secret       = local.admin_secret\\n    app_container_port = local.app_container_port\\n    service_port       = local.service_port\\n    ecr_registry_id    = aws_ecr_repository.app_repo.registry_id\\n    ecr_image_full     = \\"${aws_ecr_repository.app_repo.repository_url}:latest\\"\\n    }\\n  )\\n\\n  # prevent accidental termination of ec2 instance and data loss\\n  lifecycle {\\n    #create_before_destroy = true       #uncomment in production\\n    #prevent_destroy       = true       #uncomment in production\\n    ignore_changes = [ami]\\n  }\\n\\n  root_block_device {\\n    volume_size = 10 // Size in GB for root partition\\n    volume_type = \\"gp2\\"\\n\\n    # Even if the instance is terminated, the volume will not be deleted, delete it manually if needed\\n    delete_on_termination = true #change to false in production if data persistence is needed\\n  }\\n\\n}\\n\\nresource \\"null_resource\\" \\"get_kubeconfig\\" {\\n  depends_on = [aws_instance.k3s_server]\\n\\n  provisioner \\"local-exec\\" {\\n    command     = <<-EOT\\n      set -e\\n      for i in {1..15}; do\\n        if nc -z ${aws_instance.k3s_server.public_ip} 22; then\\n          break\\n        fi\\n        sleep 5\\n      done\\n\\n      for i in {1..15}; do\\n        scp -q -o StrictHostKeyChecking=no -i ../.keys/k3s-keys.pem \\\\\\n          ubuntu@${aws_instance.k3s_server.public_dns}:/home/ubuntu/k3s.yaml ../k3s.yaml && {\\n            sleep 5\\n            exit 0\\n          }\\n\\n        echo \\"k3s.yaml not found yet (attempt $i/15), retrying in 10s...\\"\\n        sleep 10\\n      done\\n    EOT\\n    interpreter = [\\"/bin/bash\\", \\"-c\\"]\\n  }\\n}\\n```\\n\\n> \ud83d\udc46 Replace `<your_app_name>` with your app name (no spaces, only underscores or letters)\\n\\nWe will also need a file `container.tf`\\n\\n```hcl title=\\"deploy/terraform/container.tf\\"\\n\\nresource \\"aws_ecr_repository\\" \\"app_repo\\" {\\n  name = local.app_name\\n\\n  image_tag_mutability = \\"MUTABLE\\"\\n  image_scanning_configuration {\\n    scan_on_push = true\\n  }\\n  force_delete = true\\n}\\n\\ndata \\"aws_caller_identity\\" \\"current\\" {}\\n\\nresource \\"null_resource\\" \\"docker_build_and_push\\" {\\n\\n  depends_on = [aws_ecr_repository.app_repo]\\n\\n  provisioner \\"local-exec\\" {\\n    command = <<-EOT\\n      set -e\\n      unset DOCKER_HOST\\n      \\n      REPO_URL=\\"${aws_ecr_repository.app_repo.repository_url}\\"\\n      ACCOUNT_ID=\\"${data.aws_caller_identity.current.account_id}\\"\\n      REGION=\\"${local.aws_region}\\"\\n      \\n      echo \\"LOG: Logging in to ECR...\\"\\n      aws ecr get-login-password --region $${REGION} | docker login --username AWS --password-stdin $${ACCOUNT_ID}.dkr.ecr.$${REGION}.amazonaws.com\\n      \\n      echo \\"LOG: Building Docker image...\\"\\n      docker -H unix:///var/run/docker.sock build --pull -t $${REPO_URL}:latest ${local.app_source_code_path}\\n\\n      echo \\"LOG: Pushing image to ECR...\\"\\n      docker -H unix:///var/run/docker.sock push $${REPO_URL}:latest\\n\\n      echo \\"LOG: Build and push complete.\\"\\n    EOT\\n\\n    interpreter = [\\"/bin/bash\\", \\"-c\\"]\\n  }\\n}\\n```\\n\\nThis file contains a script that builds the Docker image locally. This is done for more flexible deployment. When changing the program code, there is no need to manually update the image on EC2 or in the repository. It is updated automatically with each terraform apply. Below is a table showing the time it takes to build this image from scratch and with minimal changes.\\n\\n| Feature                        | Time      |\\n| ------------------------------ | --------- |\\n| Initial build time\\\\* | 0m45.445s |\\n| Rebuild time (changed index.ts)\\\\* | 0m26.757s |\\n\\n<sub>\\\\* All tests done from local machine (Intel(R) Core(TM) i7 9760H, Docker Desktop/Ubuntu 32 GB RAM, 300Mbps up/down) up to working state</sub>\\n\\nAlso, `resvpc.tf`\\n\\n```hcl title=\\"deploy/terraform/resvpc.tf\\"\\nresource \\"aws_vpc\\" \\"main\\" {\\n  cidr_block = local.vpc_cidr\\n\\n  enable_dns_support   = true\\n  enable_dns_hostnames = true\\n\\n  tags = { Name = \\"main-vpc\\" }\\n}\\n\\nresource \\"aws_subnet\\" \\"public_a\\" {\\n  vpc_id                  = aws_vpc.main.id\\n  cidr_block              = local.subnet_a_cidr\\n  map_public_ip_on_launch = true\\n  availability_zone       = local.az_a\\n  tags = {\\n    Name = \\"public-a\\"\\n  }\\n}\\n\\nresource \\"aws_subnet\\" \\"public_b\\" {\\n  vpc_id                  = aws_vpc.main.id\\n  cidr_block              = local.subnet_b_cidr\\n  map_public_ip_on_launch = true\\n  availability_zone       = local.az_b\\n  tags = {\\n    Name = \\"public-b\\"\\n  }\\n}\\n\\nresource \\"aws_internet_gateway\\" \\"igw\\" {\\n  vpc_id = aws_vpc.main.id\\n  tags   = { Name = \\"main-igw\\" }\\n}\\n\\nresource \\"aws_route_table\\" \\"public_rt\\" {\\n  vpc_id = aws_vpc.main.id\\n  route {\\n    cidr_block = \\"0.0.0.0/0\\"\\n    gateway_id = aws_internet_gateway.igw.id\\n  }\\n  tags = { Name = \\"public-rt\\" }\\n}\\n\\nresource \\"aws_route_table_association\\" \\"public_a_assoc\\" {\\n  subnet_id      = aws_subnet.public_a.id\\n  route_table_id = aws_route_table.public_rt.id\\n}\\n\\nresource \\"aws_route_table_association\\" \\"public_b_assoc\\" {\\n  subnet_id      = aws_subnet.public_b.id\\n  route_table_id = aws_route_table.public_rt.id\\n}\\n\\nresource \\"aws_security_group\\" \\"app_sg\\" {\\n  name   = \\"app-sg-k3s\\"\\n  vpc_id = aws_vpc.main.id\\n\\n  dynamic \\"ingress\\" {\\n    for_each = local.ingress_ports\\n    content {\\n      from_port   = ingress.value.from\\n      to_port     = ingress.value.to\\n      protocol    = ingress.value.protocol\\n      cidr_blocks = [\\"0.0.0.0/0\\"]\\n      description = ingress.value.desc\\n    }\\n  }\\n\\n  egress {\\n    from_port   = 0\\n    to_port     = 0\\n    protocol    = \\"-1\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n}\\n\\nresource \\"aws_iam_role\\" \\"k3s_node_role\\" {\\n  name = \\"k3s-node-role\\"\\n  assume_role_policy = jsonencode({\\n    Version = \\"2012-10-17\\"\\n    Statement = [{\\n      Effect    = \\"Allow\\"\\n      Principal = { Service = \\"ec2.amazonaws.com\\" }\\n      Action    = \\"sts:AssumeRole\\"\\n    }]\\n  })\\n}\\n\\nresource \\"aws_iam_role_policy_attachment\\" \\"ecr_read_only_attach\\" {\\n  policy_arn = \\"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\\"\\n  role       = aws_iam_role.k3s_node_role.name\\n}\\n\\nresource \\"aws_iam_role_policy_attachment\\" \\"ssm_core_policy\\" {\\n  policy_arn = \\"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\\"\\n  role       = aws_iam_role.k3s_node_role.name\\n}\\n\\nresource \\"aws_iam_instance_profile\\" \\"k3s_instance_profile\\" {\\n  name = \\"k3s-instance-profile\\"\\n  role = aws_iam_role.k3s_node_role.name\\n}\\n```\\n\\nAnd `outputs.tf`\\n\\n```hcl title=\\"deploy/terraform/outputs.tf\\"\\n\\noutput \\"app_endpoint\\" {\\n  value = \\"http://${aws_instance.k3s_server.public_dns}\\"\\n}\\n\\noutput \\"kubectl_config_command\\" {\\n  value = \\"scp -i .keys/k3s-keys.pem ubuntu@${aws_instance.k3s_server.public_dns}:/home/ubuntu/k3s.yaml ~/.kube/config-k3s && export KUBECONFIG=~/.kube/config-k3s\\"\\n}\\n\\noutput \\"ssh_connect_command\\" {\\n  value = \\"ssh -i .keys/k3s-keys.pem ubuntu@${aws_instance.k3s_server.public_dns}\\"\\n}\\n\\noutput \\"instance_public_ip\\" {\\n  value = aws_instance.k3s_server.public_ip\\n}\\n\\noutput \\"ecr_repository_url\\" {\\n  value = aws_ecr_repository.app_repo.repository_url\\n}\\n\\nresource \\"null_resource\\" \\"output_to_file\\" {\\n  provisioner \\"local-exec\\" {\\n    command = \\"terraform output -json > ../terraform_outputs.json\\"\\n  }\\n  depends_on = [null_resource.get_kubeconfig]\\n}\\n```\\n\\n### Step 6 - Helm\\n\\n**Helm** is a command-line tool and a set of libraries that helps manage applications in Kubernetes.\\n\\n**Helm Chart (Chart)** is a package containing everything needed to run an application in Kubernetes. It\'s the equivalent of `apt` or `yum` packages in Linux.\\n\\nA chart has this structure:\\n\\n```\\nhelm_charts/\\n\u251c\u2500\u2500 Chart.yaml        # Metadata about the chart (name, version)\\n\u251c\u2500\u2500 values.yaml       # Default values (configuration)\\n\u2514\u2500\u2500 templates/        # Folder with Kubernetes templates (YAML files)\\n    \u251c\u2500\u2500 deployment.yaml\\n    \u251c\u2500\u2500 service.yaml\\n    \u251c\u2500\u2500 ingress.yaml\\n    \u2514\u2500\u2500 ...\\n```\\n\\n### Step 7 - Provider Helm\\n\\nNow we need to create .../deploy/helm and .../deploy/helm/helm_charts folders\\n\\nyou need to create a file `Chart.yaml` in it\\n\\n```yaml title=\\"deploy/helm/helm_charts/Chart.yaml\\"\\napiVersion: v2\\nname: myappk3s\\ndescription: Helm chart for myadmin app\\nversion: 0.1.0\\nappVersion: \\"1.0.0\\"\\n```\\n\\nAnd `values.yaml`\\n\\n```yaml title=\\"deploy/helm/helm_charts/values.yaml\\"\\nappName: myappk3s\\ncontainerPort: 3500\\nservicePort: 80\\nadminSecret: \\"your_secret\\"\\n```\\nAfter this create .../deploy/helm/helm_charts/templates folder\\n\\nAnd create files here:\\n\\n  `deployment.yaml`\\n\\n```yaml title=\\"deploy/helm/helm_charts/templates/deployment.yaml\\"\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: {{ .Values.appName }}-deployment\\n  namespace: {{ .Values.appName }}\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: {{ .Values.appName }}\\n  template:\\n    metadata:\\n      labels:\\n        app: {{ .Values.appName }}\\n    spec:\\n      containers:\\n      - name: {{ .Values.appName }}\\n        image: \\"{{ .Values.ecrImageFull }}\\" \\n        ports:\\n        - containerPort: {{ .Values.containerPort }}\\n        env:\\n        - name: \\"ADMINFORTH_SECRET\\"\\n          value: \\"{{ .Values.adminSecret }}\\"\\n```\\n\\n  `ingress.yaml`\\n\\n```yaml title=\\"deploy/helm/helm_charts/templates/ingress.yaml\\"\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: {{ .Values.appName }}-ingress\\n  namespace: {{ .Values.appName }}\\nspec:\\n  rules:\\n  - http:\\n      paths:\\n      - path: /\\n        pathType: Prefix\\n        backend:\\n          service:\\n            name: {{ .Values.appName }}-service\\n            port:\\n              number: {{ .Values.servicePort }}\\n```\\n\\nAnd `service.yaml`\\n\\n```yaml title=\\"deploy/helm/helm_charts/templates/service.yaml\\"\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: {{ .Values.appName }}-service\\n  namespace: {{ .Values.appName }}\\nspec:\\n  type: ClusterIP\\n  selector:\\n    app: {{ .Values.appName }}\\n  ports:\\n  - port: {{ .Values.servicePort }}\\n    targetPort: {{ .Values.containerPort }}\\n\\n```\\n### Step 8 - Control Helm\\n\\nFor controlling helm we also use terraform, so we need to create one more folder ../deploy/helm/terraform\\nAnd inside it are the files: `main.tf`, `outputs.tf`, `variables.tf`, `terraform.tfvars`.\\n\\n```hcl title=\\"deploy/helm/terraform/main.tf\\"\\nterraform {\\n  required_providers {\\n    aws = {\\n      source  = \\"hashicorp/aws\\"\\n      version = \\"~> 5.0\\"\\n    }\\n    kubernetes = {\\n      source  = \\"hashicorp/kubernetes\\"\\n      version = \\">= 2.0.0, < 3.0.0\\"\\n    }\\n    helm = {\\n      source  = \\"hashicorp/helm\\"\\n      version = \\">= 3.0.0\\"\\n    }\\n  }\\n}\\n\\nprovider \\"kubernetes\\" {\\n  config_path = \\"../../k3s.yaml\\"\\n}\\n\\nprovider \\"helm\\" {\\n  kubernetes = {\\n    config_path = \\"../../k3s.yaml\\"\\n  }\\n}\\n\\ndata \\"local_file\\" \\"config_file\\" {\\n  filename = \\"../../terraform_outputs.json\\"\\n}\\n\\nlocals {\\n  config = jsondecode(data.local_file.config_file.content)\\n}\\n\\nresource \\"kubernetes_namespace\\" \\"myappk3s\\" {\\n  metadata {\\n    name = \\"myappk3s\\"\\n\\n    labels = {\\n      \\"app.kubernetes.io/managed-by\\" = \\"Helm\\"\\n    }\\n\\n    annotations = {\\n      \\"meta.helm.sh/release-name\\"      = \\"myapp\\"\\n      \\"meta.helm.sh/release-namespace\\" = \\"myappk3s\\"\\n    }\\n  }\\n}\\n\\nresource \\"helm_release\\" \\"myapp\\" {\\n  name             = \\"myapp\\"\\n  chart            = \\"../helm_charts\\"\\n  namespace        = kubernetes_namespace.myappk3s.metadata.0.name\\n  create_namespace = false\\n\\n  set = [\\n    {\\n      name  = \\"ecrImageFull\\"\\n      value = local.config.ecr_repository_url.value\\n    },\\n    {\\n      name  = \\"image.tag\\"\\n      value = \\"latest\\"\\n    },\\n    {\\n      name  = \\"adminSecret\\"\\n      value = var.admin_secret\\n    },\\n    {\\n      name  = \\"ingress.enabled\\"\\n      value = \\"true\\"\\n    },\\n    {\\n      name  = \\"ingress.hosts[0].host\\"\\n      value = \\"${local.config.instance_public_ip.value}.nip.io\\"\\n    },\\n    {\\n      name  = \\"ingress.hosts[0].paths[0].path\\"\\n      value = \\"/\\"\\n    },\\n    {\\n      name  = \\"ingress.hosts[0].paths[0].pathType\\"\\n      value = \\"Prefix\\"\\n    },\\n    {\\n      name  = \\"appName\\"\\n      value = var.cluster_name\\n    }\\n  ]\\n  depends_on = [kubernetes_namespace.myappk3s]\\n}\\n```\\n\\n```hcl title=\\"deploy/helm/terraform/variables.tf\\"\\nvariable \\"admin_secret\\" {\\n  description = \\"Admin secret for the application\\"\\n  type        = string\\n}\\n\\nvariable \\"cluster_name\\" {\\n  description = \\"The name of the cluster\\"\\n  type        = string\\n}\\n\\nvariable \\"app_name\\" {\\n  type    = string\\n  default = \\"myapp\\"\\n}\\n```\\n\\n```hcl title=\\"deploy/helm/terraform/outputs.tf\\"\\ndata \\"local_file\\" \\"json_file\\" {\\n  filename = \\"../../terraform_outputs.json\\"\\n}\\n\\nlocals {\\n  duplicated_json = jsondecode(data.local_file.json_file.content)\\n}\\n\\noutput \\"ssh_connect_command\\" {\\n  value = local.duplicated_json[\\"ssh_connect_command\\"][\\"value\\"]\\n}\\n\\noutput \\"kubectl_config_command\\" {\\n  value = local.duplicated_json[\\"kubectl_config_command\\"][\\"value\\"]\\n}\\n\\noutput \\"app_endpoint\\" {\\n  value = local.duplicated_json[\\"app_endpoint\\"][\\"value\\"]\\n}\\n```\\n\\n```hcl title=\\"deploy/helm/terraform/terraform.tfstate\\"\\nadmin_secret = \\"your_secret\\"\\ncluster_name = \\"myappk3s\\"\\napp_name     = \\"myapp\\"\\n```\\n\\n### step 9 - ES2 settings instantiation\\n\\nThe configuration will be performed using a small bash script.\\n\\nFot this step we need to return to ../deploy folder and create `user_data.sh.tpl`\\n\\n```bash title=\\"deploy/userdata.sh.tpl\\"\\n#!/bin/bash -e\\n\\necho \\"LOG update apt...\\"\\nexport DEBIAN_FRONTEND=noninteractive\\napt-get update -y\\n\\necho \\"LOG Installing Docker&AWS CLI...\\"\\napt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release awscli\\n\\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\\n\\necho \\"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\" | tee /etc/apt/sources.list.d/docker.list > /dev/null\\n\\napt-get update -y\\napt-get install -y docker-ce docker-ce-cli containerd.io\\n\\necho \\"LOG Starting Docker...\\"\\nsystemctl start docker\\nsystemctl enable docker\\nusermod -a -G docker ubuntu\\n\\nECR_REGISTRY=\\"${ecr_registry_id}.dkr.ecr.${aws_region}.amazonaws.com\\"\\n\\necho \\"LOG Gettong ETC password...\\"\\nECR_PASSWORD=\\"\\"\\nRETRY_COUNT=0\\nMAX_RETRIES=12 \\n\\nuntil [ $RETRY_COUNT -ge $MAX_RETRIES ]; do\\n  \\n  ECR_PASSWORD=$(aws ecr get-login-password --region ${aws_region} 2>/dev/null)\\n  if [ -n \\"$ECR_PASSWORD\\" ]; then\\n    echo \\"LOG Successfull.\\"\\n    break\\n  fi\\n  RETRY_COUNT=$((RETRY_COUNT+1))\\n  echo \\"LOG Retry (for 5s)...\\"\\n  sleep 5\\ndone\\n\\nif [ -z \\"$ECR_PASSWORD\\" ]; then\\n  echo \\"LOG ERROR: Unable to retrieve ECR password after $MAX_RETRIES attempts.\\"\\n  exit 1\\nfi\\n\\necho $ECR_PASSWORD | docker login --username AWS --password-stdin $ECR_REGISTRY\\n\\nif [ $? -ne 0 ]; then\\n  echo \\"LOG ERROR: Docker login to ECR failed.\\"\\n  exit 1\\nfi\\necho \\"LOG Docker login successful.\\"\\n\\necho \\"LOG Waiting for Docker socket...\\"\\ntimeout 60 sh -c \'until docker info > /dev/null 2>&1; do echo \\"LOG Waiting for Docker socket...\\"; sleep 3; done\'\\n\\nif ! docker info > /dev/null 2>&1; then\\n  echo \\"LOG ERROR: Docker socket not available after timeout.\\"\\n  exit 1\\nfi\\n\\necho \\"LOG Turning off ufw...\\"\\nufw disable || echo \\"LOG ufw not installed, skipping disable.\\"\\n\\necho \\"LOG Retrieving public IP...\\"\\nPUBLIC_IP=$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4)\\n\\necho \\"LOG Installing K3s \u0437 --tls-san=$${PUBLIC_IP}...\\"\\ncurl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\\"--docker --tls-san $${PUBLIC_IP}\\" sh -\\n\\necho \\"LOG Waiting for k3s.yaml to be created...\\"\\nSTART_TIME=$(date +%s)\\nuntil [ -f /etc/rancher/k3s/k3s.yaml ]; do\\n  CURRENT_TIME=$(date +%s)\\n  if (( CURRENT_TIME - START_TIME > 300 )); then\\n    echo \\"LOG ERROR: Timeout waiting for k3s.yaml.\\"\\n    echo \\"LOG k3s.yaml status check:\\"\\n    systemctl status k3s.service || systemctl status k3s-server.service || echo \\"LOG Failed to get k3s service status\\"\\n    echo \\"LOG Last 50 lines of k3s logs:\\"\\n    journalctl -u k3s.service -n 50 --no-pager || journalctl -u k3s-server.service -n 50 --no-pager || echo \\"LOG Failed to get k3s logs\\"\\n    exit 1\\n  fi\\n  echo \\"LOG Waiting for k3s.yaml... (passed $(( CURRENT_TIME - START_TIME )) seconds)\\"\\n  sleep 5\\ndone\\n\\necho \\"LOG k3s.yaml found.\\"\\n\\necho \\"LOG Updating k3s.yaml with public IP...\\"\\nsed -i \\"s/127.0.0.1/$${PUBLIC_IP}/g\\" /etc/rancher/k3s/k3s.yaml\\n\\necho \\"LOG Copying k3s.yaml to /home/ubuntu/k3s.yaml...\\"\\ncp /etc/rancher/k3s/k3s.yaml /home/ubuntu/k3s.yaml\\nchown ubuntu:ubuntu /home/ubuntu/k3s.yaml\\n```\\n### Step 10 - Configure AWS Profile\\n\\nOpen or create file `~/.aws/credentials` and add (if not already there):\\n\\n```ini\\n[myaws]\\naws_access_key_id = <your_access_key>\\naws_secret_access_key = <your_secret_key>\\n```\\n\\n### Step 11 - Run deployment\\n\\nAll deployment-related actions are automated and recorded in the script in `user_data.sh.tpl`, so no additional actions are required. To deploy the application, you only need to enter a few commands listed below and wait a few minutes. After that, you will be able to connect to the web application using the link you will receive in `terraform_output`. Next, if you wish, you can add GitHub Actions. To do this, follow the instructions in [our other post](https://adminforth.dev/blog/compose-aws-ec2-ecr-terraform-github-actions/#chellenges-when-you-build-on-ci).\\n\\n  In ../deploy/terraform folder\\n\\n```bash \\nterraform init\\n```\\n\\n```bash\\nterraform apply -auto-approve\\n```\\n\\nWait for terraform complete the creation of all resources and change directory\\n\\n```bash\\ncd ../../helm/terraform\\n```\\nAnd repeat the steps in this directory\\n\\n```bash \\nterraform init\\n```\\n\\n```bash\\nterraform apply -auto-approve\\n```\\n\\n### All done!\\n\\nYour application is now deployed on Amazon EC2 and available on the Internet."},{"id":"context7-setup-vscode","metadata":{"permalink":"/blog/context7-setup-vscode","source":"@site/blog/2025-10-21-context7-setup/index.md","title":"How to set up Context7 MCP in Visual Studio Code","description":"Context7 MCP installation guide","date":"2025-10-21T00:00:00.000Z","tags":[{"inline":true,"label":"context7","permalink":"/blog/tags/context-7"},{"inline":true,"label":"MCP","permalink":"/blog/tags/mcp"}],"readingTime":1.595,"hasTruncateMarker":true,"authors":[{"name":"Yaroslav Pechorkin","title":"Developer of AdminForth","url":"https://github.com/yaroslav8765","imageURL":"https://avatars.githubusercontent.com/u/189334989?v=4","key":"ypechorkin","page":null}],"frontMatter":{"slug":"context7-setup-vscode","title":"How to set up Context7 MCP in Visual Studio Code","authors":"ypechorkin","tags":["context7","MCP"],"description":"Context7 MCP installation guide"},"unlisted":false,"prevItem":{"title":"IaC Simplified: K3s on EC2 Deployments with Terraform, Helm & Amazon ECR","permalink":"/blog/k3s-ec2-deployment"},"nextItem":{"title":"How to translate dynamic strings in AdminForth API","permalink":"/blog/dynamic-strings-translation"}},"content":"This guide shows how you can set up Context7 MCP in your Visual Studio Code IDE (VS Code)\\n\x3c!-- truncate --\x3e\\n\\n### Preparation\\n\\nFirst of all, you\'ll need to update VS Code to version 1.105.1 or higher. To check your version, go to `Help -> About`.\\n\\nIf you used a `.deb` file to install VS Code, just run these commands in the terminal to install updates:\\n```bash\\nsudo apt update \\nsudo apt install code\\n```\\n\\nBefore opening VS Code, we need to get the Context7 API key. For this, go to https://context7.com/dashboard, create a new account, and copy the API key.\\n\\n### Setup\\n \\nNow we can proceed with the setup:\\n1) After opening VS Code, go to the extensions tab, where you\'ll see the new `MCP SERVERS` tab. Click on `Enable MCP Servers Marketplace`.\\n![](image_1.png)\\n\\n2) Find Context7 in the list and press `Install`:\\n![](image_2.png)\\n\\n3) Go to `Show Configuration JSON`:\\n![](image_3.png)\\n\\n4) Click `Edit`:\\n![](image_4.png)\\n\\n5) Insert your API key:\\n![](image_5.png)\\n\\nThe installation is complete. You can now use it.\\n\\n### Example: Installation of the Bulk-ai-flow Plugin\\n\\nHere is an example prompt you can use to add the adminforth bulk-ai-flow plugin:\\n\\n**Prompt:**  \\n`Add adminforth bulk-ai-flow plugin to this file using Context7 MCP`\\n\\n**Generated Code:**  \\n![](image_6.png)\\n\\n**Result:**  \\n![](image_7.png)\\n\\nAs we can see, the generation works really well.\\n\\n### Tips\\nIf you don\u2019t want to add `use context7` to every prompt, you can [define a simple rule in your MCP client\'s rule section](https://github.com/upstash/context7?tab=readme-ov-file#-tips).\\n\\nIf you\'re using github copilot, you can:\\n1) In the root of your repository, create the .github directory if it does not already exist.\\n2) create a file named `.github/copilot-instructions.md`\\n3) Inside of new file add:\\n\\n```txt\\nAlways use context7 when I need code generation, setup or configuration steps, or\\nlibrary/API documentation. This means you should automatically use the Context7 MCP\\ntools to resolve library id and get library docs without me having to explicitly ask.\\n```"},{"id":"dynamic-strings-translation","metadata":{"permalink":"/blog/dynamic-strings-translation","source":"@site/blog/2025-04-10-how-to-translate-dynamic-strings/index.md","title":"How to translate dynamic strings in AdminForth API","description":"Simple example of how to translate dynamic strings from database in AdminForth API","date":"2025-04-10T00:00:00.000Z","tags":[{"inline":false,"label":"Keycloak","permalink":"/blog/tags/keycloak","description":"Keycloak is an open-source identity and access management solution for modern applications and services."},{"inline":false,"label":"Auth","permalink":"/blog/tags/auth","description":"Authentication is the process of verifying the identity of a user or system. Authentication is a critical component of security in software applications and systems."}],"readingTime":4.805,"hasTruncateMarker":true,"authors":[{"name":"Ivan Borshchov","title":"Maintainer of AdminForth","url":"https://github.com/ivictbor","imageURL":"https://avatars.githubusercontent.com/u/1838656?v=4","key":"ivanb","page":null}],"frontMatter":{"slug":"dynamic-strings-translation","title":"How to translate dynamic strings in AdminForth API","authors":"ivanb","tags":["keycloak","authentication"],"description":"Simple example of how to translate dynamic strings from database in AdminForth API"},"unlisted":false,"prevItem":{"title":"How to set up Context7 MCP in Visual Studio Code","permalink":"/blog/context7-setup-vscode"},"nextItem":{"title":"Setup AdminForth Authorization via Keycloak","permalink":"/blog/keycloak-setup-example"}},"content":"When you are using [AdminForth i18n plugin for external Apps translation](https://adminforth.dev/docs/tutorial/Plugins/i18n/#translating-external-application) you might face a case when you need to translate some data stored in your database which potentially can be changed in future.\\n\x3c!-- truncate --\x3e\\n\\nLet\'s consider simple example where we have a Page resource \\n\\n```ts\\nimport { AdminForthResourceInput } from \\"adminforth\\";\\n\\nexport default {\\n  dataSource: \\"maindb\\",\\n  table: \\"pages\\",\\n  resourceId: \\"pages\\",\\n  label: \\"Pages\\",\\n  columns: [\\n    {\\n      name: \\"url\\",\\n      primaryKey: true,\\n      showIn: { all: false },\\n    },\\n    {\\n      name: \\"meta_title\\",\\n      label: \\"Meta Title\\",\\n      type: \\"string\\",\\n      showIn: { all: true },\\n    },\\n    {\\n      name: \\"meta_desc\\",\\n      label: \\"Meta Description\\",\\n      type: \\"string\\",\\n      showIn: { all: true },\\n    },\\n  ]\\n} as AdminForthResourceInput;\\n```\\n\\nYou might have this page and return it in your API for nuxt:\\n\\n```ts\\napp.get(`${admin.config.baseUrl}/api/get_page`,\\n    async (req:any, res: Response): Promise<void> => {\\n      const pageUrl = req.query.pageUrl;\\n      if (!pageUrl) {\\n        res.status(400).json({ error: \\"pageUrl is required\\" });\\n        return;\\n      }\\n      const page = await admin.resource(\\"pages\\").get([Filters.EQ(\\"url\\", pageUrl)]);\\n      if (!page) {\\n        res.status(404).json({ error: `Page not found ${pageUrl}` });\\n        return;\\n      }\\n      res.json({\\n        meta_title: page.meta_title,\\n        meta_desc: page.meta_desc,\\n      });\\n    }\\n  ) \\n);\\n```\\n\\nNow you want to translate page meta title and meta description. You can do this by using `i18n` plugin for AdminForth.\\n\\n```ts\\nimport { AdminForth } from \\"adminforth\\";\\n\\nexport const SEO_PAGE_CATEGORY = \\"seo_page_config\\";\\n\\napp.get(`${admin.config.baseUrl}/api/get_page`,\\\\\\n//diff-add\\n admin.express.translatable(\\n    async (req:any, res: Response): Promise<void> => {\\n      const pageUrl = req.query.pageUrl;\\n      if (!pageUrl) {\\n        res.status(400).json({ error: \\"pageUrl is required\\" });\\n        return;\\n      }\\n      const page = await admin.resource(\\"pages\\").get([Filters.EQ(\\"url\\", pageUrl)]);\\n      if (!page) {\\n        res.status(404).json({ error: `Page not found ${pageUrl}` });\\n        return;\\n      }\\n\\n//diff-add\\n      const translateKeys = [ \\"meta_title\\", \\"meta_desc\\", ];\\n//diff-add\\n      const [meta_title, meta_desc] = \\n//diff-add\\n          await Promise.all(translateKeys.map((key: string) => req.tr(page[key], SEO_PAGE_CATEGORY)));\\n        \\n\\n      res.json({\\n  //diff-remove\\n        meta_title: page.meta_title,\\n  //diff-add\\n        meta_title,\\n  //diff-remove\\n        meta_desc: page.meta_desc,\\n  //diff-add\\n        meta_desc,\\n      });\\n    }\\n//diff-add\\n  ) \\n);\\n```\\n\\nLooks straightforward, but here are 2 issues:\\n\\n# Issue one - you need to call `req.tr` for each string before translating it in Admin\\n\\nSince translation strings are created only when first time you call `req.tr` function, you need to ensure you will call this API for all your pages (in all envs e.g. local, dev, staging, prod ), this might be not convinient - you have to call this API, then go to translation page and translate it with bulk action LLM or manually, but you might forget one page or so.\\n\\nTo fix this we suggest next, go to Page resource and add next function on top level:\\n\\n```ts\\n//diff-remove\\nimport { AdminForthResourceInput } from \\"adminforth\\";\\n//diff-add\\nimport AdminForth, { AdminForthResourceInput, Filters, IAdminForth } from \\"adminforth\\";\\n//diff-add\\nimport I18nPlugin from \\"@adminforth/i18n/index.js\\";\\n\\n//diff-add\\nimport { SEO_PAGE_CATEGORY } from \\"../api.ts\\";\\n\\n//diff-add\\nexport async function feedAllPageTranslations(adminforth: IAdminForth) {\\n//diff-add\\n  const i18nPlugin = adminforth.getPluginByClassName<I18nPlugin>(\'I18nPlugin\');\\n//diff-add\\n  const pages = await adminforth.resource(\'pages\').list([]);\\n//diff-add\\n  await Promise.all(\\n    //diff-add\\n    pages.map(async (page: any) => {\\n  //diff-add\\n      await Promise.all(\\n  //diff-add\\n      [\'meta_title\', \'meta_desc\'].map(async (key) => {\\n  //diff-add\\n        if (page[key]) {\\n  //diff-add\\n          await i18nPlugin.feedCategoryTranslations(\\n    //diff-add\\n            [{ en_string: page[key], source: `pages.${page.url}.${key}` }], SEO_PAGE_CATEGORY\\n    //diff-add\\n          );\\n  //diff-add\\n        }\\n//diff-add\\n      })\\n//diff-add\\n    );\\n//diff-add\\n  })\\n//diff-add\\n  );\\n//diff-add\\n}\\n\\nexport default {\\n  dataSource: \\"maindb\\",\\n  table: \\"pages\\",\\n  resourceId: \\"pages\\",\\n  ...\\n```\\n\\nThis function will iterate all pages and call `feedCategoryTranslations` function for each page and each key. This will create translation strings in your database for each page and each key. If translation objects already exist, it will skip them (**will not** create duplicates and **will not** overwrite translations).\\n\\nNow we need to call this function in hooks:\\n\\n```ts\\nexport default {\\n  dataSource: \\"maindb\\",\\n  table: \\"pages\\",\\n  resourceId: \\"pages\\",\\n  label: \\"Pages\\",\\n//diff-add\\n  hooks: {\\n//diff-add\\n    create: {\\n//diff-add\\n      afterSave: async ({ record, adminforth }: { record: Record<string, string>, adminforth: IAdminForth }) => {\\n//diff-add\\n        feedAllPageTranslations(adminforth);\\n//diff-add\\n        return { ok: true };\\n//diff-add\\n      },\\n//diff-add\\n    },\\n//diff-add\\n    edit: {\\n//diff-add\\n      afterSave: async ({ oldRecord, updates, adminforth }: { oldRecord: Record<string, string>, updates: Record<string, string>, adminforth: IAdminForth }) => {\\n//diff-add\\n        feedAllPageTranslations(adminforth);\\n//diff-add\\n        return { ok: true }; \\n//diff-add\\n      },\\n//diff-add\\n    },\\n  },  \\n  columns: [\\n    ...\\n```\\n\\n> Please note that we run this function without await, so it will not block your API. SO function will be called in background when hook will already return and user will not wait for it.\\n\\n\\nNow every time you will create or edit page, it will call `feedAllPageTranslations` function and create translation strings for each page and each key. \\nYou can also import this function into index script and run after database discover, if you already have pages in your database and you want to create translation strings for them even without clicking on create or edit button.\\n\\n# Issue 2 - after modification of page attributes old translation strings will not be removed\\n\\nYou can mitigate this by adding couple of lines into edit hook:\\n\\n```ts\\n    edit: {\\n      afterSave: async ({ oldRecord, updates, adminforth }: { oldRecord: Record<string, string>, updates: Record<string, string>, adminforth: IAdminForth }) => {\\n       //diff-add\\n         if (Object.keys(updates).length) {\\n       //diff-add\\n          // find old strings which were edited and which are not used anymore\\n       //diff-add\\n        const oldStrings = await adminforth.resource(\'translations\').list([\\n       //diff-add\\n            Filters.AND(\\n       //diff-add\\n              Filters.EQ(\'category\', \'seo_page_config\'),\\n       //diff-add\\n              Filters.IN(\'en_string\', Object.keys(updates).map((key: string) => oldRecord[key]))\\n       //diff-add\\n            )\\n       //diff-add\\n          ]);\\n       //diff-add\\n          // delete them\\n       //diff-add\\n          await Promise.all(\\n       //diff-add\\n            oldStrings.map((oldString: any) => {\\n       //diff-add\\n              return adminforth.resource(\'translations\').delete(oldString.id);\\n       //diff-add\\n            })\\n       //diff-add\\n          );\\n       //diff-add\\n        }\\n        feedAllPageTranslations(adminforth);\\n        return { ok: true }; \\n      },\\n    },\\n```\\n\\nThis will delete all old translation strings which are not used anymore.\\n\\nIf your have ability to delete pages, you can also add delete hook, you can do this as a homework.\\n\\n\\n# Conclusion\\n\\nIn this article we have shown how to translate dynamic strings in AdminForth API. We have also shown how to create translation strings for each page and each key in your database.\\nWe have also shown how to delete old translation strings which are not used anymore. This will help you to keep your translation strings clean and up to date."},{"id":"keycloak-setup-example","metadata":{"permalink":"/blog/keycloak-setup-example","source":"@site/blog/2025-03-25-keycloack/index.md","title":"Setup AdminForth Authorization via Keycloak","description":"The ultimate guide to setting up AdminForth authorization via Keycloak","date":"2025-03-25T00:00:00.000Z","tags":[{"inline":false,"label":"Keycloak","permalink":"/blog/tags/keycloak","description":"Keycloak is an open-source identity and access management solution for modern applications and services."},{"inline":false,"label":"Auth","permalink":"/blog/tags/auth","description":"Authentication is the process of verifying the identity of a user or system. Authentication is a critical component of security in software applications and systems."}],"readingTime":1.995,"hasTruncateMarker":true,"authors":[{"name":"Maksym Pipkun","title":"Developer of AdminForth","url":"https://github.com/NoOne7135","imageURL":"https://avatars.githubusercontent.com/u/120279695?v=4","key":"mpipkun","page":null}],"frontMatter":{"slug":"keycloak-setup-example","title":"Setup AdminForth Authorization via Keycloak","authors":"mpipkun","tags":["keycloak","authentication"],"description":"The ultimate guide to setting up AdminForth authorization via Keycloak"},"unlisted":false,"prevItem":{"title":"How to translate dynamic strings in AdminForth API","permalink":"/blog/dynamic-strings-translation"},"nextItem":{"title":"Amazon EC2 Deployments with GitHub Actions, Terraform, Docker & Self-hosted Registry","permalink":"/blog/compose-ec2-deployment-github-actions-registry"}},"content":"Keycloak is an open-source identity and access management solution that provides authentication and authorization services. It can be used to secure applications and services by managing user identities, roles, and permissions.\\n\\nIn this guide, we will walk you through the process of setting up AdminForth authorization via Keycloak. Most important we will show you how to set up Keycloak in a Docker container and configure it to work with AdminForth.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites\\n\\n- Docker installed on your machine\\n- Basic knowledge of Docker and Docker Compose\\n\\n## Step 1: Create a Docker Compose File\\n\\nCreate a `docker-compose.yml` file in your project directory. This file will define the Keycloak service and its configuration.\\n\\n```yaml\\nservices:\\n  pg:\\n    image: postgres:latest\\n    environment:\\n      POSTGRES_USER: demo\\n      POSTGRES_PASSWORD: demo\\n      POSTGRES_DB: demo\\n    ports:\\n      - \\"5432:5432\\"\\n    volumes:\\n      - pg-data:/var/lib/postgresql/data\\n  keycloak:\\n    image: quay.io/keycloak/keycloak:latest\\n    command: start-dev\\n    environment:\\n      - KEYCLOAK_ADMIN=admin\\n      - KEYCLOAK_ADMIN_PASSWORD=admin\\n      - DB_VENDOR=postgres\\n      - DB_ADDR=pg\\n      - DB_DATABASE=demo\\n      - DB_USER=demo\\n      - DB_PASSWORD=demo\\n    ports:\\n      - \\"8080:8080\\"\\n    depends_on:\\n      - pg\\n    volumes:\\n      - keycloak-data:/opt/keycloak/data\\n\\nvolumes:\\n  keycloak-data:\\n```\\n\\nRun service:\\n\\n```bash\\ndocker compose -p af-dev-demo up -d --build --remove-orphans --wait\\n```\\n\\n## Step 2: Singn in to Keycloak and Create a Keycloak Realm\\n\\n1. Open the Keycloak UI at `http://localhost:8080/`.\\n2. Sign in with the credentials `admin` and `admin`.\\n\\n![alt text](image-3.png)\\n\\n3. Select the \\"Realms\\" tab and click `Create Realm`.\\n\\n![alt text](image-4.png)\\n\\n4. Enter a name for your realm and click `Create`.\\n\\n![alt text](image-5.png)\\n\\n## Step 3: Create a Keycloak Client\\n\\n1. Go to `Clients` tab and click `Create Client`.\\n\\n![alt text](image-10.png)\\n\\n2. Choose `OpenID Connect`, enter a client ID for your client and click `Next`.\\n\\n![alt text](image.png)\\n\\n3. Swith `Client authentication` to `On` and click `Next`.\\n\\n![alt text](image-1.png)\\n\\n4. Enter a `Valid redirect URI` and click `Save`.\\n\\n![alt text](image-9.png)\\n\\n5. In the `Client details` go to `Credentials` tab and copy the `Client secret`.\\n\\n![alt text](image-2.png)\\n\\n6. Add the credentials to your `.env` file:\\n\\n```bash\\nKEYCLOAK_CLIENT_ID=your_keycloak_client_id\\nKEYCLOAK_CLIENT_SECRET=your_keycloak_client_secret\\nKEYCLOAK_REALM=your_keycloak_realm\\nKEYCLOAK_URL=http://localhost:8080\\n```\\n\\n## Step 4: Create a Keycloak User\\n\\n1. Go to `Users` tab and click `Create new user`.\\n\\n![alt text](image-6.png)\\n\\n2. Enter a `Username`, `Email`, `First name` and `Last name` and click `Create`.\\n\\n![alt text](image-7.png)\\n\\n3. In the  `User details` go to `Credentials` tab and click `Set password`.\\n\\n![alt text](image-12.png)\\n\\n4. Enter a `Password`, turn `Temporary` to `Off` and click `Save`.\\n\\n![alt text](image-8.png)\\n\\nFinally, you can sign in to AdminForth with your Keycloak credentials. (if user with the same email exists in AdminForth)"},{"id":"compose-ec2-deployment-github-actions-registry","metadata":{"permalink":"/blog/compose-ec2-deployment-github-actions-registry","source":"@site/blog/2025-02-19-compose-ec2-deployment-github-actions-registry/index.md","title":"Amazon EC2 Deployments with GitHub Actions, Terraform, Docker & Self-hosted Registry","description":"The ultimate step-by-step guide to cost-effective, build-time-efficient, and easy managable EC2 deployments using GitHub Actions, Terraform, Docker, and a self-hosted registry.","date":"2025-02-19T00:00:00.000Z","tags":[{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"Amazon Web Services (AWS) is a cloud computing platform that provides a wide range of services for building and deploying applications."},{"inline":false,"label":"Terraform","permalink":"/blog/tags/terraform","description":"Terraform is an open-source infrastructure as code software tool created by HashiCorp that enables users to define and provision data center infrastructure using a declarative configuration language."},{"inline":false,"label":"GitHub Actions","permalink":"/blog/tags/github-actions","description":"GitHub Actions is a continuous integration and continuous deployment (CI/CD) service provided by GitHub that allows you to automate your software development workflows."}],"readingTime":15.745,"hasTruncateMarker":true,"authors":[{"name":"Ivan Borshchov","title":"Maintainer of AdminForth","url":"https://github.com/ivictbor","imageURL":"https://avatars.githubusercontent.com/u/1838656?v=4","key":"ivanb","page":null}],"frontMatter":{"slug":"compose-ec2-deployment-github-actions-registry","title":"Amazon EC2 Deployments with GitHub Actions, Terraform, Docker & Self-hosted Registry","authors":"ivanb","tags":["aws","terraform","github-actions"],"description":"The ultimate step-by-step guide to cost-effective, build-time-efficient, and easy managable EC2 deployments using GitHub Actions, Terraform, Docker, and a self-hosted registry.","image":"/ogs/ga-tf-aws.jpg"},"unlisted":false,"prevItem":{"title":"Setup AdminForth Authorization via Keycloak","permalink":"/blog/keycloak-setup-example"},"nextItem":{"title":"IaaC Simplified: Amazon EC2 Deployments with GitHub Actions, Terraform, Docker & Amazon ECR","permalink":"/blog/compose-aws-ec2-ecr-terraform-github-actions"}},"content":"![alt text](ga-tf-aws.jpg)\\n\\n\\nThis guide is a hackers extended addition of [Deploying AdminForth to EC2 with Amazon ECR](/blog/compose-aws-ec2-ecr-terraform-github-actions/). The key difference in this post that we will not use Amazon ECR but self-host registry on EC2 itself. Automatically from terraform. And will see whether we will win something in terms of build time.\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## Costs for Amazon ECR vs consts for self-hosted registry on EC2\\n\\nMost of AWS services are formed from EC2 prices plus some extra overhead for own cost. In same way, Amazon ECR pricing is pretty same.\\n\\n| Feature | Amazon ECR | Self-hosted registry on EC2 |\\n| --- | --- | --- |\\n| Storage | $0.10 per GB/month | $0.10 per GB/month for gp2 EBS volume |\\n| Data transfer for egress | $0.09 per GB | $0.09 per GB |\\n\\nSo as you can see there is still no difference in terms of cost. However the approach in this system allows to replace Amazon EC2 with any other cloud provider which does not charge for egress traffic.\\n\\n\\n# Bechnmarking build time\\n\\nWhen I implmented this solution, I was curious whether it will be faster to build images on EC2 or on CI. So I did a little bit of testing.\\nFirst I used results form [deploying AdminForth to EC2 with Terraform without registry](/blog/compose-ec2-deployment-github-actions/) where we built images on EC2. Then I did the same test but with self-hosted registry on EC2 and compared to [deploying AdminForth to EC2 with Amazon ECR](/blog/compose-aws-ec2-ecr-terraform-github-actions/) where we built images on CI and pushed to Amazon ECR.\\n\\n| Feature | Without Registry (build directly on EC2) | With self-hosted registry | With Amazon ECR |\\n| --- | --- | --- | --- |\\n| Initial build time\\\\* |  3m 13.541s | 2m 48.412s | 3m 54s |\\n| Rebuild time (changed `index.ts`)\\\\* | 0m 51.653s | 0m42.131s | 0m 54.120s | \\n\\n<sub>\\\\* All tests done from local machine (Intel(R) Core(TM) Ultra 9 185H, Docker Desktop/WSL2 64 GB RAM, 300Mbps up/down) up to working state</sub>\\n\\nSo it indeed own self-hosted registry is faster then ECR and overall build time of pure AdminForth is faster then building on EC2. When ECR is slower then self-hosted registry, it is because of network speed.\\n\\n\\n# Chellenges when you build on CI\\n\\n\\n# Registry authorization and traffic encryption\\n\\nHosting custom CNCF registry, from other hand is a security responsibility. \\n\\nIf you don\'t protect it right, someone will be able to push any image to your registry and then pull it to your EC2 instance. This is a big security issue, so we have to protect our registry. \\n\\nFirst of all we need to set some authorization to our registry so everyone who will push/pull images will be authorized. Here we have 2 options: HTTP basic auth and Client certificate auth. We will use first one as it is easier to setup. We will generate basic login and password automatically in terraform so no extra actions are needed from you.\\n\\nBut this is not enough. Basic auth is not encrypted, so someone can perform MITM attack and get your credentials. So we need to encrypt traffic between CI and registry. We can do it by using TLS certificates. So we will generate self-signed TLS certificates, and attach them to our registry.\\n\\nThough the challenge is that we need to provide CA certificate to every daemon which will work with our registry. So we need to provide CA certificate to buildx daemon on CI, also if we want to do it from local machine, we need to provide CA certificate to local docker daemon.\\n\\n# Practice - deploy setup\\n\\nAssume you have your AdminForth project in `myadmin`.\\n\\n\\n## Step 1 - Dockerfile and .dockerignore\\n\\n\\nThis guide assumes you have created your AdminForth application with latest version of `adminforth create-app` command. \\nThis command already creates a `Dockerfile` and `.dockerignore` for you, so you can use them as is.\\n\\n\\n## Step 2 - compose.yml\\n\\ncreate folder `deploy` and create file `compose.yml` inside:\\n\\n```yml title=\\"deploy/compose.yml\\"\\nservices:\\n  traefik:\\n    image: \\"traefik:v2.5\\"\\n    command:\\n      - \\"--api.insecure=true\\"\\n      - \\"--providers.docker=true\\"\\n      - \\"--entrypoints.web.address=:80\\"\\n    ports:\\n      - \\"80:80\\"\\n    volumes:\\n      - \\"/var/run/docker.sock:/var/run/docker.sock:ro\\"\\n\\n  myadmin:\\n    image: localhost:5000/myadmin:latest\\n    build:\\n      context: ../adminforth-app\\n      tags:\\n        - localhost:5000/myadmin:latest\\n      cache_from:\\n        - type=registry,ref=localhost:5000/myadmin:cache\\n      cache_to:\\n        - type=registry,ref=localhost:5000/myadmin:cache,mode=max,compression=zstd,image-manifest=true,oci-mediatypes=true\\n      \\n    pull_policy: always\\n    restart: always\\n    env_file:\\n      - .env.secrets.prod\\n\\n    volumes:\\n      - myadmin-db:/code/db\\n    labels:\\n      - \\"traefik.enable=true\\"\\n      - \\"traefik.http.routers.myadmin.rule=PathPrefix(`/`)\\"\\n      - \\"traefik.http.services.myadmin.loadbalancer.server.port=3500\\"\\n      - \\"traefik.http.routers.myadmin.priority=2\\"\\n\\nvolumes:\\n  myadmin-db:\\n```\\n\\n## Step 3 - create a SSH keypair\\n\\nMake sure you are still in `deploy` folder, run next command:\\n\\n```bash title=\\"deploy\\"\\nmkdir .keys && ssh-keygen -f .keys/id_rsa -N \\"\\"\\n```\\n\\nNow it should create `deploy/.keys/id_rsa` and `deploy/.keys/id_rsa.pub` files with your SSH keypair. Terraform script will put the public key to the EC2 instance and will use private key to connect to the instance. Also you will be able to use it to connect to the instance manually.\\n\\n## Step 4 - create TLS certificates to encrypt traffic between CI and registry\\n\\nMake sure you are still in `deploy` folder, run next command:\\n\\nRun next command to create TLS certificates:\\n\\n```bash \\nopenssl req -new -x509 -days 3650 -newkey rsa:4096 -nodes -keyout .keys/ca.key -subj \\"/CN=My Custom CA\\" -out .keys/ca.pem\\n```\\n\\nThis will create `deploy/.keys/ca.key` and `deploy/.keys/ca.pem` files.\\n\\n## Step 5 - .gitignore file\\n\\nCreate `deploy/.gitignore` file with next content:\\n\\n```bash\\n.terraform/\\n.keys/\\n*.tfstate\\n*.tfstate.*\\n*.tfvars\\ntfplan\\n.env.secrets.prod\\n```\\n\\n## Step 6 - file with secrets for local deploy\\n\\nCreate file `deploy/.env.secrets.prod`\\n\\n```bash\\nADMINFORTH_SECRET=<your_secret>\\n```\\n\\n\\n\\n## Step 7 - main terraform file main.tf\\n\\nFirst of all install Terraform as described here [terraform installation](https://developer.hashicorp.com/terraform/install#linux).\\n\\n\\nCreate file `main.tf` in `deploy` folder:\\n\\n```hcl title=\\"deploy/main.tf\\"\\n\\nlocals {\\n  app_name = \\"<your_app_name>\\"\\n  aws_region = \\"us-east-1\\"\\n}\\n\\nprovider \\"aws\\" {\\n  region = local.aws_region\\n  profile = \\"myaws\\"\\n}\\n\\ndata \\"aws_ami\\" \\"ubuntu_linux\\" {\\n  most_recent = true\\n  owners      = [\\"amazon\\"]\\n\\n  filter {\\n    name   = \\"name\\"\\n    values = [\\"ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-*\\"]\\n  }\\n}\\n\\ndata \\"aws_vpc\\" \\"default\\" {\\n  default = true\\n}\\n\\nresource \\"aws_eip\\" \\"eip\\" {\\n domain = \\"vpc\\"\\n}\\nresource \\"aws_eip_association\\" \\"eip_assoc\\" {\\n instance_id   = aws_instance.app_instance.id\\n allocation_id = aws_eip.eip.id\\n}\\n\\ndata \\"aws_subnet\\" \\"default_subnet\\" {\\n  filter {\\n    name   = \\"vpc-id\\"\\n    values = [data.aws_vpc.default.id]\\n  }\\n\\n  filter {\\n    name   = \\"default-for-az\\"\\n    values = [\\"true\\"]\\n  }\\n\\n  filter {\\n    name   = \\"availability-zone\\"\\n    values = [\\"${local.aws_region}a\\"]\\n  }\\n}\\n\\nresource \\"aws_security_group\\" \\"instance_sg\\" {\\n  name   = \\"${local.app_name}-instance-sg\\"\\n  vpc_id = data.aws_vpc.default.id\\n\\n  ingress {\\n    description = \\"Allow HTTP\\"\\n    from_port   = 80\\n    to_port     = 80\\n    protocol    = \\"tcp\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n\\n  ingress {\\n    description = \\"Allow Docker registry\\"\\n    from_port   = 5000\\n    to_port     = 5000\\n    protocol    = \\"tcp\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n\\n  # SSH\\n  ingress {\\n    description = \\"Allow SSH\\"\\n    from_port   = 22\\n    to_port     = 22\\n    protocol    = \\"tcp\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n\\n  egress {\\n    description = \\"Allow all outbound traffic\\"\\n    from_port   = 0\\n    to_port     = 0\\n    protocol    = \\"-1\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n}\\n\\nresource \\"aws_key_pair\\" \\"app_deployer\\" {\\n  key_name   = \\"terraform-deploy_${local.app_name}-key\\"\\n  public_key = file(\\"./.keys/id_rsa.pub\\") # Path to your public SSH key\\n}\\n\\nresource \\"aws_instance\\" \\"app_instance\\" {\\n  ami                    = data.aws_ami.ubuntu_linux.id\\n  instance_type          = \\"t3a.small\\"  # just change it to another type if you need, check https://instances.vantage.sh/\\n  subnet_id              = data.aws_subnet.default_subnet.id\\n  vpc_security_group_ids = [aws_security_group.instance_sg.id]\\n  key_name               = aws_key_pair.app_deployer.key_name\\n\\n  # prevent accidental termination of ec2 instance and data loss\\n  # if you will need to recreate the instance still (not sure why it can be?), you will need to remove this block manually by next command:\\n  # > terraform taint aws_instance.app_instance\\n  lifecycle {\\n    prevent_destroy = true\\n    ignore_changes = [ami]\\n  }\\n\\n  root_block_device {\\n    volume_size = 20 // Size in GB for root partition\\n    volume_type = \\"gp2\\"\\n    \\n    # Even if the instance is terminated, the volume will not be deleted, delete it manually if needed\\n    delete_on_termination = false\\n  }\\n\\n  user_data = <<-EOF\\n    #!/bin/bash\\n    sudo apt-get update\\n    sudo apt-get install ca-certificates curl\\n    sudo install -m 0755 -d /etc/apt/keyrings\\n    sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\\n    sudo chmod a+r /etc/apt/keyrings/docker.asc\\n\\n    # Add the repository to Apt sources:\\n    echo \\\\\\n      \\"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\\\\n      $(. /etc/os-release && echo \\"$VERSION_CODENAME\\") stable\\" | \\\\\\n      sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\\n    sudo apt-get update\\n\\n    sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin screen\\n\\n    systemctl start docker\\n    systemctl enable docker\\n    usermod -a -G docker ubuntu\\n\\n    echo \\"done\\" > /home/ubuntu/user_data_done\\n\\n  EOF\\n\\n  tags = {\\n    Name = \\"${local.app_name}-instance\\"\\n  }\\n}\\n\\nresource \\"null_resource\\" \\"wait_for_user_data\\" {\\n  provisioner \\"remote-exec\\" {\\n    inline = [\\n      \\"echo \'Waiting for EC2 software install to finish...\'\\",\\n      \\"while [ ! -f /home/ubuntu/user_data_done ]; do echo \'...\'; sleep 2; done\\",\\n      \\"echo \'EC2 software install finished.\'\\"\\n    ]\\n\\n    connection {\\n      type        = \\"ssh\\"\\n      user        = \\"ubuntu\\"\\n      private_key = file(\\"./.keys/id_rsa\\")\\n      host        = aws_eip_association.eip_assoc.public_ip\\n    }\\n  }\\n\\n  depends_on = [aws_instance.app_instance]\\n}\\n\\nresource \\"null_resource\\" \\"setup_registry\\" {\\n  provisioner \\"local-exec\\" {\\n    command = <<-EOF\\n      echo \\"Generating secret for local registry\\"\\n      sha256sum ./.keys/id_rsa | cut -d \' \' -f1 | tr -d \'\\\\n\' > ./.keys/registry.pure\\n\\n      echo \\"Creating htpasswd file for local registry\\"\\n      docker run --rm --entrypoint htpasswd httpd:2 -Bbn ci-user $(cat ./.keys/registry.pure) > ./.keys/registry.htpasswd\\n\\n      echo \\"Generating server certificate for registry\\"\\n      openssl genrsa -out ./.keys/registry.key 4096\\n      echo \\"subjectAltName=DNS:appserver.local,DNS:localhost,IP:127.0.0.1\\" > san.ext\\n      openssl req -new -key ./.keys/registry.key -subj \\"/CN=appserver.local\\" -addext \\"$(cat san.ext)\\" -out ./.keys/registry.csr\\n\\n      openssl x509 -req -days 365 -CA ./.keys/ca.pem -CAkey ./.keys/ca.key -set_serial 01 -in ./.keys/registry.csr -extfile san.ext -out ./.keys/registry.crt \\n\\n      echo \\"Copying registry secret files to the instance\\"\\n      rsync -t -avz -e \\"ssh -i ./.keys/id_rsa -o StrictHostKeyChecking=no\\" \\\\\\n        ./.keys/registry.* ubuntu@${aws_eip_association.eip_assoc.public_ip}:/home/ubuntu/registry-auth\\n    EOF\\n  }\\n\\n  provisioner \\"remote-exec\\" {\\n    inline = [<<-EOF\\n      # remove old registry if exists\\n      docker rm -f registry\\n      # run new registry\\n      docker run -d --network host \\\\\\n        --name registry \\\\\\n        --restart always \\\\\\n        -v /home/ubuntu/registry-data:/var/lib/registry \\\\\\n        -v /home/ubuntu/registry-auth:/auth\\\\\\n        -e \\"REGISTRY_AUTH=htpasswd\\" \\\\\\n        -e \\"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\\" \\\\\\n        -e \\"REGISTRY_AUTH_HTPASSWD_PATH=/auth/registry.htpasswd\\" \\\\\\n        -e \\"REGISTRY_HTTP_TLS_CERTIFICATE=/auth/registry.crt\\" \\\\\\n        -e \\"REGISTRY_HTTP_TLS_KEY=/auth/registry.key\\" \\\\\\n        registry:2\\n\\n      EOF\\n    ]\\n\\n    connection {\\n      type        = \\"ssh\\"\\n      user        = \\"ubuntu\\"\\n      private_key = file(\\"./.keys/id_rsa\\")\\n      host        = aws_eip_association.eip_assoc.public_ip\\n    }\\n  }\\n\\n  triggers = {\\n    always_run = 1 # change number to redeploy registry (if for some reason it was removed)\\n  }\\n\\n  depends_on = [null_resource.wait_for_user_data]\\n}\\n\\n\\nresource \\"null_resource\\" \\"sync_files_and_run\\" {\\n\\n  provisioner \\"local-exec\\" {\\n    command = <<-EOF\\n\\n      # map appserver.local to the instance (in CI we don\'t know the IP, so have to use this mapping)\\n      # so then in GA pipeline we will use \\n      #  - name: Set up Docker Buildx\\n      #   uses: docker/setup-buildx-action@v3\\n      #   with:\\n      #     buildkitd-config-inline: |\\n      #       [registry.\\"appserver.local:5000\\"]\\n      #         ca=[\\"deploy/.keys/ca.pem\\"]\\n\\n      grep -q \\"appserver.local\\" /etc/hosts || echo \\"${aws_eip_association.eip_assoc.public_ip} appserver.local\\" | sudo tee -a /etc/hosts\\n\\n      # hosts modification may take some time to apply\\n      sleep 5\\n\\n      # generate buildx authorization\\n      sha256sum ./.keys/id_rsa | cut -d \' \' -f1 | tr -d \'\\\\n\' > ./.keys/registry.pure\\n      echo \'{\\"auths\\":{\\"appserver.local:5000\\":{\\"auth\\":\\"\'$(echo -n \\"ci-user:$(cat ./.keys/registry.pure)\\" | base64 -w 0)\'\\"}}}\' > ~/.docker/config.json\\n\\n      echo \\"Running build\\"\\n      docker buildx bake --progress=plain --push --allow=fs.read=.. -f compose.yml\\n\\n      # compose temporarily it is not working https://github.com/docker/compose/issues/11072#issuecomment-1848974315\\n      # docker compose --progress=plain -p app -f ./compose.yml build --push\\n\\n      # if you will change host, pleasee add -o StrictHostKeyChecking=no\\n      echo \\"Copy files to the instance\\" \\n      rsync -t -avz --mkpath -e \\"ssh -i ./.keys/id_rsa -o StrictHostKeyChecking=no\\" \\\\\\n        --delete \\\\\\n        --exclude \'.terraform\' \\\\\\n        --exclude \'.keys\' \\\\\\n        --exclude \'tfplan\' \\\\\\n        . ubuntu@${aws_eip_association.eip_assoc.public_ip}:/home/ubuntu/app/deploy/\\n\\n      EOF\\n  }\\n\\n  # Run docker compose after files have been copied\\n  provisioner \\"remote-exec\\" {\\n    inline = [<<-EOF\\n      # login to docker registry\\n      cat /home/ubuntu/registry-auth/registry.pure | docker login localhost:5000 -u ci-user --password-stdin\\n        \\n      cd /home/ubuntu/app/deploy\\n\\n      echo \\"Spinning up the app\\"\\n      docker compose --progress=plain -p app -f compose.yml up -d --remove-orphans\\n\\n      # cleanup unused cache (run in background to not block terraform)\\n      screen -dm docker system prune -f\\n      screen -dm docker exec registry registry garbage-collect /etc/docker/registry/config.yml --delete-untagged=true \\n    EOF\\n    ]\\n\\n    connection {\\n      type        = \\"ssh\\"\\n      user        = \\"ubuntu\\"\\n      private_key = file(\\"./.keys/id_rsa\\")\\n      host        = aws_eip_association.eip_assoc.public_ip\\n    }\\n\\n\\n  }\\n\\n  # Ensure the resource is triggered every time based on timestamp or file hash\\n  triggers = {\\n    always_run = timestamp()\\n  }\\n\\n  depends_on = [aws_eip_association.eip_assoc, null_resource.setup_registry]\\n}\\n\\n\\noutput \\"instance_public_ip\\" {\\n  value = aws_eip_association.eip_assoc.public_ip\\n}\\n\\n\\n######### META, tf state ##############\\n\\n\\n# S3 bucket for storing Terraform state\\nresource \\"aws_s3_bucket\\" \\"terraform_state\\" {\\n  bucket = \\"${local.app_name}-terraform-state\\"\\n}\\n\\nresource \\"aws_s3_bucket_lifecycle_configuration\\" \\"terraform_state\\" {\\n  bucket = aws_s3_bucket.terraform_state.bucket\\n\\n  rule {\\n    status = \\"Enabled\\"\\n    id = \\"Keep only the latest version of the state file\\"\\n\\n    filter {\\n      prefix = \\"\\"\\n    }\\n\\n    noncurrent_version_expiration {\\n      noncurrent_days = 30\\n    }\\n  }\\n}\\n\\nresource \\"aws_s3_bucket_versioning\\" \\"terraform_state\\" {\\n  bucket = aws_s3_bucket.terraform_state.bucket\\n\\n  versioning_configuration {\\n    status = \\"Enabled\\"\\n  }\\n}\\n\\nresource \\"aws_s3_bucket_server_side_encryption_configuration\\" \\"terraform_state\\" {\\n  bucket = aws_s3_bucket.terraform_state.bucket\\n\\n  rule {\\n    apply_server_side_encryption_by_default {\\n      sse_algorithm     = \\"AES256\\"\\n    }\\n  }\\n}\\n\\n\\n\\n```\\n\\n> \ud83d\udc46 Replace `<your_app_name>` with your app name (no spaces, only underscores or letters)\\n\\n\\n\\n### Step 7.1 - Configure AWS Profile\\n\\nOpen or create file `~/.aws/credentials` and add (if not already there):\\n\\n```ini\\n[myaws]\\naws_access_key_id = <your_access_key>\\naws_secret_access_key = <your_secret_key>\\n```\\n\\n\\n\\n### Step 7.2 - Run deployment\\n\\n\\nWe will run first deployment from local machine to create S3 bucket for storing Terraform state. In other words this deployment will create resources needed for storing Terraform state in the cloud and runnign deployment from GitHub actions.\\n\\n```bash\\nterraform init\\n```\\n\\nNow run deployement:\\n\\n```bash\\nterraform apply -auto-approve\\n```\\n\\n> \ud83d\udc46 Please note that this command might block ask you your `sudo` password to append `appserver.local` to `/etc/hosts` file. \\n\\n> \ud83d\udc46 Please note that command might show errors about pushing images, this is fine because current deployment is done here only to setup S3 bucket for state migration before migrating to cloud. \\n\\n\\n## Step 8 - Migrate state to the cloud\\n\\nFirst deployment had to create S3 bucket for storing Terraform state. Now we need to migrate the state to the cloud.\\n\\nAdd to the end of `main.tf`:\\n\\n```hcl title=\\"main.tf\\"\\n\\n# Configure the backend to use the S3 bucket\\nterraform {\\n backend \\"s3\\" {\\n   bucket         = \\"<your_app_name>-terraform-state\\"\\n   key            = \\"state.tfstate\\"  # Define a specific path for the state file\\n   region         = \\"us-east-1\\"\\n   profile        = \\"myaws\\"\\n   use_lockfile   = true\\n }\\n}\\n```\\n\\n> \ud83d\udc46 Replace `<your_app_name>` with your app name (no spaces, only underscores or letters). \\n> Unfortunately we can\'t use variables, HashiCorp thinks it is too dangerous \ud83d\ude25\\n\\n\\nNow run:\\n\\n```bash\\nterraform init -migrate-state\\n```\\n\\nNow run test deployment:\\n\\n```bash\\nterraform apply -auto-approve\\n```\\n\\nNow you can delete local `terraform.tfstate` file and `terraform.tfstate.backup` file as they are in the cloud now.\\n\\n\\n## Step 9 - CI/CD - Github Actions\\n\\nCreate file `.github/workflows/deploy.yml`:\\n\\n```yml title=\\".github/workflows/deploy.yml\\"\\nname: Deploy myadmin\\nrun-name: ${{ github.actor }} builds myadmin \ud83d\ude80\\non: [push]\\njobs:\\n  Explore-GitHub-Actions:\\n    runs-on: ubuntu-latest\\n\\n    concurrency:\\n      group: build-group\\n      cancel-in-progress: false\\n\\n    steps:\\n      - run: echo \\"\ud83c\udf89 The job was automatically triggered by a ${{ github.event_name }} event.\\"\\n      - run: echo \\"\ud83d\udc27 This job is now running on a ${{ runner.os }} server\\"\\n      - run: echo \\"\ud83d\udd0e The name of your branch is ${{ github.ref }}\\"\\n      - name: Check out repository code\\n        uses: actions/checkout@v4\\n\\n      - name: Set up Terraform\\n        uses: hashicorp/setup-terraform@v2\\n        with:\\n          terraform_version: 1.10.1 \\n      \\n      - name: Import Registry CA\\n        run: |\\n          mkdir -p deploy/.keys\\n          echo \\"$VAULT_REGISTRY_CA_PEM\\" > deploy/.keys/ca.pem\\n          echo \\"$VAULT_REGISTRY_CA_KEY\\" > deploy/.keys/ca.key\\n        env:\\n          VAULT_REGISTRY_CA_PEM: ${{ secrets.VAULT_REGISTRY_CA_PEM }}\\n          VAULT_REGISTRY_CA_KEY: ${{ secrets.VAULT_REGISTRY_CA_KEY }}\\n\\n\\n      - name: Set up Docker Buildx\\n        uses: docker/setup-buildx-action@v3\\n        with:\\n          buildkitd-config-inline: |\\n            [registry.\\"appserver.local:5000\\"]\\n              ca=[\\"deploy/.keys/ca.pem\\"]\\n              \\n          # use host network for resolving appserver.local\\n          driver-opts: network=host\\n\\n      - name: Import registry SSH keys\\n        run: |\\n          mkdir -p deploy/.keys\\n          echo \\"$VAULT_SSH_PRIVATE_KEY\\" > deploy/.keys/id_rsa\\n          echo \\"$VAULT_SSH_PUBLIC_KEY\\" > deploy/.keys/id_rsa.pub\\n          chmod 600 deploy/.keys/id_rsa*\\n        env:\\n          VAULT_SSH_PRIVATE_KEY: ${{ secrets.VAULT_SSH_PRIVATE_KEY }}\\n          VAULT_SSH_PUBLIC_KEY: ${{ secrets.VAULT_SSH_PUBLIC_KEY }}\\n\\n      - name: Setup AWS credentials\\n        run: |\\n          mkdir -p ~/.aws\\n          cat <<EOL > ~/.aws/credentials\\n          [myaws]\\n          aws_access_key_id=${VAULT_AWS_ACCESS_KEY_ID}\\n          aws_secret_access_key=${VAULT_AWS_SECRET_ACCESS_KEY}\\n          EOL\\n        env:\\n          VAULT_AWS_ACCESS_KEY_ID: ${{ secrets.VAULT_AWS_ACCESS_KEY_ID }}\\n          VAULT_AWS_SECRET_ACCESS_KEY: ${{ secrets.VAULT_AWS_SECRET_ACCESS_KEY }}\\n\\n      - name: Prepare env\\n        run: |\\n          echo \\"ADMINFORTH_SECRET=$VAULT_ADMINFORTH_SECRET\\" > deploy/.env.secrets.prod\\n        env:\\n          VAULT_ADMINFORTH_SECRET: ${{ secrets.VAULT_ADMINFORTH_SECRET }}\\n\\n      - name: Terraform build\\n        run: |\\n          cd deploy\\n          terraform init -reconfigure\\n          # example of unlocking tf state if needed\\n          # terraform force-unlock fb397548-8697-ea93-ab80-128a4f508fdf --force\\n          terraform plan -out=tfplan \\n          terraform apply tfplan \\n                \\n          \\n      - run: echo \\"\ud83c\udf4f This job\'s status is ${{ job.status }}.\\"\\n```\\n\\n\\n### Step 8.1 - Add secrets to GitHub\\n\\nGo to your GitHub repository, then `Settings` -> `Secrets` -> `New repository secret` and add:\\n\\n- `VAULT_AWS_ACCESS_KEY_ID` - your AWS access key\\n- `VAULT_AWS_SECRET_ACCESS_KEY` - your AWS secret key\\n- `VAULT_SSH_PRIVATE_KEY` - execute `cat ~/.ssh/id_rsa` and paste to GitHub secrets\\n- `VAULT_SSH_PUBLIC_KEY` - execute `cat ~/.ssh/id_rsa.pub` and paste to GitHub secrets\\n- `VAULT_REGISTRY_CA_PEM` - execute `cat deploy/.keys/ca.pem` and paste to GitHub secrets\\n- `VAULT_REGISTRY_CA_KEY` - execute `cat deploy/.keys/ca.key` and paste to GitHub secrets\\n- `VAULT_ADMINFORTH_SECRET` - generate some random string and paste to GitHub secrets, e.g. `openssl rand -base64 32 | tr -d \'\\\\n\'`\\n\\n\\nNow you can push your changes to GitHub and see how it will be deployed automatically.\\n\\n\\n### Adding secrets\\n\\nOnce you will have sensitive tokens/passwords in your apps you have to store them in a secure way. \\n\\nSimplest way is to use GitHub secrets. \\n\\nLet\'s imagine you have `OPENAI_API_KEY` which will be used one of AI-powered plugins of adminforth. We can\'t put this key to the code, so we have to store it in GitHub secrets.\\n\\nOpen your GitHub repository, then `Settings` -> `Secrets` -> `New repository secret` and add `VAULT_OPENAI_API_KEY` with your key.\\n\\nNow open GitHub actions file and add it to the `env` section:\\n\\n```yml title=\\".github/workflows/deploy.yml\\"\\n      - name: Prepare env\\n        run: |\\n          echo \\"ADMINFORTH_SECRET=$VAULT_ADMINFORTH_SECRET\\" > deploy/.env.secrets.prod\\n//diff-add\\n          echo \\"OPENAI_API_KEY=$VAULT_OPENAI_API_KEY\\" >> deploy/.env.secrets.prod\\n//diff-add\\n        env:\\n          VAULT_ADMINFORTH_SECRET: ${{ secrets.VAULT_ADMINFORTH_SECRET }}\\n//diff-add\\n          VAULT_OPENAI_API_KEY: ${{ secrets.VAULT_OPENAI_API_KEY }}\\n```\\n\\n\\nIn the same way you can add any other secrets to your GitHub actions.\\n\\n\\n\\n### Want to run builds from your local machine?\\n\\nThis guide originally was created to run full builds from GitHub actions only, so out of the box it will fail to push images to registry from your local machine.\\n\\nBut for debug purporses you can run it from your local machine too with some addition steps.\\n\\n#### 1. You need to make local Docker buildx builder to trust self-signed TLS certificate\\n\\nCreate folder `deploy/.local` and create next files:\\n\\n```toml title=deploy/.local/buildkitd.toml \\n[registry.\\"appserver.local:5000\\"]\\n  insecure = false\\n  ca = [\\"../.keys/ca.pem\\"]\\n```\\n\\n```sh title=deploy/.local/create-builder.sh\\n#!/bin/bash\\ncd \\"$(dirname \\"$0\\")\\"\\ndocker buildx rm mybuilder || true\\ndocker buildx create --name mybuilder --driver docker-container   --use --config ./buildkitd.toml\\n```\\n\\nNow create builder:\\n\\n```bash\\nbash .local/create-builder.sh\\n```\\n\\n#### 2. You need to deliver same secrets from local machine as from CI vault\\n\\nCreate file `deploy/.env.secrets.prod` with next content:\\n\\n```sh\\nADMINFORTH_SECRET=<your secret>\\n```\\n\\nPlease note that if you are running builds both from GA and local, the `ADMINFORTH_SECRET` should much to GA secret. Otherwise all existing users will be logged out.\\n\\n#### 2. You need to add app.server.local to your hosts file (Windows/WSL only)\\n\\n> This step is not needed on Linux / Mac because teraform provisioner will autiomatically add it to `/etc/hosts` file.\\n> However in WSL we can\'t modify Windows native hosts file, so we need to do it manually.\\n\\nIn power shell run \\n\\n```\\nStart-Process notepad \\"C:\\\\Windows\\\\System32\\\\drivers\\\\etc\\\\hosts\\" -Verb runAs\\n```\\n\\nCheck your public IP in Terraform output and add\\n\\n```\\n<your public ip> appserver.local\\n```\\n\\n> Bad news is that instance public IP will be known only after first run, so some steps would fail because there will be no hosts mapping. However since EC2 provisioning takes some time it is even possible to copy IP from terminal and inser it to hosts file from first run \ud83e\udd2a\\n\\n\\n#### 3. Using local build from multiple projects\\n\\nThe easiest way would be probably to rename `appserver.local` to unique name for each project.\\n\\nThen you can put all certificate mappings to a `buildkitd.toml` and move it along with `create-builder.sh` script to a common folder, e.g. home"},{"id":"compose-aws-ec2-ecr-terraform-github-actions","metadata":{"permalink":"/blog/compose-aws-ec2-ecr-terraform-github-actions","source":"@site/blog/2025-02-19-compose-aws-ec2-ecr-terraform-github-actions/index.md","title":"IaaC Simplified: Amazon EC2 Deployments with GitHub Actions, Terraform, Docker & Amazon ECR","description":"The ultimate step-by-step guide to cost-effective, build-time-efficient, and easy managable EC2 deployments using GitHub Actions, Terraform, Docker, and a Amazon ECR registry.","date":"2025-02-19T00:00:00.000Z","tags":[{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"Amazon Web Services (AWS) is a cloud computing platform that provides a wide range of services for building and deploying applications."},{"inline":false,"label":"Terraform","permalink":"/blog/tags/terraform","description":"Terraform is an open-source infrastructure as code software tool created by HashiCorp that enables users to define and provision data center infrastructure using a declarative configuration language."},{"inline":false,"label":"GitHub Actions","permalink":"/blog/tags/github-actions","description":"GitHub Actions is a continuous integration and continuous deployment (CI/CD) service provided by GitHub that allows you to automate your software development workflows."}],"readingTime":20.305,"hasTruncateMarker":true,"authors":[{"name":"Ivan Borshchov","title":"Maintainer of AdminForth","url":"https://github.com/ivictbor","imageURL":"https://avatars.githubusercontent.com/u/1838656?v=4","key":"ivanb","page":null}],"frontMatter":{"slug":"compose-aws-ec2-ecr-terraform-github-actions","title":"IaaC Simplified: Amazon EC2 Deployments with GitHub Actions, Terraform, Docker & Amazon ECR","authors":"ivanb","tags":["aws","terraform","github-actions"],"description":"The ultimate step-by-step guide to cost-effective, build-time-efficient, and easy managable EC2 deployments using GitHub Actions, Terraform, Docker, and a Amazon ECR registry.","image":"/ogs/ga-tf-ecr.jpg"},"unlisted":false,"prevItem":{"title":"Amazon EC2 Deployments with GitHub Actions, Terraform, Docker & Self-hosted Registry","permalink":"/blog/compose-ec2-deployment-github-actions-registry"},"nextItem":{"title":"How I Open-Sourced My Secret Access Tokens from GitHub, Slack, and NPM \u2014 and Who Actually Cares","permalink":"/blog/how-i-opensourced-my-secret-tokens"}},"content":"![alt text](ga-tf-ecr.jpg)\\n\\n\\nThis guide shows how to deploy own Docker apps (with AdminForth as example) to Amazon EC2 instance with Docker and Terraform involving pushing images into Amazon ECR.\\n\\nNeeded resources:\\n- GitHub actions Free plan which includes 2000 minutes per month (1000 of 2-minute builds per month - more then enough for many projects, if you are not running tests). Extra builds would cost `0.008$` per minute.\\n- AWS account where we will auto-spawn EC2 instance. We will use `t3a.small` instance (2 vCPUs, 2GB RAM) which costs `~14$` per month in `us-east-1` region (cheapest region). Also it will take `$2` per month for EBS gp2 storage (20GB) for EC2 instance. \\n- Also AWS ECR will charge for `$0.09` per GB of data egress traffic (from EC2 to the internet) - this needed to load docker build cache.\\n\\nThe setup shape:\\n- Build is done using IaaC approach with HashiCorp Terraform, so almoast no manual actions are needed from you. Every resource including EC2 server instance is described in code which is commited to repo.\\n- Docker build process is done on GitHub actions server, so EC2 server is not overloaded with builds\\n- Changes in infrastructure including changing server type, adding S3 Bucket, changing size of sever disk is also can be done by commiting code to repo.\\n- Docker images and build cache are stored on Amazon ECR\\n- Total build time for average commit to AdminForth app (with Vite rebuilds) is around 2 minutes.\\n\\n\x3c!-- truncate --\x3e\\n\\n# Building on CI versus building on EC2?\\n\\nPreviously we had a blog post about [deploying AdminForth to EC2 with Terraform without registry](/blog/compose-ec2-deployment-github-actions/). That method might work well but has a significant disadvantage - build process happens on EC2 itself and uses EC2 RAM and CPU. This can be a problem if your EC2 instance is well-loaded without extra free resources. Moreover, low-end EC2 instances have a small amount of RAM and CPU, so build process which involves vite/tsc/etc can be slow or even fail / cause OOM killer to crash EC2 instance.\\n\\nSo obviously to solve this problem we need to move the build process to CI, however it introduces new chellenges and we will solve them in this post.\\n\\nQuick difference between approaches from previous post and current post:\\n\\n| Feature | Without Registry | With ECR Registry |\\n| --- | --- | --- |\\n| How build happens | Source code is rsync-ed from CI to EC2 and docker build is done there | Docker build is done on CI and docker image is pushed to registry, then Docker on EC2 pulls from registry |\\n| Where build is done | On EC2 | On CI |\\n| How Docker build layers are cached | Cache is stored on EC2 | GitHub actions has no own Docker cache out of the box, so it should be stored in dedicated place (we use Amazon ECR) |\\n| Advantages | Cheaper (no egrass cache traffik from EC2) and faster | Build is done on CI, so EC2 server is not overloaded |\\n| Disadvantages | Build on EC2 requires additional server RAM / requires swap / overloads CPU | More terraform code is needed. Extra cost for egress traffik to GitHub for cache transfer |\\n| Initial build time\\\\* | 3m 13.541s | 3m 54s |\\n| Rebuild time (changed `index.ts`)\\\\*| 0m 51.653s | 0m 54.120s |\\n\\n<sub>\\\\* All tests done from local machine (Intel(R) Core(TM) Ultra 9 185H, Docker Desktop/WSL2 64 GB RAM, 300Mbps up/down) up to working state</sub>\\n\\n\\n## Chellenges when you build on CI\\n\\nA little bit of theory.\\n\\nWhen you move build process to CI you have to solve next chellenges:\\n1) We need to deliver built docker images to EC2 somehow (and only we)\\n2) We need to persist cache between builds\\n\\n### Delivering images\\n\\n#### Exporing images to tar files\\n\\nSimplest option which you can find is save docker images to tar files and deliver them to EC2. We can easily do it in terraform (using `docker save -o ...` command on CI and `docker load ...` command on EC2). However this option has a significant disadvantage - it is slow. Docker images are big (always include all layers, without any options), so it takes infinity to do save/load and another infinity to transfer them to EC2 (via relatively slow rsync/SSH and relatively slow GitHub actions outbound connection).\\n\\n#### Docker registry\\n\\nFaster, right option which we will use here - involve Docker registry. Registry is a repository which stores docker images. It does it in a smart way - it saves each image as several layers, so if you will update last layer, then only last layer will be pushed to registry and then only last will be pulled to EC2. \\nTo give you row compare - whole-layers image might take `1GB`, but last layer created by `npm run build` command might take `50MB`. And most builds you will do only last layer changes, so it will be 20 times faster to push/pull last layer than whole image.\\nAnd this is not all, registry uses TLS HTTP protocol so it is faster then SSH/rsync encrypted connection. \\n\\nOf course you have to care about a way of registry authentication (so only you and your CI/EC2 can push/pull images).\\n\\nWhat docker registry can you use? Pretty known options:\\n1) Docker Hub - most famous. It is free for public images, so literally every opensource project uses it. However it is not free for private images, and you have to pay for it. Payment model is pretty strange - you pay for user who can login, like 11$ per month, you might pay for your devops only but all this sounds strange. \\n2) GHCR - Registry from GitHub. Has free plan but allows to store only 500MB and allows to transfer 1GB of traffic per month. Then you pay for every extra GB in storage (`$0.0008` per GB/day or `$0.24` per GB/month) and for every extra GB in traffic ($0.09 per GB). Probably small images will fit in free plan, but generally even alpine-based docker images are bigger than 500MB, so it is non-free option.\\n3) Amazon ECR - Same as GHCR but from Amazon. Price is `$0.10` per GB of storage per month and `$0.09` per GB of data transfer from Amazon (as all Amazon egress traffic). So it is cheaper than GHCR.\\n4) Self-hosted registry web system. In our software development company, we use Harbor. It is a powerful free open-source registry that can be installed to own server. It allows pushing and pulling without limit. Also, it has internal life-cycle rules that cleanup unnecessary images and layers. The main drawbacks of it are that it is not so fast to install and configure, plus you have to get a domain and another powerfull server to run it. So unless you are a software development company, it is not worth using it.\\n5) Self-hosted minimal CNCF Distribution [registry](https://distribution.github.io/distribution/) on EC2 itself. So since we already have EC2, we can run registry on it directly. The `registry` container is pretty light-weight and it will not consume a lot of extra CPU/RAM on server. Plus images will be stored close to application so pull will be fast, however securing this right is a bit tricky. If you want to try it we have special [EC2 with CNCF registry post](/blog/compose-ec2-deployment-github-actions-registry/).\\n\\nIn the post we will use Amazon ECR as registry (3rd way). \\n\\n### Persisting cache\\n\\nDocker builds without layer cache persistence are possible but very slow. Most builds only change a couple of layers, and having no ability to cache them will cause the Docker builder to regenerate all layers from scratch. This can, for example, increase the Docker build time from a minute to ten minutes or even more.\\n\\nOut of the box, GitHub Actions can\'t save Docker layers between builds, so you have to use external storage.\\n\\n> Though some CI systems can persist docker build cache, e.g. open-source self-hosted Woodpecker CI allows it out of the box. However GitHub actions which is pretty popular, reasonably can\'t allow such free storage to anyone\\n\\nSo when build-in Docker cache can\'t be used, there is one alternative - Docker BuildKit external cache. \\nSo BuildKit allows you to connect external storage. There are several options, but most sweet for us is using Docker registry as cache storage (not only as images storage to deliver them to application server). \\n\\nDrawback is that buildx which is running on GitHub action server will download cache from registry which is inside of AWS. And all AWS egress traffic is charged. So you will pay for every build which uses cache. However cache is comnpressed. To give you idea basic alpine image with AdminForth cache is 180MB. One one commercial project we did full release within 2 months and 400 builds so it took `400 * 180MB * $0.09 / GB = $6.48` for cache transfer for whole project. \\n\\n> *BuildKit cache in Compose issue*\\n> Previously we used docker compose to build & run our app, it can be used to both build, push and pull images, but has [issues with external cache connection](https://github.com/docker/compose/issues/11072#issuecomment-1848974315). While they are not solved we have to use `docker buildx bake` command to build images. It is not so bad, but is another point of configuration which we will cover in this post.\\n\\n\\n# Prerequisites\\n\\nI will assume you run Ubuntu (Native or WSL2).\\n\\nYou should have terraform, here is official repository: \\n\\n```\\nwget -O - https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\\necho \\"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\\nsudo apt update && sudo apt install terraform\\n```\\n\\n\\nAWS CLI:\\n\\n```bash\\nsudo snap install aws-cli --classic\\n```\\n\\nAlso you need Doker Daemon running. We recommend Docker Desktop running. ON WSL2 make sure you have Docker Desktop WSL2 integration enabled.\\n\\n```bash\\ndocker version\\n```\\n\\n# Practice - deploy setup\\n\\nAssume you have your AdminForth project in `myadmin`.\\n\\n\\n## Step 1 - Dockerfile and .dockerignore\\n\\n\\nThis guide assumes you have created your AdminForth application with latest version of `adminforth create-app` command. \\nThis command already creates a `Dockerfile` and `.dockerignore` for you, so you can use them as is.\\n\\n\\n## Step 2 - compose.yml\\n\\ncreate folder `deploy` and create file `compose.yml` inside:\\n\\n```yml title=\\"deploy/compose.yml\\"\\nservices:\\n  traefik:\\n    image: \\"traefik:v2.5\\"\\n    command:\\n      - \\"--api.insecure=true\\"\\n      - \\"--providers.docker=true\\"\\n      - \\"--entrypoints.web.address=:80\\"\\n    ports:\\n      - \\"80:80\\"\\n    volumes:\\n      - \\"/var/run/docker.sock:/var/run/docker.sock:ro\\"\\n\\n  myadmin:\\n    image: ${MYADMIN_REPO}:latest\\n    build:\\n      context: ../adminforth-app\\n      tags:\\n        - ${MYADMIN_REPO}:latest\\n      cache_from:\\n        - type=registry,ref=${MYADMIN_REPO}:cache\\n      cache_to:\\n        - type=registry,ref=${MYADMIN_REPO}:cache,mode=max,compression=zstd,image-manifest=true,oci-mediatypes=true\\n      \\n    pull_policy: always\\n    restart: always\\n    env_file:\\n      - .env.secrets.prod\\n\\n    volumes:\\n      - myadmin-db:/code/db\\n    labels:\\n      - \\"traefik.enable=true\\"\\n      - \\"traefik.http.routers.myadmin.rule=PathPrefix(`/`)\\"\\n      - \\"traefik.http.services.myadmin.loadbalancer.server.port=3500\\"\\n      - \\"traefik.http.routers.myadmin.priority=2\\"\\n\\nvolumes:\\n  myadmin-db:\\n```\\n\\n## Step 3 - create a SSH keypair\\n\\nMake sure you are still in `deploy` folder, run next command:\\n\\n```bash title=\\"deploy\\"\\nmkdir .keys && ssh-keygen -f .keys/id_rsa -N \\"\\"\\n```\\n\\nNow it should create `deploy/.keys/id_rsa` and `deploy/.keys/id_rsa.pub` files with your SSH keypair. Terraform script will put the public key to the EC2 instance and will use private key to connect to the instance. Also you will be able to use it to connect to the instance manually.\\n\\n## Step 4 - create TLS certificates to encrypt traffic between CI and registry\\n\\nMake sure you are still in `deploy` folder, run next command:\\n\\nRun next command to create TLS certificates:\\n\\n```bash \\nopenssl req -new -x509 -days 3650 -newkey rsa:4096 -nodes -keyout .keys/ca.key -subj \\"/CN=My Custom CA\\" -out .keys/ca.pem\\n```\\n\\nThis will create `deploy/.keys/ca.key` and `deploy/.keys/ca.pem` files.\\n\\n## Step 5 - .gitignore file\\n\\nCreate `deploy/.gitignore` file with next content:\\n\\n```bash\\n.terraform/\\n.keys/\\n*.tfstate\\n*.tfstate.*\\n*.tfvars\\ntfplan\\n.env.secrets.prod\\n```\\n\\n## Step 6 - file with secrets for local deploy\\n\\nCreate file `deploy/.env.secrets.prod`\\n\\n```bash\\nADMINFORTH_SECRET=<your_secret>\\n```\\n\\n\\n## Step 7 - main terraform file main.tf\\n\\nFirst of all install Terraform as described here [terraform installation](https://developer.hashicorp.com/terraform/install#linux).\\n\\n\\nCreate file `main.tf` in `deploy` folder:\\n\\n```hcl title=\\"deploy/main.tf\\"\\n\\nlocals {\\n  app_name = \\"testtf\\"\\n  aws_region = \\"us-east-1\\"\\n}\\n\\nprovider \\"aws\\" {\\n  region = local.aws_region\\n  profile = \\"myaws\\"\\n}\\n\\ndata \\"aws_ami\\" \\"ubuntu_linux\\" {\\n  most_recent = true\\n  owners      = [\\"amazon\\"]\\n\\n  filter {\\n    name   = \\"name\\"\\n    values = [\\"ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-*\\"]\\n  }\\n}\\n\\ndata \\"aws_vpc\\" \\"default\\" {\\n  default = true\\n}\\n\\nresource \\"aws_eip\\" \\"eip\\" {\\n domain = \\"vpc\\"\\n}\\n\\nresource \\"aws_eip_association\\" \\"eip_assoc\\" {\\n instance_id   = aws_instance.app_instance.id\\n allocation_id = aws_eip.eip.id\\n}\\n\\ndata \\"aws_subnet\\" \\"default_subnet\\" {\\n  filter {\\n    name   = \\"vpc-id\\"\\n    values = [data.aws_vpc.default.id]\\n  }\\n\\n  filter {\\n    name   = \\"default-for-az\\"\\n    values = [\\"true\\"]\\n  }\\n\\n  filter {\\n    name   = \\"availability-zone\\"\\n    values = [\\"${local.aws_region}a\\"]\\n  }\\n}\\n\\nresource \\"aws_security_group\\" \\"instance_sg\\" {\\n  name   = \\"${local.app_name}-instance-sg\\"\\n  vpc_id = data.aws_vpc.default.id\\n\\n  ingress {\\n    description = \\"Allow HTTP\\"\\n    from_port   = 80\\n    to_port     = 80\\n    protocol    = \\"tcp\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n\\n  ingress {\\n    description = \\"Allow Docker registry\\"\\n    from_port   = 5000\\n    to_port     = 5000\\n    protocol    = \\"tcp\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n\\n  # SSH\\n  ingress {\\n    description = \\"Allow SSH\\"\\n    from_port   = 22\\n    to_port     = 22\\n    protocol    = \\"tcp\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n\\n  egress {\\n    description = \\"Allow all outbound traffic\\"\\n    from_port   = 0\\n    to_port     = 0\\n    protocol    = \\"-1\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n}\\n\\nresource \\"aws_key_pair\\" \\"app_deployer\\" {\\n  key_name   = \\"terraform-deploy_${local.app_name}-key\\"\\n  public_key = file(\\"./.keys/id_rsa.pub\\") # Path to your public SSH key\\n}\\n\\nresource \\"aws_instance\\" \\"app_instance\\" {\\n  ami                    = data.aws_ami.ubuntu_linux.id\\n  instance_type          = \\"t3a.small\\"  # just change it to another type if you need, check https://instances.vantage.sh/\\n  subnet_id              = data.aws_subnet.default_subnet.id\\n  vpc_security_group_ids = [aws_security_group.instance_sg.id]\\n  key_name               = aws_key_pair.app_deployer.key_name\\n  iam_instance_profile = aws_iam_instance_profile.ec2_profile.name\\n\\n  # prevent accidental termination of ec2 instance and data loss\\n  # if you will need to recreate the instance still (not sure why it can be?), you will need to remove this block manually by next command:\\n  # > terraform taint aws_instance.app_instance\\n  lifecycle {\\n    prevent_destroy = true\\n    ignore_changes = [ami]\\n  }\\n\\n  root_block_device {\\n    volume_size = 20 // Size in GB for root partition\\n    volume_type = \\"gp2\\"\\n    \\n    # Even if the instance is terminated, the volume will not be deleted, delete it manually if needed\\n    delete_on_termination = false\\n  }\\n\\n  user_data = <<-EOF\\n    #!/bin/bash\\n    sudo apt-get update\\n    sudo apt-get install ca-certificates curl python3 python3-pip -y\\n    sudo install -m 0755 -d /etc/apt/keyrings\\n    sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\\n    sudo chmod a+r /etc/apt/keyrings/docker.asc\\n\\n    # Add the repository to Apt sources:\\n    echo \\\\\\n      \\"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\\\\n      $(. /etc/os-release && echo \\"$VERSION_CODENAME\\") stable\\" | \\\\\\n      sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\\n    sudo apt-get update\\n\\n    sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin screen\\n\\n    systemctl start docker\\n    systemctl enable docker\\n    usermod -a -G docker ubuntu\\n\\n    sudo snap install aws-cli --classic\\n\\n    echo \\"done\\" > /home/ubuntu/user_data_done\\n\\n  EOF\\n\\n  tags = {\\n    Name = \\"${local.app_name}-instance\\"\\n  }\\n}\\n\\nresource \\"null_resource\\" \\"wait_for_user_data\\" {\\n  provisioner \\"remote-exec\\" {\\n    inline = [\\n      \\"echo \'Waiting for EC2 software install to finish...\'\\",\\n      \\"while [ ! -f /home/ubuntu/user_data_done ]; do echo \'...\'; sleep 2; done\\",\\n      \\"echo \'EC2 software install finished.\'\\"\\n    ]\\n\\n    connection {\\n      type        = \\"ssh\\"\\n      user        = \\"ubuntu\\"\\n      private_key = file(\\"./.keys/id_rsa\\")\\n      host        = aws_eip_association.eip_assoc.public_ip\\n    }\\n  }\\n\\n  depends_on = [aws_instance.app_instance]\\n}\\n\\nresource \\"aws_ecr_repository\\" \\"myadmin_repo\\" {\\n  name = \\"${local.app_name}-myadmin\\"\\n  force_delete = true\\n}\\n\\nresource \\"aws_ecr_lifecycle_policy\\" \\"safe_cleanup\\" {\\n  repository = aws_ecr_repository.myadmin_repo.name\\n\\n  policy = jsonencode({\\n    rules = [\\n      {\\n        rulePriority = 1\\n        description  = \\"Delete untagged images older than 7 days\\"\\n        selection = {\\n          tagStatus     = \\"untagged\\"\\n          countType     = \\"sinceImagePushed\\"\\n          countUnit     = \\"days\\"\\n          countNumber   = 7\\n        }\\n        action = {\\n          type = \\"expire\\"\\n        }\\n      }\\n    ]\\n  })\\n}\\n\\nresource \\"local_file\\" \\"compose_env\\" {\\n  content  = \\"MYADMIN_REPO=${aws_ecr_repository.myadmin_repo.repository_url}\\"\\n  filename = \\"${path.module}/.env.ecr\\"\\n}\\n\\n// allow ec2 instance to login to ECR too pull images\\nresource \\"aws_iam_role\\" \\"ec2_role\\" {\\n  name = \\"${local.app_name}-ec2-role\\"\\n\\n  assume_role_policy = jsonencode({\\n    Version = \\"2012-10-17\\",\\n    Statement = [{\\n      Effect = \\"Allow\\",\\n      Principal = {\\n        Service = \\"ec2.amazonaws.com\\"\\n      },\\n      Action = \\"sts:AssumeRole\\"\\n    }]\\n  })\\n}\\n\\nresource \\"aws_iam_role_policy_attachment\\" \\"ecr_access\\" {\\n  role       = aws_iam_role.ec2_role.name\\n  policy_arn = \\"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPowerUser\\"\\n}\\n\\nresource \\"aws_iam_instance_profile\\" \\"ec2_profile\\" {\\n  name = \\"${local.app_name}-instance-profile\\"\\n  role = aws_iam_role.ec2_role.name\\n}\\n\\nresource \\"null_resource\\" \\"sync_files_and_run\\" {\\n\\n  provisioner \\"local-exec\\" {\\n    command = <<-EOF\\n      aws ecr get-login-password --region ${local.aws_region} --profile myaws | docker login --username AWS --password-stdin ${aws_ecr_repository.myadmin_repo.repository_url}\\n\\n      echo \\"Running build\\"\\n      env $(cat .env.ecr | grep -v \\"#\\" | xargs) docker buildx bake --progress=plain --push --allow=fs.read=.. -f compose.yml\\n\\n      # if you will change host, pleasee add -o StrictHostKeyChecking=no\\n      echo \\"Copy files to the instance\\" \\n      rsync -t -avz --mkpath -e \\"ssh -i ./.keys/id_rsa -o StrictHostKeyChecking=no\\" \\\\\\n        --delete \\\\\\n        --exclude \'.terraform\' \\\\\\n        --exclude \'.keys\' \\\\\\n        --exclude \'tfplan\' \\\\\\n        . ubuntu@${aws_eip_association.eip_assoc.public_ip}:/home/ubuntu/app/deploy/\\n\\n      EOF\\n  }\\n\\n  # Run docker compose after files have been copied\\n  provisioner \\"remote-exec\\" {\\n    inline = [<<-EOF\\n      aws ecr get-login-password --region ${local.aws_region} | docker login --username AWS --password-stdin ${aws_ecr_repository.myadmin_repo.repository_url}\\n\\n      cd /home/ubuntu/app/deploy\\n\\n      echo \\"Spinning up the app\\"\\n      docker compose --progress=plain -p app  --env-file .env.ecr -f compose.yml up -d --remove-orphans\\n\\n      # cleanup unused cache (run in background to not block terraform)\\n      screen -dm docker system prune -f\\n    EOF\\n    ]\\n\\n    connection {\\n      type        = \\"ssh\\"\\n      user        = \\"ubuntu\\"\\n      private_key = file(\\"./.keys/id_rsa\\")\\n      host        = aws_eip_association.eip_assoc.public_ip\\n    }\\n\\n\\n  }\\n\\n  # Ensure the resource is triggered every time based on timestamp or file hash\\n  triggers = {\\n    always_run = timestamp()\\n  }\\n\\n  depends_on = [aws_eip_association.eip_assoc, null_resource.wait_for_user_data]\\n}\\n\\n\\noutput \\"instance_public_ip\\" {\\n  value = aws_eip_association.eip_assoc.public_ip\\n}\\n\\n\\n######### META, tf state ##############\\n\\n\\n# S3 bucket for storing Terraform state\\nresource \\"aws_s3_bucket\\" \\"terraform_state\\" {\\n  bucket = \\"${local.app_name}-terraform-state\\"\\n}\\n\\nresource \\"aws_s3_bucket_lifecycle_configuration\\" \\"terraform_state\\" {\\n  bucket = aws_s3_bucket.terraform_state.bucket\\n\\n  rule {\\n    status = \\"Enabled\\"\\n    id = \\"Keep only the latest version of the state file\\"\\n\\n    filter {\\n      prefix = \\"\\"\\n    }\\n\\n    noncurrent_version_expiration {\\n      noncurrent_days = 30\\n    }\\n  }\\n}\\n\\nresource \\"aws_s3_bucket_versioning\\" \\"terraform_state\\" {\\n  bucket = aws_s3_bucket.terraform_state.bucket\\n\\n  versioning_configuration {\\n    status = \\"Enabled\\"\\n  }\\n}\\n\\nresource \\"aws_s3_bucket_server_side_encryption_configuration\\" \\"terraform_state\\" {\\n  bucket = aws_s3_bucket.terraform_state.bucket\\n\\n  rule {\\n    apply_server_side_encryption_by_default {\\n      sse_algorithm     = \\"AES256\\"\\n    }\\n  }\\n}\\n\\n\\n```\\n\\n> \ud83d\udc46 Replace `<your_app_name>` with your app name (no spaces, only underscores or letters)\\n\\n\\n\\n### Step 7.1 - Configure AWS Profile\\n\\nOpen or create file `~/.aws/credentials` and add (if not already there):\\n\\n```ini\\n[myaws]\\naws_access_key_id = <your_access_key>\\naws_secret_access_key = <your_secret_key>\\n```\\n\\n\\n\\n### Step 7.2 - Run deployment\\n\\n\\nWe will run first deployment from local machine to create S3 bucket for storing Terraform state. In other words this deployment will create resources needed for storing Terraform state in the cloud and runnign deployment from GitHub actions.\\n\\nIn `deploy` folder run:\\n\\n```bash \\nterraform init\\n```\\n\\nNow run deployement:\\n\\n```bash\\nterraform apply -auto-approve\\n```\\n\\n> \ud83d\udc46 Please note that this command might block ask you your `sudo` password to append `appserver.local` to `/etc/hosts` file. \\n\\n> \ud83d\udc46 Please note that command might show errors about pushing images, this is fine because current deployment is done here only to setup S3 bucket for state migration before migrating to cloud. \\n\\n\\n## Step 8 - Migrate state to the cloud\\n\\nFirst deployment had to create S3 bucket for storing Terraform state. Now we need to migrate the state to the cloud.\\n\\nAdd to the end of `main.tf`:\\n\\n```hcl title=\\"main.tf\\"\\n\\n# Configure the backend to use the S3 bucket\\nterraform {\\n backend \\"s3\\" {\\n   bucket         = \\"<your_app_name>-terraform-state\\"\\n   key            = \\"state.tfstate\\"  # Define a specific path for the state file\\n   region         = \\"us-east-1\\"\\n   profile        = \\"myaws\\"\\n   use_lockfile   = true\\n }\\n}\\n```\\n\\n> \ud83d\udc46 Replace `<your_app_name>` with your app name (no spaces, only underscores or letters). \\n> Unfortunately we can\'t use variables, HashiCorp thinks it is too dangerous \ud83d\ude25\\n\\n\\nNow run:\\n\\n```bash\\nterraform init -migrate-state\\n```\\n\\nNow run test deployment:\\n\\n```bash\\nterraform apply -auto-approve\\n```\\n\\nNow you can delete local `terraform.tfstate` file and `terraform.tfstate.backup` file as they are in the cloud now.\\n\\n\\n## Step 9 - CI/CD - Github Actions\\n\\nCreate file `.github/workflows/deploy.yml`:\\n\\n```yml title=\\".github/workflows/deploy.yml\\"\\nname: Deploy myadmin\\nrun-name: ${{ github.actor }} builds myadmin \ud83d\ude80\\non: [push]\\njobs:\\n  Explore-GitHub-Actions:\\n    runs-on: ubuntu-latest\\n\\n    concurrency:\\n      group: build-group\\n      cancel-in-progress: false\\n\\n    steps:\\n      - run: echo \\"\ud83c\udf89 The job was automatically triggered by a ${{ github.event_name }} event.\\"\\n      - run: echo \\"\ud83d\udc27 This job is now running on a ${{ runner.os }} server\\"\\n      - run: echo \\"\ud83d\udd0e The name of your branch is ${{ github.ref }}\\"\\n      - name: Check out repository code\\n        uses: actions/checkout@v4\\n\\n      - name: Set up Terraform\\n        uses: hashicorp/setup-terraform@v2\\n        with:\\n          terraform_version: 1.10.1 \\n      \\n      - name: Import Registry CA\\n        run: |\\n          mkdir -p deploy/.keys\\n          echo \\"$VAULT_REGISTRY_CA_PEM\\" > deploy/.keys/ca.pem\\n          echo \\"$VAULT_REGISTRY_CA_KEY\\" > deploy/.keys/ca.key\\n        env:\\n          VAULT_REGISTRY_CA_PEM: ${{ secrets.VAULT_REGISTRY_CA_PEM }}\\n          VAULT_REGISTRY_CA_KEY: ${{ secrets.VAULT_REGISTRY_CA_KEY }}\\n\\n\\n      - name: Set up Docker Buildx\\n        uses: docker/setup-buildx-action@v3\\n\\n      - name: Import registry SSH keys\\n        run: |\\n          mkdir -p deploy/.keys\\n          echo \\"$VAULT_SSH_PRIVATE_KEY\\" > deploy/.keys/id_rsa\\n          echo \\"$VAULT_SSH_PUBLIC_KEY\\" > deploy/.keys/id_rsa.pub\\n          chmod 600 deploy/.keys/id_rsa*\\n        env:\\n          VAULT_SSH_PRIVATE_KEY: ${{ secrets.VAULT_SSH_PRIVATE_KEY }}\\n          VAULT_SSH_PUBLIC_KEY: ${{ secrets.VAULT_SSH_PUBLIC_KEY }}\\n\\n      - name: Setup AWS credentials\\n        run: |\\n          mkdir -p ~/.aws\\n          cat <<EOL > ~/.aws/credentials\\n          [myaws]\\n          aws_access_key_id=${VAULT_AWS_ACCESS_KEY_ID}\\n          aws_secret_access_key=${VAULT_AWS_SECRET_ACCESS_KEY}\\n          EOL\\n        env:\\n          VAULT_AWS_ACCESS_KEY_ID: ${{ secrets.VAULT_AWS_ACCESS_KEY_ID }}\\n          VAULT_AWS_SECRET_ACCESS_KEY: ${{ secrets.VAULT_AWS_SECRET_ACCESS_KEY }}\\n\\n      - name: Prepare env\\n        run: |\\n          echo \\"ADMINFORTH_SECRET=$VAULT_ADMINFORTH_SECRET\\" > deploy/.env.secrets.prod\\n        env:\\n          VAULT_ADMINFORTH_SECRET: ${{ secrets.VAULT_ADMINFORTH_SECRET }}\\n\\n      - name: Terraform build\\n        run: |\\n          cd deploy\\n          terraform init -reconfigure\\n          # example of unlocking tf state if needed\\n          # terraform force-unlock fb397548-8697-ea93-ab80-128a4f508fdf --force\\n          terraform plan -out=tfplan \\n          terraform apply tfplan \\n                \\n          \\n      - run: echo \\"\ud83c\udf4f This job\'s status is ${{ job.status }}.\\"\\n```\\n\\n\\n### Step 10 - Add secrets to GitHub\\n\\nGo to your GitHub repository, then `Settings` -> `Secrets` -> `New repository secret` and add:\\n\\n- `VAULT_AWS_ACCESS_KEY_ID` - your AWS access key\\n- `VAULT_AWS_SECRET_ACCESS_KEY` - your AWS secret key\\n- `VAULT_SSH_PRIVATE_KEY` - execute `cat deploy/.keys/id_rsa` and paste to GitHub secrets\\n- `VAULT_SSH_PUBLIC_KEY` - execute `cat deploy/.keys/id_rsa.pub` and paste to GitHub secrets\\n- `VAULT_REGISTRY_CA_PEM` - execute `cat deploy/.keys/ca.pem` and paste to GitHub secrets\\n- `VAULT_REGISTRY_CA_KEY` - execute `cat deploy/.keys/ca.key` and paste to GitHub secrets\\n- `VAULT_ADMINFORTH_SECRET` - generate some random string and paste to GitHub secrets, e.g. `openssl rand -base64 32 | tr -d \'\\\\n\'`\\n\\n\\nNow you can push your changes to GitHub and see how it will be deployed automatically.\\n\\n\\n### Adding secrets\\n\\nOnce you will have sensitive tokens/passwords in your apps you have to store them in a secure way. \\n\\nSimplest way is to use GitHub secrets. \\n\\nLet\'s imagine you have `OPENAI_API_KEY` which will be used one of AI-powered plugins of adminforth. We can\'t put this key to the code, so we have to store it in GitHub secrets.\\n\\nOpen your GitHub repository, then `Settings` -> `Secrets` -> `New repository secret` and add `VAULT_OPENAI_API_KEY` with your key.\\n\\nNow open GitHub actions file and add it to the `env` section:\\n\\n```yml title=\\".github/workflows/deploy.yml\\"\\n      - name: Prepare env\\n        run: |\\n          echo \\"ADMINFORTH_SECRET=$VAULT_ADMINFORTH_SECRET\\" > deploy/.env.secrets.prod\\n//diff-add\\n          echo \\"OPENAI_API_KEY=$VAULT_OPENAI_API_KEY\\" >> deploy/.env.secrets.prod\\n//diff-add\\n        env:\\n          VAULT_ADMINFORTH_SECRET: ${{ secrets.VAULT_ADMINFORTH_SECRET }}\\n//diff-add\\n          VAULT_OPENAI_API_KEY: ${{ secrets.VAULT_OPENAI_API_KEY }}\\n```\\n\\n\\nIn the same way you can add any other secrets to your GitHub actions.\\n\\n### How to connect to EC2 instance?\\n\\nTo connect to EC2 instance you can use SSH.\\n\\n```bash\\ncd deploy\\nssh -i ./.keys/id_rsa ubuntu@<your_ec2_ip>\\n```\\n\\nIP address can be found in terminal output after terraform apply.\\n\\n### Out of space on EC2 instance? Extend EBS volume\\n\\nTo upgrade EBS volume size you have to do next steps:\\n\\nIn `main.tf` file:\\n\\n```hcl title=\\"main.tf\\"\\n  root_block_device {\\n//diff-remove\\n    volume_size = 20 // Size in GB for root partition\\n//diff-add\\n    volume_size = 40 // Size in GB for root partition\\n    volume_type = \\"gp2\\"\\n  }\\n```\\n\\nAnd run build. \\n\\nThis will increase physical size of EBS volume, but you have to increase filesystem size too.\\n\\nLogin to EC2 instance:\\n\\n```bash\\nssh -i ./.keys/id_rsa ubuntu@<your_ec2_ip>\\n```\\n\\n> You can find your EC2 IP in AWS console by visiting EC2 -> Instances -> Your instance -> IPv4 Public IP \\n\\n\\nNow run next commands:\\n\\n```bash\\nlsblk\\n```\\n\\nThis would show something like this:\\n\\n```bash\\nNAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\\nloop0     7:0    0 99.4M  1 loop /snap/core/10908\\nnvme0n1 259:0    0   40G  0 disk\\n\u2514\u2500nvme0n1p1 259:1    0   20G  0 part /\\n```\\n\\nHere we see that `nvme0n1` is our disk and `nvme0n1p1` is our partition.\\n\\nNow to extend partition run:\\n\\n```bash\\nsudo growpart /dev/nvme0n1 1\\nsudo resize2fs /dev/nvme0n1p1\\n```\\n\\nThis will extend partition to the full disk size. No reboot is needed.\\n\\n\\n### Want slack notifications about build?\\n\\nCreate Slack channel and add [Slack app](https://slack.com/apps/A0F7YS25R-incoming-webhooks) to it. \\n\\nThen create webhook URL and add it to GitHub secrets as `SLACK_WEBHOOK_URL`.\\n\\nAdd this steps to the end of your GitHub actions file:\\n\\n```yml title=\\".github/workflows/deploy.yml\\"\\n      - name: Notify Slack on success\\n        if: success()\\n        run: |\\n          curl -X POST -H \'Content-type: application/json\' --data \\\\\\n          \\"{\\\\\\"text\\\\\\": \\\\\\"\u2705 *${{ github.actor }}* successfully built *${{ github.ref_name }}* with commit \\\\\\\\\\\\\\"${{ github.event.head_commit.message }}\\\\\\\\\\\\\\".\\\\n:link: <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Build> | :link: <${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }}|View Commit>\\\\\\"}\\" \\\\\\n          $SLACK_WEBHOOK_URL \\n        env:\\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\\n\\n      - name: Notify Slack on failure\\n        if: failure()\\n        run: |\\n          curl -X POST -H \'Content-type: application/json\' --data \\\\\\n          \\"{\\\\\\"text\\\\\\": \\\\\\"\u274c *${{ github.actor }}* failed to build *${{ github.ref_name }}* with commit \\\\\\\\\\\\\\"${{ github.event.head_commit.message }}\\\\\\\\\\\\\\".\\\\n:link: <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Build> | :link: <${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }}|View Commit>\\\\\\"}\\" \\\\\\n          $SLACK_WEBHOOK_URL \\n        env:\\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\\n\\n```"},{"id":"how-i-opensourced-my-secret-tokens","metadata":{"permalink":"/blog/how-i-opensourced-my-secret-tokens","source":"@site/blog/2025-01-24-how-i-published-token/index.md","title":"How I Open-Sourced My Secret Access Tokens from GitHub, Slack, and NPM \u2014 and Who Actually Cares","description":"Our framework has a CI pipeline that runs npm run build, publishes the package to NPM (npm publish), and creates a new release on GitHub. It also sends a notification about the release to a Slack webhook for our team.","date":"2025-01-24T00:00:00.000Z","tags":[],"readingTime":4.34,"hasTruncateMarker":true,"authors":[{"name":"Ivan Borshchov","title":"Maintainer of AdminForth","url":"https://github.com/ivictbor","imageURL":"https://avatars.githubusercontent.com/u/1838656?v=4","key":"ivanb","page":null}],"frontMatter":{"slug":"how-i-opensourced-my-secret-tokens","title":"How I Open-Sourced My Secret Access Tokens from GitHub, Slack, and NPM \u2014 and Who Actually Cares","authors":"ivanb"},"unlisted":false,"prevItem":{"title":"IaaC Simplified: Amazon EC2 Deployments with GitHub Actions, Terraform, Docker & Amazon ECR","permalink":"/blog/compose-aws-ec2-ecr-terraform-github-actions"},"nextItem":{"title":"Why manual Release Notes and Versions are a chaos and how to fix it","permalink":"/blog/why-manual-release-notes-and-versions-are-a-chaos-and-how-to-fix-it"}},"content":"Our framework has a CI pipeline that runs `npm run build`, publishes the package to NPM (`npm publish`), and creates a new release on GitHub. It also sends a notification about the release to a Slack webhook for our team.\\n\\nSecrets for these services were stored in our CI\u2019s built-in Vault (we are running a self-hosted Woodpecker CI).\\n\\nRecently, while moving plugins to separate repositories, I decided to try [Infisical](https://infisical.com/) for centralized secrets management instead of the internal CI Vault. Infisical provides a self-hosted open-source solution, has a well-organized UI, and offers better access control than our CI Vault. It was important to me that I could reuse secrets across different repositories without copying them every time I created a new plugin.\\n\\nHere\u2019s what I did:\\n\\n\x3c!-- truncate --\x3e\\n\\n```yaml title=\\".woodpecker.yml\\"\\n  \\nsteps:\\n//diff-add\\n  init-secrets:\\n//diff-add\\n    when:\\n//diff-add\\n      - event: push\\n//diff-add\\n    image: infisical/cli\\n//diff-add\\n    environment:\\n//diff-add\\n      INFISICAL_TOKEN:\\n//diff-add\\n        from_secret: VAULT_TOKEN\\n//diff-add\\n    commands:\\n//diff-add\\n      - infisical export --domain https://vault.devforth.io/api --format=dotenv-export --env=\\"prod\\" > .vault.env\\n//diff-add\\n    secrets:\\n//diff-add\\n      - VAULT_TOKEN\\n\\n  release:\\n    image: node:20\\n    when:\\n      - event: push\\n    commands:\\n//diff-add\\n      - export $(cat .vault.env | xargs)\\n      - cd adminforth\\n      - npm clean-install\\n      - npm run build\\n      - npm audit signatures\\n      # does publish to npm, creates release on github, and sends notification to slack webhook\\n      - npx semantic-release \\n//diff-remove\\n    secrets:\\n//diff-remove\\n      - VAULT_NPM_TOKEN\\n//diff-remove\\n      - VAULT_GITHUB_TOKEN\\n//diff-remove\\n      - VAULT_SLACK_TOKEN\\n```\\n\\nPretty dumb method to export secrets to the `.vault.env` file, but it was late evening, and I didn\u2019t want to spend much time on it at the start.\\n\\nI made the first push, and everything worked fine on the first attempt. I was happy.\\n\\nThen I started adding the same code to the first plugin, and the plugin build failed with a very unexpected error.\\n\\nIt said that my NPM token was invalid. I was surprised and started printing the environment variables to see what was wrong (printing environment variables to the build log is a pretty bad practice and is the last thing you want to do, but I knew it was an internal CI, and the project was private).\\n\\nI saw that my NPM token was still in the environment variables and was the same.\\n\\nI went back to the first repository and retried the build. It failed with the same error.\\n\\nI went to NPM and found out that the token had disappeared entirely from the list. I was shocked and recreated it.\\n\\nOn the next build, I discovered that the Slack webhook was also not working. However, GitHub releases were created without issues in both repositories.\\n\\nThen I noticed an email push notification from Slack titled \\"Notification about invalidated webhook URLs.\\"\\n\\n![Slack Notification about invalidated webhook URLs](image.png)\\n\\nThis was the moment I realized that `npm publish` had simply taken my `.vault.env` file and published it to NPM.\\n\\nShortly after, I noticed a recent email from NPM titled \\"Granular access token deleted.\\"\\n\\n![npm Granular access token deleted](image-1.png)\\n\\nThe next thing I did was revoke all tokens, including the GitHub token, which still worked, and unpublish all packages from NPM (though they might still be cloned by some caches/aggregators/archivers).\\n\\n# How Services Handle Leaked Secret Tokens\\n\\n## GitHub\\n\\nGitHub was not able to recognize that the token had been leaked to an NPM package and revoke it. Although they do a pretty good job when you push other vendors\u2019 secrets to a GitHub repository, it seems they don\u2019t check NPM sources.\\n\\n## NPM \\n\\nNPM detected that the NPM token was published to their registry and revoked it. However, it was hard to understand why\u2014it was simply deleted. They sent an email, but it did not explain why the token was deleted or specify the source of the leak. Showing an error in the tokens list on the NPM website would have been the best option.\\n\\n## Slack\\n\\nI was surprised, but Slack did a great job. They monitor NPM (I don\u2019t think they monitor the whole NPM registry; there\u2019s probably some interesting technology behind it). They detected that the NPM token was published to the registry and invalidated it. They sent an email with a clear explanation of why it was invalidated and what steps to take next.\\n\\n# Conclusion\\n\\nWe can talk a lot about bad programming practices, but the main takeaway is that we are human. And humans still make mistakes.  \\nIt makes a lot of sense to monitor for such human errors.  \\n\\nIn my case, NPM and Slack saved me from a potential security breach. Without their intervention, I would have learned about the issue only when someone used my tokens for malicious purposes.  \\nGitHub didn\u2019t detect or revoke the token, and many other services wouldn\u2019t have done so either.\\n\\nHere are some common recommendations I learned from this experience:\\n\\n- Try to limit token access as much as possible to only the required granularity. Even if something is leaked, it won\u2019t cause much harm. Don\u2019t grant access to all resources/packages/repos unless it\u2019s necessary.\\n- Check what you publish, especially when making changes to your build pipeline. I missed the fact that the `.env` file was being published.\\n- Appreciate services that monitor for leaks and respond to them. They can save you from potential security breaches."},{"id":"why-manual-release-notes-and-versions-are-a-chaos-and-how-to-fix-it","metadata":{"permalink":"/blog/why-manual-release-notes-and-versions-are-a-chaos-and-how-to-fix-it","source":"@site/blog/2025-01-19-how-adminforth-manages-version/index.md","title":"Why manual Release Notes and Versions are a chaos and how to fix it","description":"Learn what profits you can get from automatic versioning and learn how simply you can configure it!","date":"2025-01-19T00:00:00.000Z","tags":[{"inline":false,"label":"Git","permalink":"/blog/tags/git","description":"Git is a distributed version control system that allows multiple developers to collaborate on a project."},{"inline":false,"label":"Versioning","permalink":"/blog/tags/versioning","description":"Versioning is the process of assigning unique identifiers to different versions of a software application or project."},{"inline":false,"label":"NPM","permalink":"/blog/tags/npm","description":"NPM is a package manager for the JavaScript programming language that allows developers to share and reuse code."}],"readingTime":13.12,"hasTruncateMarker":true,"authors":[{"name":"Ivan Borshchov","title":"Maintainer of AdminForth","url":"https://github.com/ivictbor","imageURL":"https://avatars.githubusercontent.com/u/1838656?v=4","key":"ivanb","page":null}],"frontMatter":{"slug":"why-manual-release-notes-and-versions-are-a-chaos-and-how-to-fix-it","title":"Why manual Release Notes and Versions are a chaos and how to fix it","description":"Learn what profits you can get from automatic versioning and learn how simply you can configure it!","authors":"ivanb","tags":["git","versioning","npm"],"image":"/ogs/autover.jpg"},"unlisted":false,"prevItem":{"title":"How I Open-Sourced My Secret Access Tokens from GitHub, Slack, and NPM \u2014 and Who Actually Cares","permalink":"/blog/how-i-opensourced-my-secret-tokens"},"nextItem":{"title":"Backup database to AWS Glacier","permalink":"/blog/backup-database-to-aws-glacier"}},"content":"I have a feeling that after first ~600 versions of Adminforth we faced all possible issues with manual versioning and release notes. \\n\\nManual versioning and CHANGELOG.md is unreliable as human beings are. It is pretty easy to forget it with relevant information, forget to include some changes, forget to push it to GitHub, push it at wrong time, and many more things.\\n\\nThat is why we decided to move the idea of generating versions, and GitHub releases from git commit messages using great tool called [semantic-release](https://semantic-release.gitbook.io/semantic-release/usage/configuration).\\n\\nIn this post I will explain why we did a transition from manual releases to automatic, what profits we got from it, and also will show you simple example how to do it in your project!\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## Prehistory and issues\\n\\nBefore 1.6.0 AdminForth was using manual CHANGELOG.md. \\n\\nDuring development we were reviwing PRs, merged them all to `main` branch, pulled to local machine and there did manually npm release which also created a git tag and pushed it to GitHub.\\n\\nWe were constantly releasing to `next` pre-release version from `main` and used `next` internally on our projets for testing. \\n\\n### What is npm pre-release version?\\n\\nIf you are not familiar with pre-release versions, I can explain it on simple example.\\n\\nIf you have version `1.2.3` in your `package.json` and you run `npm version patch` it will bump version to `1.2.4`. If you run it again it will bump version to `1.2.5`.\\nIf you have version `1.2.3` and run `npm version prerelease --preid=next` it will bump version to `1.2.4-next.0`. If you run it again it will bump version to `1.2.4-next.1`. If you run `npm version patch` now it will release version `1.2.4`, hoever if at last step you run `npm version minor` it would release version `1.3.0`.\\n\\nPlease pay attention that `npm` is pretty smart and aligned with normal software release cycle \u2013 once you run `npm version pre-release` on stable version, it understands that you start working on new version and will bump patch version. Once you run `npm version patch` at pre-release version, it will release it as stable version without bumping patch number.\\n\\nThis is very useful because we can collect features and fixes in `next` version without releasing them to `latest` version, so users who do `npm install adminforth` will not get new experimental features and fixes, but thouse who want to test them early can do `npm install adminforth@next`. Our team also using `next` for commercial projects to test new features so in the end of the day we have more stable `latest` version.\\n\\nIn new automatic release process we preserved this approach, but made it in separate git branches.\\n\\nOnce we collected enough features and fixes in `next`, we were doing a release to `latest` version, and at this time we did release documentation. \\n\\n### Issue one: easy to forget add something to CHANGELOG.md\\n\\nSo when I merged PRs to `main` branch, I had to check commits and write them to CHANGELOG.md. \\n\\nAt this point I wasted time to understand how to call the change in CHANGELOG.md, and categorize it as `Added`, `Changed`, `Fixed`, `Removed`, `Security`.\\n\\nAlso there was a chance that I will skip some commit, or will understand wrong what it was about.\\n\\n### Issue two: CHANGELOG.md might be forgot to be changed before release as all\\n\\nThere was a chance that I will forget to update `CHANGELOG.md` at all. I merged PRs, I am hurry and doing release. \\n\\nSometimes I will soon understand that I forgot to update it and will push it to GitHub. It will born another dump commit message like \\"Update a CHANGELOG.md\\". \\n\\nAlso I am not sure that users are looking into `main` branch to see CHANGELOG, probably they see it in npmjs.com.\\n\\n### Issue three: lack of GitHub releases or need to maintain both\\n\\nWe did not have GitHub releases at all at that time. We had only tags. And tags were applied only after release.\\n\\nBut honestly, when I am by myself working with some pacakge, first try to find GitHub releases, though CHANGELOG.md.\\n\\nSo if I would add GitHub releases, I would have to do a lot of clicks or would need some script / CLI to create release notes. This process would have similar issues as with CHANGELOG.md.\\n\\n### Issue four: manual tags\\n\\nSince release was manual from my PC there was a chance that I will do some minor fix, will forget to commit it, then will do release and release script will add tag to previous, irrelevant commit. \\n\\nIn such projects with manual tags, there is only one reliabile way for package user to to check what is difference in source code between two versions: using some tool like https://npmdiff.dev/ and not rely on git and CHANGELOG\\n\\nSince git tags were applied only after release and there again was a chance that I will forget to push them to GitHub with a release.\\n\\n### Issue five: dump commits\\n\\nSince with every manual release we updated CHANGELOG.md and updated version in `pacakge.json` every time, we had to do a commit. \\n\\nSo it borned a lot of dump commits like \\"Update CHANGELOG.md\\", \\"Update version to 1.2.3\\", \\"Update version to 1.2.4\\", \\"Update version to 1.2.5\\".\\n\\nSometime it wass forgot to be commited separately and was commited with other changes in some fix which is even worsen.\\n\\n### Issue six: releae delay and bus-factor\\n\\nSo if I was busy new features were waiting for release, becuase only I had access to do releases.\\n\\nSonner I passed access to some of my colleagues to do releases but from time to time it caused state desync and release issues between our PCs if someone forgot to push something to GitHub after release.\\n\\nEven couple of release contrubuters are onboard, it still takes a time to do all the stuff with Changelog, version, tags, etc, so it delays release.\\n\\n### Issue seven: lack of pre-release versions changes\\n\\nWhile we releasing to `next` we added items under one version in CHANGELOG for simplicity. It would be another extra time to add every `next` version in CHANGELOG and describe whole markdown for it. \\n\\nSo user was not able to distinguish `1.5.0-next.0` from `1.5.0-next.1` in CHANGELOG, which was not an issue for `latest` users but issue for `next` users.\\n\\n\\n## Semantic-release: what is it and how it works\\n\\n`semantic-release` is a tool that solved all issues above. Basically it is a CLI tool that you run on your CI/CD server on every push to your repository. \\n\\nSo on every push it analyzes commit messages from previous release and does next things:\\n\\n- It understands what type of release it should do: major, minor, patch, pre-release depending on commit messages. E.g. if you have a commit message `feat: add new feature` it will do a minor release, if you have `fix: fix bug` it will do a patch release.\\n- It reads previous version from git tags and does a new version based on type of release. So it does not edit `package.json` file.\\n- It generates a GitHub tag and release notes based on commit messages. So you do not need to write CHANGELOG.md anymore.\\n- It publishes a new version to npmjs.com. So you do not need to do `npm publish` anymore.\\n- It is capable to release to `next` channel from separate `next` branch without version bumping. So you can collect features and fixes in `next` without releasing them to `latest`.\\n- It has plugins, for example to send Slack notifications about releases.\\n\\nThe sweet thing that it is all executed on CI/CD server, so you do not need to do anything manually.\\n\\n\\n## Ussage example\\n\\nI will show a flow on empty fake small project to not overcomplicate things. \\n\\nThis will allow you to apply it to your project once you ready.\\n\\nWe will use minimal typescript package with `npm` and `semantic-release` to show how it works.\\n\\nFirst init new git repository:\\n\\n```bash\\necho \\"# test-sem-release\\" >> README.md\\ngit init\\ngit add README.md\\ngit commit -m \\"first commit\\"\\ngit branch -M main\\ngit remote add origin git@github.com:devforth/test-sem-release.git\\ngit push -u origin main\\n```\\n\\nNow lets init new `npm package`:\\n\\n```bash\\nnpm init -y\\nnpm install typescript --save-dev\\nnpx tsc --init\\n```\\n\\nCreate a file `index.ts`:\\n\\n```typescript title=\\"index.ts\\"\\nexport const greet = (name: string): string => {\\n  return `Hello, ${name}!`;\\n};\\n```\\n\\n\\nIn `package.json` add:\\n\\n```json\\n{\\n  \\"name\\": \\"@devforth/test-sem-release\\",\\n//diff-add\\n  \\"publishConfig\\": {\\n//diff-add\\n    \\"access\\": \\"public\\"\\n//diff-add\\n  },\\n  \\"version\\": \\"1.0.0\\",\\n  \\"main\\": \\"index.js\\",\\n  \\"scripts\\": {\\n    \\"test\\": \\"echo \\\\\\"Error: no test specified\\\\\\" && exit 1\\"\\n//diff-add\\n    \\"build\\": \\"tsc\\",\\n  },\\n  \\"author\\": \\"\\",\\n  \\"license\\": \\"ISC\\",\\n  \\"description\\": \\"\\",\\n//diff-add\\n  \\"release\\": {\\n//diff-add\\n    \\"branches\\": [\\n//diff-add\\n      \\"main\\",\\n//diff-add\\n      {\\n//diff-add\\n        \\"name\\": \\"next\\",\\n//diff-add\\n        \\"prerelease\\": true\\n//diff-add\\n      }\\n//diff-add\\n    ],\\n//diff-add\\n    \\"plugins\\": [\\n//diff-add\\n      \\"@semantic-release/commit-analyzer\\",\\n//diff-add\\n      \\"@semantic-release/release-notes-generator\\",\\n//diff-add\\n      \\"@semantic-release/npm\\",\\n//diff-add\\n      \\"@semantic-release/github\\"\\n//diff-add\\n    ],\\n//diff-add\\n  }\\n}\\n```\\n\\nMake sure name in `package.json` has your organisation name like mine `@devforth/` and you have access to publish packages to npmjs.com.\\n\\n\\nAlso install `semantic-release`:\\n\\n```bash\\nnpm i -D semantic-release\\n```\\n\\n\\n## Connecting to CI\\n\\nWe will use Woodpecker CI for this example. Woodpecker is a free and open-source CI/CD tool that you can install to your own server / VPS and will not need to pay for build minutes, and will only for server. No limits on pipelines, users, repositories, etc. If you want to try it, we have [Woodpecker installation guide](https://devforth.io/blog/step-by-step-guide-to-modern-secure-ci-setup/)\\n\\nCreate a file `.woodpecker.yml` in `deploy` directory:\\n\\n```yml title=\\"deploy/.woodpecker.yml\\"\\nclone:\\n  git:\\n    image: woodpeckerci/plugin-git\\n    settings:\\n      partial: false\\n      depth: 5\\n\\nsteps:\\n  release:\\n    image: node:22\\n    when:\\n      - event: push\\n    commands:\\n      - npm clean-install\\n      - npm run build\\n      - npm audit signatures\\n      - npx semantic-release\\n    secrets:\\n      - GITHUB_TOKEN\\n      - NPM_TOKEN\\n```\\n\\nGo to Woodpecker, authorize via GitHub, click `Add repository`, find your repository and add it.\\n\\nDisable `Project settings` -> `Allow Pull Requests` because we do not want to trigger builds on PRs.\\nEnable `Project settings` -> `Trusted`\\nEnable `Project Visibility` -> `Internal` unless you want to make it public.\\n\\n> We strictly recommend to use `Internal` visibility for your projects, because if you use `Public` visibility, your build logs will be public and if accidentally you will print some secret to console, it will be public (generally it happens when you debug something and print environment variables).\\n\\n![Woodpecker project settings](image-4.png)\\n\\n\\n### Generating GitHub acces token\\n\\nIf your repo is in GitHub organisation, you need first enable access to personal access tokens for your organisation (if not yet done):\\n\\n1. In the upper-right corner of GitHub, select your profile photo, then click `Your organizations`. \\n2. Next to the organization, click `Settings`.\\n3. In the left sidebar, under  Personal access tokens, click `Settings`.\\n4. Select `Allow access via fine-grained personal access tokens`\\n5. We recommend setting `Require administrator approval`\\n6. \\"Allow access via personal access tokens (classic)\\"\\n\\nNow go to your profile, click on `Settings` -> `Developer settings` -> `Personal access tokens` -> `Generate new token`\\n\\nFor permissions,\\n* Select `Contents`: `Read and Write`\\n* Select `Metadata`: `Read-only` (if not yet selected)\\n\\n\\nIn Woodpecker go to `Settings`, `Secrets`, `Add Secret`, put name name: `GITHUB_TOKEN` and paste your token:\\n\\n![Woodpecker Secrets](image.png)\\n\\nIn `Available at following events` select `Push`.\\n\\n\\n### Generating NPM token\\n\\nGo to your npmjs.com account, click on `Profile Avatar` -> `Access Tokens` -> `Generate New Token` -> `New Granular Access Token`.\\n\\nPackages and scopes Permissions: Read and Write.\\n\\nIn similar way to GitHub token, add it to Woodpecker as secret with name `NPM_TOKEN`\\n\\n\\n## Testing\\n\\nFor now we did not yet push anything to GitHub and did not publish anything to npm. \\n\\nLets do it now.\\n\\nJust push your first commit as:\\n\\n\\n```sh\\nfeat: initial commit\\n```\\n\\nThis will trigger semantic-release to do first release `v1.0.0`.\\n\\nNow change something is index.ts and push it as fix\\n\\n```sh\\nfix: fix greet function\\n```\\n\\nThis will trigger semantic-release to do release `v1.0.1`.\\n\\n\\nNow change something in `index.ts` and push it as feat\\n\\n```sh\\nfeat: add new function\\n```\\n\\nThis will trigger semantic-release to do release `v1.1.0` because we added new feature, not just fixed something.\\n\\n\\n### Next distribution channel\\n\\nNow we will show how to release to `next` channel.\\n\\n```sh\\ngit checkout -b next\\n```\\n\\nChange something and push it as fix\\n\\n```sh\\nfix: fix greet function in next\\n```\\n\\nCommit it and push:\\n\\n```sh\\ngit push --set-upstream origin next\\n```\\n\\nThis will trigger semantic-release to do release `v1.1.1-next.1`. Please not that it bumped patch version because we are in `next` channel.\\n\\nNow lets add feature to next\\n\\n```\\nfeat: add new feature in next\\n```\\n\\nIt will trigger release `v1.2.0-next.1` because we added new feature and minor version was bumped. Please not that next number started from 1 again.\\n\\nNoe lets merge `next` to `main` and push it:\\n\\n```\\ngit checkout main\\ngit merge next\\ngit push\\n```\\n\\nThis will trigger release `v1.2.0` because we merged `next` to `main` and it was a feature release.\\n\\n\\n## Slack notifications about releases\\n\\nSo now we have automatic releases with release notes on GitHub. \\nFor our internal team we use Slack and we want to get notifications about releases there.\\n\\n```sh\\nnpm i -D semantic-release-slack-bot\\n```\\n\\nInto \\"release\\" section of `package.json` add slack plugin:\\n\\n```json\\n \\"plugins\\": [\\n      \\"@semantic-release/commit-analyzer\\",\\n      \\"@semantic-release/release-notes-generator\\",\\n      \\"@semantic-release/npm\\",\\n      \\"@semantic-release/github\\",\\n//diff-add\\n      [\\n//diff-add\\n        \\"semantic-release-slack-bot\\",\\n//diff-add\\n        {\\n//diff-add\\n          \\"notifyOnSuccess\\": true,\\n//diff-add\\n          \\"notifyOnFail\\": true,\\n//diff-add\\n          \\"slackIcon\\": \\":package:\\",\\n//diff-add\\n          \\"markdownReleaseNotes\\": true\\n//diff-add\\n        }\\n//diff-add\\n      ]\\n//diff-add\\n    ],\\n```\\n\\n\\nAlso create channel in Slack, click on channel name, go to `Integrations` -> `Add an App` -> `Incoming Webhooks` -> `Add to Slack` -> \\"Add Incoming Webhook to Workspace\\" -> \\"Add to Slack\\" -> \\"Copy Webhook URL\\"\\n\\nAdd it to Woodpecker as secret `SLACK_WEBHOOK` environment variable.\\n\\nAlso add this secterd to `.woodpecker.yml`:\\n\\n```yml title=\\"deploy/.woodpecker.yml\\"\\n    secrets:\\n      - GITHUB_TOKEN\\n      - NPM_TOKEN\\n//diff-add\\n      - SLACK_WEBHOOK\\n```\\n\\n\\nThis will send notifications to Slack channel about succesfull releases when `npm run build` is done without errors:\\n\\n![Slack notifications about releases](image-5.png)\\n\\n\\n## Should I maintain CHANGELOG.md anymore?\\n\\n`semantic-release` has a plugin for generating not only GitHub release notes, but also CHANGELOG.md.\\n\\nSince previusly we used CHANGELOG.md I thought it would be good to have it in project. But once I entered [plugin page](https://github.com/semantic-release/changelog) I got a notice which [explained complexity](https://semantic-release.gitbook.io/semantic-release/support/faq#should-release-notes-be-committed-to-a-changelog.md-in-my-repository-during-a-release) added for this approach.\\n\\nSo we ended with a simple [link to GitHub releases](https://github.com/devforth/adminforth/blob/main/CHANGELOG.md)\\n \\n## Is it all that good?\\n\\nWell, there are no perfect approaches in the world. \\n\\nOf course `semantic-release` has some cons.\\n\\nFirst of all, while you can write a commit messages without any prefix and they will not be included in release, you still have to follow strict commit message format when you are releasing feature or fix. And you don\'t have to forget to use this format. Instead of making manual forming of release notes which is done by one person, now every developer in team has to follow the same format and has to write clear commit messages.\\n\\nAnd there are couple of bad things with last points:\\n- It is not so easy to modify commit message once it is pushed to GitHub, so commit writing becomes one of the most critical parts of development process where you have to be very careful.\\n- Commit message should be understood not only by developers of framework, but also by users of framework. And some developers might think that these are two absolutely different dementions of understanding: first one is for developers, second one is for users. But in fact, at AdminForth, we think that set of user-friendly commit messages is a very small subset of set of developer-friendly commit messages. So if you write user-friendly commit messages, there is no chance that developers will not understand them. \\n\\n\\nYou are still fine with merging incoming PRs and ignore their commit messages becuse GitHub allows to edit commit message before merging ( using `Squash and merge` option )"},{"id":"backup-database-to-aws-glacier","metadata":{"permalink":"/blog/backup-database-to-aws-glacier","source":"@site/blog/2024-12-11-backup/index.md","title":"Backup database to AWS Glacier","description":"Every reliable system requires a backup strategy.","date":"2024-12-11T00:00:00.000Z","tags":[{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"Amazon Web Services (AWS) is a cloud computing platform that provides a wide range of services for building and deploying applications."},{"inline":false,"label":"Terraform","permalink":"/blog/tags/terraform","description":"Terraform is an open-source infrastructure as code software tool created by HashiCorp that enables users to define and provision data center infrastructure using a declarative configuration language."}],"readingTime":1.86,"hasTruncateMarker":true,"authors":[{"name":"Ivan Borshchov","title":"Maintainer of AdminForth","url":"https://github.com/ivictbor","imageURL":"https://avatars.githubusercontent.com/u/1838656?v=4","key":"ivanb","page":null}],"frontMatter":{"slug":"backup-database-to-aws-glacier","title":"Backup database to AWS Glacier","authors":"ivanb","tags":["aws","terraform"]},"unlisted":false,"prevItem":{"title":"Why manual Release Notes and Versions are a chaos and how to fix it","permalink":"/blog/why-manual-release-notes-and-versions-are-a-chaos-and-how-to-fix-it"},"nextItem":{"title":"Deploy AdminForth to EC2 with terraform on CI","permalink":"/blog/compose-ec2-deployment-github-actions"}},"content":"Every reliable system requires a backup strategy. \\n\\nIf you have no own backup infrastructure, here can suggest a small docker container that will help you to backup your database to AWS Glacier.\\n\\n\x3c!-- truncate --\x3e\\n\\nAs a base guide we will use a previous blog post about [deploying adminforth infrastructure](/blog/compose-ec2-deployment-github-actions).\\n\\n\\nFirst we need to allocate a new bucket in AWS S3 with modifying terraform configuration:\\n\\n```hcl title=\\"deploy/main.tf\\"\\nresource \\"aws_s3_bucket\\" \\"backup_bucket\\" {\\n  bucket = \\"${local.app_name}-backups\\"\\n}\\n\\nresource \\"aws_s3_bucket_lifecycle_configuration\\" \\"backup_bucket\\" {\\n  bucket = aws_s3_bucket.backup_bucket.bucket\\n\\n  rule {\\n    id     = \\"glacier-immediate\\"\\n    status = \\"Enabled\\"\\n\\n    transition {\\n      days          = 0\\n      storage_class = \\"GLACIER\\"\\n    }\\n  }\\n}\\n\\nresource \\"aws_s3_bucket_server_side_encryption_configuration\\" \\"backup_bucket\\" {\\n  bucket = aws_s3_bucket.backup_bucket.bucket\\n\\n  rule {\\n    apply_server_side_encryption_by_default {\\n      sse_algorithm = \\"AES256\\"\\n    }\\n  }\\n}\\n\\nresource \\"aws_iam_user\\" \\"backup_user\\" {\\n  name = \\"${local.app_name}-backup-user\\"\\n}\\n\\nresource \\"aws_iam_access_key\\" \\"backup_user_key\\" {\\n  user = aws_iam_user.backup_user.name\\n}\\n\\nresource \\"aws_iam_user_policy\\" \\"backup_user_policy\\" {\\n  name = \\"${local.app_name}-backup-policy\\"\\n  user = aws_iam_user.backup_user.name\\n\\n  policy = jsonencode({\\n    Version = \\"2012-10-17\\"\\n    Statement = [\\n      {\\n        Effect = \\"Allow\\"\\n        Action = [\\n          \\"s3:PutObject\\",\\n          \\"s3:GetObject\\",\\n          \\"s3:DeleteObject\\",\\n          \\"s3:ListBucket\\"\\n        ]\\n        Resource = [\\n          aws_s3_bucket.backup_bucket.arn,\\n          \\"${aws_s3_bucket.backup_bucket.arn}/*\\"\\n        ]\\n      }\\n    ]\\n  })\\n}\\n\\n```\\n\\nAlso add a section to main.tf to output the new bucket name and credentials:\\n\\n```hcl title=\\"deploy/main.tf\\"\\n      \\"docker system prune -f\\",\\n      # \\"docker buildx prune -f --filter \'type!=exec.cachemount\'\\",\\n      \\"cd /home/ubuntu/app/deploy\\",\\n//diff-add      \\n      \\"echo \'AWS_BACKUP_ACCESS_KEY=${aws_iam_access_key.backup_user_key.id}\' >> .env.live\\",\\n//diff-add\\n      \\"echo \'AWS_BACKUP_SECRET_KEY=${aws_iam_access_key.backup_user_key.secret}\' >> .env.live\\",\\n//diff-add\\n      \\"echo \'AWS_BACKUP_BUCKET=${aws_s3_bucket.backup_bucket.id}\' >> .env.live\\",\\n//diff-add\\n      \\"echo \'AWS_BACKUP_REGION=${local.aws_region}\' >> .env.live\\",\\n\\n      \\"docker compose -p app -f compose.yml --env-file ./.env.live up --build -d --quiet-pull\\"\\n    ]\\n```\\n\\n\\nAdd new service into compose file:\\n\\n```yaml title=\\"deploy/compose.yml\\"\\n\\n  database_glacierizer:\\n    image: devforth/docker-database-glacierizer:v1.7\\n\\n    environment:\\n      - PROJECT_NAME=MYAPP\\n      # do backup every day at 00:00\\n      - CRON=0 0 * * *  \\n\\n      - DATABASE_TYPE=PostgreSQL\\n      - DATABASE_HOST=db\\n      - DATABASE_NAME=adminforth\\n      - DATABASE_USER=admin\\n      - DATABASE_PASSWORD=${VAULT_POSTGRES_PASSWORD}\\n      - GLACIER_EXPIRE_AFTER=90\\n      - GLACIER_STORAGE_CLASS=flexible\\n      - GLACIER_BUCKET_NAME=${AWS_BACKUP_BUCKET}\\n\\n      - AWS_DEFAULT_REGION=${AWS_BACKUP_REGION}\\n      - AWS_ACCESS_KEY_ID=${AWS_BACKUP_ACCESS_KEY}\\n      - AWS_SECRET_ACCESS_KEY=${AWS_BACKUP_SECRET_KEY}\\n```\\n\\n\\n\\n## Pricing\\n\\nJust to give you row idea about pricing, here is a small calculation for case when you doing backup once per day (like in config)\\n* Compressed database backup has size of 50 MB always and not growing. (Compression is already done by glacierizer)\\n* Cost of glacier every month will be ~$0.80 after first 3 month, and will stay same every next month.  \\n* On first, second and third month cost will increase slowly from $0.00 to $0.80 per month."},{"id":"compose-ec2-deployment-github-actions","metadata":{"permalink":"/blog/compose-ec2-deployment-github-actions","source":"@site/blog/2024-11-14-compose-ec2-deployment-ci/index.md","title":"Deploy AdminForth to EC2 with terraform on CI","description":"Here is more advanced snippet to deploy AdminForth to Terraform.","date":"2024-11-14T00:00:00.000Z","tags":[{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"Amazon Web Services (AWS) is a cloud computing platform that provides a wide range of services for building and deploying applications."},{"inline":false,"label":"Terraform","permalink":"/blog/tags/terraform","description":"Terraform is an open-source infrastructure as code software tool created by HashiCorp that enables users to define and provision data center infrastructure using a declarative configuration language."},{"inline":false,"label":"GitHub Actions","permalink":"/blog/tags/github-actions","description":"GitHub Actions is a continuous integration and continuous deployment (CI/CD) service provided by GitHub that allows you to automate your software development workflows."}],"readingTime":7.675,"hasTruncateMarker":true,"authors":[{"name":"Ivan Borshchov","title":"Maintainer of AdminForth","url":"https://github.com/ivictbor","imageURL":"https://avatars.githubusercontent.com/u/1838656?v=4","key":"ivanb","page":null}],"frontMatter":{"slug":"compose-ec2-deployment-github-actions","title":"Deploy AdminForth to EC2 with terraform on CI","authors":"ivanb","tags":["aws","terraform","github-actions"]},"unlisted":false,"prevItem":{"title":"Backup database to AWS Glacier","permalink":"/blog/backup-database-to-aws-glacier"},"nextItem":{"title":"Deploy AdminForth to EC2 with terraform (without CI)","permalink":"/blog/compose-ec2-deployment"}},"content":"Here is more advanced snippet to deploy AdminForth to Terraform.\\n\\nHere Terraform state will be stored in the cloud, so you can run this deployment from any machine including stateless CI/CD.\\n\\nWe will use GitHub Actions as CI/CD, but you can use any other CI/CD, for example self-hosted free WoodpeckerCI.\\n\\n\x3c!-- truncate --\x3e\\n\\nAssume you have your AdminForth project in `myadmin`.\\n\\n\\n## Step 1 - Dockerfile and .dockerignore\\n\\n\\nThis guide assumes you have created your AdminForth application with latest version of `adminforth create-app` command. \\nThis command already creates a `Dockerfile` and `.dockerignore` for you, so you can use them as is.\\n\\n\\n## Step 2 - compose.yml\\n\\nCreate folder `deploy` and create file `compose.yml` inside:\\n\\n```yml title=\\"deploy/compose.yml\\"\\n\\nservices:\\n  traefik:\\n    image: \\"traefik:v2.5\\"\\n    command:\\n      - \\"--api.insecure=true\\"\\n      - \\"--providers.docker=true\\"\\n      - \\"--entrypoints.web.address=:80\\"\\n    ports:\\n      - \\"80:80\\"\\n    volumes:\\n      - \\"/var/run/docker.sock:/var/run/docker.sock:ro\\"\\n\\n  myadmin:\\n    build: ../myadmin\\n    restart: always\\n    env_file:\\n      - ./myadmin/.env.secrets.prod\\n    volumes:\\n      - myadmin-db:/code/db\\n    labels:\\n      - \\"traefik.enable=true\\"\\n      - \\"traefik.http.routers.myadmin.rule=PathPrefix(`/`)\\"\\n      - \\"traefik.http.services.myadmin.loadbalancer.server.port=3500\\"\\n      - \\"traefik.http.routers.myadmin.priority=2\\"\\n\\nvolumes:\\n  myadmin-db:\\n```\\n\\n## Step 3 - create a SSH keypair\\n\\nMake sure you are in `deploy` folder, run next command here:\\n\\n```bash title=\\"deploy\\"\\nmkdir .keys && ssh-keygen -f .keys/id_rsa -N \\"\\"\\n```\\n\\nNow it should create `deploy/.keys/id_rsa` and `deploy/.keys/id_rsa.pub` files with your SSH keypair. Terraform script will put the public key to the EC2 instance and will use private key to connect to the instance. Also you will be able to use it to connect to the instance manually.\\n\\n## Step 4 - .gitignore file\\n\\nCreate `deploy/.gitignore` file with next content:\\n\\n```bash\\n.terraform/\\n.keys/\\n*.tfstate\\n*.tfstate.*\\n*.tfvars\\ntfplan\\n.env.secrets.prod\\n```\\n\\n## Step 5 - Main terraform file main.tf\\n\\n\\nFirst of all install Terraform as described here [terraform installation](https://developer.hashicorp.com/terraform/install#linux).\\n\\n\\nCreate file `main.tf` in `deploy` folder:\\n\\n```hcl title=\\"deploy/main.tf\\"\\n\\nlocals {\\n  app_name = \\"<your_app_name>\\" # replace with your app name\\n  aws_region = \\"eu-central-1\\"\\n}\\n\\n\\nprovider \\"aws\\" {\\n  region = local.aws_region\\n  profile = \\"myaws\\"\\n}\\n\\ndata \\"aws_ami\\" \\"ubuntu_linux\\" {\\n  most_recent = true\\n  owners      = [\\"amazon\\"]\\n\\n  filter {\\n    name   = \\"name\\"\\n    values = [\\"ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-*\\"]\\n  }\\n}\\n\\ndata \\"aws_vpc\\" \\"default\\" {\\n  default = true\\n}\\n\\nresource \\"aws_eip\\" \\"eip\\" {\\n domain = \\"vpc\\"\\n}\\n\\nresource \\"aws_eip_association\\" \\"eip_assoc\\" {\\n instance_id   = aws_instance.app_instance.id\\n allocation_id = aws_eip.eip.id\\n}\\n\\ndata \\"aws_subnet\\" \\"default_subnet\\" {\\n  filter {\\n    name   = \\"vpc-id\\"\\n    values = [data.aws_vpc.default.id]\\n  }\\n\\n  filter {\\n    name   = \\"default-for-az\\"\\n    values = [\\"true\\"]\\n  }\\n\\n  filter {\\n    name   = \\"availability-zone\\"\\n    values = [\\"${local.aws_region}a\\"]\\n  }\\n}\\n\\nresource \\"aws_security_group\\" \\"instance_sg\\" {\\n  name   = \\"${local.app_name}-instance-sg\\"\\n  vpc_id = data.aws_vpc.default.id\\n\\n  ingress {\\n    description = \\"Allow HTTP\\"\\n    from_port   = 80\\n    to_port     = 80\\n    protocol    = \\"tcp\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n\\n  # SSH\\n  ingress {\\n    description = \\"Allow SSH\\"\\n    from_port   = 22\\n    to_port     = 22\\n    protocol    = \\"tcp\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n\\n  egress {\\n    description = \\"Allow all outbound traffic\\"\\n    from_port   = 0\\n    to_port     = 0\\n    protocol    = \\"-1\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n}\\n\\nresource \\"aws_key_pair\\" \\"app_deployer\\" {\\n  key_name   = \\"terraform-deploy_${local.app_name}-key\\"\\n  public_key = file(\\"./.keys/id_rsa.pub\\") # Path to your public SSH key\\n}\\n\\nresource \\"aws_instance\\" \\"app_instance\\" {\\n  ami                    = data.aws_ami.ubuntu_linux.id\\n  instance_type          = \\"t3a.small\\"\\n  subnet_id              = data.aws_subnet.default_subnet.id\\n  vpc_security_group_ids = [aws_security_group.instance_sg.id]\\n  key_name               = aws_key_pair.app_deployer.key_name\\n\\n  # prevent accidental termination of ec2 instance and data loss\\n  # if you will need to recreate the instance still (not sure why it can be?), you will need to remove this block manually by next command:\\n  # > terraform taint aws_instance.app_instance\\n  lifecycle {\\n    prevent_destroy = true\\n    ignore_changes = [ami]\\n  }\\n\\n  root_block_device {\\n    volume_size = 20 // Size in GB for root partition\\n    volume_type = \\"gp2\\"\\n\\n    # Even if the instance is terminated, the volume will not be deleted, delete it manually if needed\\n    delete_on_termination = false\\n  }\\n\\n  user_data = <<-EOF\\n    #!/bin/bash\\n    sudo apt-get update\\n    sudo apt-get install ca-certificates curl\\n    sudo install -m 0755 -d /etc/apt/keyrings\\n    sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\\n    sudo chmod a+r /etc/apt/keyrings/docker.asc\\n\\n    # Add the repository to Apt sources:\\n    echo \\\\\\n      \\"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\\\\n      $(. /etc/os-release && echo \\"$VERSION_CODENAME\\") stable\\" | \\\\\\n      sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\\n    sudo apt-get update\\n\\n    sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\\n\\n    systemctl start docker\\n    systemctl enable docker\\n    usermod -a -G docker ubuntu\\n\\n    echo \\"done\\" > /home/ubuntu/user_data_done\\n  EOF\\n\\n  tags = {\\n    Name = \\"${local.app_name}-instance\\"\\n  }\\n}\\n\\nresource \\"null_resource\\" \\"wait_for_user_data\\" {\\n  provisioner \\"remote-exec\\" {\\n    inline = [\\n      \\"echo \'Waiting for EC2 software install to finish...\'\\",\\n      \\"while [ ! -f /home/ubuntu/user_data_done ]; do echo \'...\'; sleep 2; done\\",\\n      \\"echo \'EC2 software install finished.\'\\"\\n    ]\\n\\n    connection {\\n      type        = \\"ssh\\"\\n      user        = \\"ubuntu\\"\\n      private_key = file(\\"./.keys/id_rsa\\")\\n      host        = aws_eip_association.eip_assoc.public_ip\\n    }\\n  }\\n\\n  depends_on = [aws_instance.app_instance]\\n}\\n\\nresource \\"null_resource\\" \\"sync_files_and_run\\" {\\n  # Use rsync to exclude node_modules, .git, db\\n  provisioner \\"local-exec\\" {\\n    # heredoc syntax\\n    # remove files that where deleted on the source\\n    command = <<-EOF\\n    #  -o StrictHostKeyChecking=no\\n    rsync -t -av -e \\"ssh -i ./.keys/id_rsa -o StrictHostKeyChecking=no\\" \\\\\\n      --delete \\\\\\n      --exclude \'node_modules\' \\\\\\n      --exclude \'.git\' \\\\\\n      --exclude \'.terraform\' \\\\\\n      --exclude \'terraform*\' \\\\\\n      --exclude \'tfplan\' \\\\\\n      --exclude \'.keys\' \\\\\\n      --exclude \'.vscode\' \\\\\\n      --exclude \'.env\' \\\\\\n      --exclude \'db\' \\\\\\n      ../ ubuntu@${aws_eip_association.eip_assoc.public_ip}:/home/ubuntu/app/\\n    EOF\\n  }\\n\\n  # Run docker compose after files have been copied\\n  provisioner \\"remote-exec\\" {\\n    inline = [\\n      # please note that prune might destroy build cache and make build slower, however it releases disk space\\n      \\"docker system prune -f\\",\\n      # \\"docker buildx prune -f --filter \'type!=exec.cachemount\'\\",\\n      \\"cd /home/ubuntu/app/deploy\\",\\n      \\"COMPOSE_BAKE=true docker compose --progress=plain -p app -f compose.yml up --build -d\\"\\n    ]\\n\\n    connection {\\n      type        = \\"ssh\\"\\n      user        = \\"ubuntu\\"\\n      private_key = file(\\"./.keys/id_rsa\\")\\n      host        = aws_eip_association.eip_assoc.public_ip\\n    }\\n  }\\n\\n  # Ensure the resource is triggered every time based on timestamp or file hash\\n  triggers = {\\n    always_run = timestamp()\\n  }\\n\\n  depends_on = [null_resource.wait_for_user_data, aws_eip_association.eip_assoc]\\n}\\n\\n\\noutput \\"instance_public_ip\\" {\\n  value = aws_eip_association.eip_assoc.public_ip\\n}\\n\\n\\n######### This scetion is for tf state storage ##############\\n\\n# S3 bucket for storing Terraform state\\nresource \\"aws_s3_bucket\\" \\"terraform_state\\" {\\n  bucket = \\"${local.app_name}-terraform-state\\"\\n}\\n\\nresource \\"aws_s3_bucket_lifecycle_configuration\\" \\"terraform_state\\" {\\n  bucket = aws_s3_bucket.terraform_state.bucket\\n\\n  rule {\\n    status = \\"Enabled\\"\\n    id = \\"Keep only the latest version of the state file\\"\\n\\n    filter {\\n      prefix = \\"\\"\\n    }\\n\\n    noncurrent_version_expiration {\\n      noncurrent_days = 30\\n    }\\n  }\\n}\\n\\nresource \\"aws_s3_bucket_versioning\\" \\"terraform_state\\" {\\n  bucket = aws_s3_bucket.terraform_state.bucket\\n\\n  versioning_configuration {\\n    status = \\"Enabled\\"\\n  }\\n}\\n\\nresource \\"aws_s3_bucket_server_side_encryption_configuration\\" \\"terraform_state\\" {\\n  bucket = aws_s3_bucket.terraform_state.bucket\\n\\n  rule {\\n    apply_server_side_encryption_by_default {\\n      sse_algorithm     = \\"AES256\\"\\n    }\\n  }\\n}\\n```\\n\\n> \ud83d\udc46 Replace `<your_app_name>` with your app name (no spaces, only underscores or letters)\\n\\n### Step 5.1 - Configure AWS Profile\\n\\nOpen or create file ~/.aws/credentials and add (if not already there):\\n\\n```ini\\n[myaws]\\naws_access_key_id = <your_access_key>\\naws_secret_access_key = <your_secret_key>\\n```\\n\\n### Step 5.2 - Run deployment\\n\\nTo run the deployment first time, you need to run:\\n\\n```bash\\nterraform init\\n```\\n\\nNow run deployement:\\n\\n```bash\\nterraform apply -auto-approve\\n```\\n\\n## Step 6 - Migrate state to the cloud\\n\\nFirst deployment had to create S3 bucket for storing Terraform state. Now we need to migrate the state to the cloud.\\n\\nAdd to the end of `main.tf`:\\n\\n```hcl title=\\"main.tf\\"\\n\\n# Configure the backend to use the S3 bucket\\nterraform {\\n backend \\"s3\\" {\\n   bucket         = \\"<your_app_name>-terraform-state\\"\\n   key            = \\"state.tfstate\\"  # Define a specific path for the state file\\n   region         = \\"eu-central-1\\"\\n   profile        = \\"myaws\\"\\n   use_lockfile   = true\\n }\\n}\\n```\\n\\n> \ud83d\udc46 Replace `<your_app_name>` with your app name (no spaces, only underscores or letters). \\n> Unfortunately we can\'t use variables, HashiCorp thinks it is too dangerous \ud83d\ude25\\n\\n\\nNow run:\\n\\n```bash\\nterraform init -migrate-state\\n```\\n\\nNow run test deployment:\\n\\n```bash\\nterraform apply -auto-approve\\n```\\n\\nNow you can delete local `terraform.tfstate` file and `terraform.tfstate.backup` file as they are in the cloud now.\\n\\n\\n## Step 7 - CI/CD - Github Actions\\n\\nCreate file `.github/workflows/deploy.yml`:\\n\\n```yml title=\\".github/workflows/deploy.yml\\"\\nname: Deploy \\nrun-name: ${{ github.actor }} builds app \ud83d\ude80\\non: [push]\\njobs:\\n  Explore-GitHub-Actions:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - run: echo \\"\ud83c\udf89 The job was automatically triggered by a ${{ github.event_name }} event.\\"\\n      - run: echo \\"\ud83d\udc27 This job is now running on a ${{ runner.os }} server\\"\\n      - run: echo \\"\ud83d\udd0e The name of your branch is ${{ github.ref }}\\"\\n      - name: Check out repository code\\n        uses: actions/checkout@v4\\n      - name: Set up Terraform\\n        uses: hashicorp/setup-terraform@v2\\n        with:\\n          terraform_version: 1.10.1 \\n      - run: echo \\"\ud83d\udca1 The ${{ github.repository }} repository has been cloned to the runner.\\"\\n      - name: Prepare env\\n        run: |\\n          echo \\"ADMINFORTH_SECRET=$VAULT_ADMINFORTH_SECRET\\" > deploy/.env.secrets.prod\\n        env:\\n          VAULT_ADMINFORTH_SECRET: ${{ secrets.VAULT_ADMINFORTH_SECRET }}\\n      - name: Start building\\n        env:\\n          VAULT_AWS_ACCESS_KEY_ID: ${{ secrets.VAULT_AWS_ACCESS_KEY_ID }}\\n          VAULT_AWS_SECRET_ACCESS_KEY: ${{ secrets.VAULT_AWS_SECRET_ACCESS_KEY }}\\n          VAULT_SSH_PRIVATE_KEY: ${{ secrets.VAULT_SSH_PRIVATE_KEY }}\\n          VAULT_SSH_PUBLIC_KEY: ${{ secrets.VAULT_SSH_PUBLIC_KEY }}\\n        run: |\\n          /bin/sh -x deploy/deploy.sh\\n          \\n      - run: echo \\"\ud83c\udf4f This job\'s status is ${{ job.status }}.\\"\\n```\\n\\n### Step 7.1 - Create deploy script\\n\\nNow create file `deploy/deploy.sh`:\\n\\n```bash title=\\"deploy/deploy.sh\\"\\n\\n# cd to dir of script\\ncd \\"$(dirname \\"$0\\")\\"\\n\\nmkdir -p ~/.aws ./.keys\\n\\ncat <<EOF > ~/.aws/credentials\\n[myaws]\\naws_access_key_id=$VAULT_AWS_ACCESS_KEY_ID\\naws_secret_access_key=$VAULT_AWS_SECRET_ACCESS_KEY\\nEOF\\n\\ncat <<EOF > ./.keys/id_rsa\\n$VAULT_SSH_PRIVATE_KEY\\nEOF\\n\\ncat <<EOF > ./.keys/id_rsa.pub\\n$VAULT_SSH_PUBLIC_KEY\\nEOF\\n\\nchmod 600 ./.keys/id_rsa*\\n\\n# force Terraform to reinitialize the backend without migrating the state.\\nterraform init -reconfigure\\nterraform plan -out=tfplan\\nterraform apply tfplan\\n```\\n\\n### Step 7.2 - Add secrets to GitHub\\n\\nGo to your GitHub repository, then `Settings` -> `Secrets` -> `New repository secret` and add:\\n\\n- `VAULT_AWS_ACCESS_KEY_ID` - your AWS access key\\n- `VAULT_AWS_SECRET_ACCESS_KEY` - your AWS secret key\\n- `VAULT_SSH_PRIVATE_KEY` - make `cat ~/.ssh/id_rsa` and paste to GitHub secrets\\n- `VAULT_SSH_PUBLIC_KEY` - make `cat ~/.ssh/id_rsa.pub` and paste to GitHub secrets\\n- `VAULT_ADMINFORTH_SECRET` - your AdminForth secret - random string, for example `openssl rand -base64 32 | tr -d \'\\\\n\'`\\n\\nNow you can push your changes to GitHub and see how it will be deployed automatically."},{"id":"compose-ec2-deployment","metadata":{"permalink":"/blog/compose-ec2-deployment","source":"@site/blog/2024-10-31-compose-ec2-deployment/index.md","title":"Deploy AdminForth to EC2 with terraform (without CI)","description":"Here is a row snippet to deploy AdminForth to Terraform.","date":"2024-10-31T00:00:00.000Z","tags":[{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"Amazon Web Services (AWS) is a cloud computing platform that provides a wide range of services for building and deploying applications."},{"inline":false,"label":"Terraform","permalink":"/blog/tags/terraform","description":"Terraform is an open-source infrastructure as code software tool created by HashiCorp that enables users to define and provision data center infrastructure using a declarative configuration language."}],"readingTime":3.49,"hasTruncateMarker":true,"authors":[{"name":"Ivan Borshchov","title":"Maintainer of AdminForth","url":"https://github.com/ivictbor","imageURL":"https://avatars.githubusercontent.com/u/1838656?v=4","key":"ivanb","page":null}],"frontMatter":{"slug":"compose-ec2-deployment","title":"Deploy AdminForth to EC2 with terraform (without CI)","authors":"ivanb","tags":["aws","terraform"]},"unlisted":false,"prevItem":{"title":"Deploy AdminForth to EC2 with terraform on CI","permalink":"/blog/compose-ec2-deployment-github-actions"},"nextItem":{"title":"Build AI-Assisted blog with AdminForth and Nuxt in 20 minutes","permalink":"/blog/ai-blog"}},"content":"Here is a row snippet to deploy AdminForth to Terraform.\\n\\n\x3c!-- truncate --\x3e\\n\\n\\nAssume you have your AdminForth project in `myadmin` created with `adminforth create-app` command. This command already creates a `Dockerfile` and `.dockerignore` for you, so you can use them as is.\\n\\n\\nCreate file `compose.yml`:\\n\\n```yml title=\\"compose.yml\\"\\n\\nservices:\\n  traefik:\\n    image: \\"traefik:v2.5\\"\\n    command:\\n      - \\"--api.insecure=true\\"\\n      - \\"--providers.docker=true\\"\\n      - \\"--entrypoints.web.address=:80\\"\\n    ports:\\n      - \\"80:80\\"\\n    volumes:\\n      - \\"/var/run/docker.sock:/var/run/docker.sock:ro\\"\\n\\n  myadmin:\\n    build: ./myadmin\\n    restart: always\\n    env_file:\\n      - ./myadmin/.env\\n    volumes:\\n      - myadmin-db:/code/db\\n    labels:\\n      - \\"traefik.enable=true\\"\\n      - \\"traefik.http.routers.myadmin.rule=PathPrefix(`/`)\\"\\n      - \\"traefik.http.services.myadmin.loadbalancer.server.port=3500\\"\\n      - \\"traefik.http.routers.myadmin.priority=2\\"\\n\\nvolumes:\\n  myadmin-db:\\n```\\n\\n\\n\\nCreate file `main.tf`:\\n\\n```hcl title=\\"main.tf\\"\\n\\nprovider \\"aws\\" {\\n  region = \\"eu-central-1\\"\\n  profile = \\"myaws\\"\\n}\\n\\ndata \\"aws_ami\\" \\"ubuntu_linux\\" {\\n  most_recent = true\\n  owners      = [\\"amazon\\"]\\n\\n  filter {\\n    name   = \\"name\\"\\n    values = [\\"ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-*\\"]\\n  }\\n}\\n\\ndata \\"aws_vpc\\" \\"default\\" {\\n  default = true\\n}\\n\\nresource \\"aws_eip\\" \\"eip\\" {\\n vpc = true\\n}\\nresource \\"aws_eip_association\\" \\"eip_assoc\\" {\\n instance_id   = aws_instance.myadmin_instance.id\\n allocation_id = aws_eip.eip.id\\n}\\n\\ndata \\"aws_subnet\\" \\"default_subnet\\" {\\n  filter {\\n    name   = \\"vpc-id\\"\\n    values = [data.aws_vpc.default.id]\\n  }\\n\\n  filter {\\n    name   = \\"default-for-az\\"\\n    values = [\\"true\\"]\\n  }\\n\\n  filter {\\n    name   = \\"availability-zone\\"\\n    values = [\\"eu-central-1a\\"]\\n  }\\n}\\n\\n\\nresource \\"aws_security_group\\" \\"instance_sg\\" {\\n  name   = \\"myadmin-instance-sg\\"\\n  vpc_id = data.aws_vpc.default.id\\n\\n  ingress {\\n    description = \\"Allow HTTP\\"\\n    from_port   = 80\\n    to_port     = 80\\n    protocol    = \\"tcp\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n\\n  # SSH\\n  ingress {\\n    description = \\"Allow SSH\\"\\n    from_port   = 22\\n    to_port     = 22\\n    protocol    = \\"tcp\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n\\n  egress {\\n    description = \\"Allow all outbound traffic\\"\\n    from_port   = 0\\n    to_port     = 0\\n    protocol    = \\"-1\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n}\\n\\nresource \\"aws_key_pair\\" \\"myadmin_deploy_key\\" {\\n  key_name   = \\"terraform-myadmin_deploy_key-key\\"\\n  public_key = file(\\"~/.ssh/id_rsa.pub\\") # Path to your public SSH key\\n}\\n\\n\\nresource \\"aws_instance\\" \\"myadmin_instance\\" {\\n  ami                    = data.aws_ami.ubuntu_linux.id\\n  instance_type          = \\"t3a.small\\"\\n  subnet_id              = data.aws_subnet.default_subnet.id\\n  vpc_security_group_ids = [aws_security_group.instance_sg.id]\\n  key_name               = aws_key_pair.myadmin_deploy_key.key_name\\n\\n  # prevent accidental termination of ec2 instance and data loss\\n  # if you will need to recreate the instance still (not sure why it can be?), you will need to remove this block manually by next command:\\n  # > terraform taint aws_instance.app_instance\\n  lifecycle {\\n    prevent_destroy = true\\n    ignore_changes = [ami]\\n  }\\n\\n\\n  root_block_device {\\n    volume_size = 20 // Size in GB for root partition\\n    volume_type = \\"gp2\\"\\n\\n    # Even if the instance is terminated, the volume will not be deleted, delete it manually if needed\\n    delete_on_termination = false\\n  }\\n\\n  user_data = <<-EOF\\n    #!/bin/bash\\n    sudo apt-get update\\n    sudo apt-get install ca-certificates curl\\n    sudo install -m 0755 -d /etc/apt/keyrings\\n    sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\\n    sudo chmod a+r /etc/apt/keyrings/docker.asc\\n\\n    # Add the repository to Apt sources:\\n    echo \\\\\\n      \\"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\\\\n      $(. /etc/os-release && echo \\"$VERSION_CODENAME\\") stable\\" | \\\\\\n      sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\\n    sudo apt-get update\\n\\n    sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\\n\\n    systemctl start docker\\n    systemctl enable docker\\n    usermod -a -G docker ubuntu\\n\\n    echo \\"done\\" > /home/ubuntu/user_data_done\\n  EOF\\n\\n  tags = {\\n    Name = \\"myadmin-instance\\"\\n  }\\n}\\n\\nresource \\"null_resource\\" \\"wait_for_user_data\\" {\\n  provisioner \\"remote-exec\\" {\\n    inline = [\\n      \\"echo \'Waiting for EC2 software install to finish...\'\\",\\n      \\"while [ ! -f /home/ubuntu/user_data_done ]; do echo \'...\'; sleep 2; done\\",\\n      \\"echo \'EC2 software install finished.\'\\"\\n    ]\\n\\n    connection {\\n      type        = \\"ssh\\"\\n      user        = \\"ubuntu\\"\\n      private_key = file(\\"./.keys/id_rsa\\")\\n      host        = aws_eip_association.eip_assoc.public_ip\\n    }\\n  }\\n\\n  depends_on = [aws_instance.app_instance]\\n}\\n\\nresource \\"null_resource\\" \\"sync_files_and_run\\" {\\n  # Use rsync to exclude node_modules, .git, db\\n  provisioner \\"local-exec\\" {\\n    # heredoc syntax\\n    command = <<-EOF\\n    rsync -t -av \\\\\\n      --delete \\\\\\n      --exclude \'node_modules\' \\\\\\n      --exclude \'.git\' \\\\\\n      --exclude \'.terraform\' \\\\\\n      --exclude \'terraform*\' \\\\\\n      --exclude \'tfplan\' \\\\\\n      --exclude \'.keys\' \\\\\\n      --exclude \'.vscode\' \\\\\\n      --exclude \'.env\' \\\\\\n      --exclude \'db\' \\\\\\n      ./ ubuntu@${aws_eip_association.eip_assoc.public_ip}:/home/ubuntu/app/\\n    EOF\\n    \\n  }\\n\\n  # Run docker compose after files have been copied\\n  provisioner \\"remote-exec\\" {\\n    inline = [\\n      # -a would destroy cache\\n      \\"docker system prune -f\\",\\n      \\"cd /home/ubuntu/app/\\",\\n      \\"COMPOSE_BAKE=true docker compose --progress=plain -p app -f compose.yml up --build -d\\"\\n    ]\\n\\n    connection {\\n      type        = \\"ssh\\"\\n      user        = \\"ubuntu\\"\\n      private_key = file(\\"~/.ssh/id_rsa\\")\\n      host        = aws_eip_association.eip_assoc.public_ip\\n    }\\n  }\\n\\n  # Ensure the resource is triggered every time based on timestamp or file hash\\n  triggers = {\\n    always_run = timestamp()\\n  }\\n\\n  depends_on = [null_resource.wait_for_user_data, aws_eip_association.eip_assoc]\\n}\\n\\n\\noutput \\"instance_public_ip\\" {\\n  value = aws_eip_association.eip_assoc.public_ip\\n}\\n```\\n\\n\\nTo run the deployment first time, you need to run:\\n\\n```bash\\nterraform init\\n```\\n\\nThen with any change in code:\\n\\n```bash\\nterraform apply -auto-approve\\n```"},{"id":"ai-blog","metadata":{"permalink":"/blog/ai-blog","source":"@site/blog/2024-10-01-ai-blog/index.md","title":"Build AI-Assisted blog with AdminForth and Nuxt in 20 minutes","description":"Many developers today are using copilots to write code faster and relax their minds from a routine tasks.","date":"2024-10-01T00:00:00.000Z","tags":[{"inline":false,"label":"Nuxt.js","permalink":"/blog/tags/nuxt","description":"Nuxt.js is a free and open-source web application framework based on Vue.js, Node.js, Webpack, and Babel.js."},{"inline":false,"label":"ChatGPT","permalink":"/blog/tags/chatgpt","description":"ChatGPT is a conversational AI model that can generate human-like responses to text inputs."}],"readingTime":19.01,"hasTruncateMarker":true,"authors":[{"name":"Ivan Borshchov","title":"Maintainer of AdminForth","url":"https://github.com/ivictbor","imageURL":"https://avatars.githubusercontent.com/u/1838656?v=4","key":"ivanb","page":null}],"frontMatter":{"slug":"ai-blog","title":"Build AI-Assisted blog with AdminForth and Nuxt in 20 minutes","authors":"ivanb","tags":["nuxt","chatgpt"]},"unlisted":false,"prevItem":{"title":"Deploy AdminForth to EC2 with terraform (without CI)","permalink":"/blog/compose-ec2-deployment"},"nextItem":{"title":"Chat-GPT plugin to co-write texts and strings","permalink":"/blog/chatgpt-plugin"}},"content":"Many developers today are using copilots to write code faster and relax their minds from a routine tasks.\\n\\nBut what about writing plain text? For example blogs and micro-blogs: sometimes you want to share your progress but you are lazy for typing. Then you can give a try to AI-assisted blogging. Our Open-Source AdminForth framework has couple of new AI-capable plugins to write text and generate images.\\n\\n![alt text](nuxtBlog.gif)\\n\\n\x3c!-- truncate --\x3e\\n\\n\\nFor AI plugins are backed by OpenAI API, but their architecture allows to be easily extended for other AI providers once OpenAI competitors will reach the same or better level of quality.\\n\\nHere we will suggest you simple as 1-2-3 steps to build and host a blog with AI assistant which will help you to write posts.\\n\\nOur tech stack will include:\\n\\n- [Nuxt.js](https://nuxt.com/) - SEO-friendly page rendering framework\\n- [AdminForth](https://adminforth.dev/) - Admin panel framework for creating posts\\n- [AdminForth RichEditor plugin](https://adminforth.dev/docs/tutorial/Plugins/RichEditor/) - WYSIWYG editor with AI assistant in Copilot style\\n- Node and typescript\\n- Prisma for migrations\\n- SQLite for database, though you can easily switch it to Postgres or MongoDB\\n\\n## Prerequirements\\n\\nWe will use Node v20, if you not have it installed, we recommend [NVM](https://github.com/nvm-sh/nvm?tab=readme-ov-file#install--update-script)\\n\\n```bash\\nnvm install 20\\nnvm alias default 20\\nnvm use 20\\n```\\n\\n## Step 1: Create a new AdminForth project\\n\\n```bash\\nnpx adminforth create-app --app-name ai-blog\\n```\\n\\nAdd modules:\\n\\n```bash\\ncd ai-blog\\nnpm i @adminforth/upload @adminforth/rich-editor @adminforth/text-complete @adminforth/chat-gpt slugify http-proxy @adminforth/image-generation-adapter-openai @adminforth/completion-adapter-open-ai-chat-gpt\\n```\\n\\n\\n## Step 2: Prepare environment\\n\\n### OpenAI\\n\\nTo allocate OpenAI API key, go to https://platform.openai.com/, open Dashboard -> API keys -> Create new secret key.\\n\\n### S3\\n\\n1. Go to https://aws.amazon.com and login.\\n2. Go to Services -> S3 and create a bucket. Put in bucket name e.g. `my-ai-blog-bucket`. \\nFirst of all go to your bucket settings, Permissions, scroll down to Block public access (bucket settings for this bucket) and uncheck all checkboxes.\\nGo to bucket settings, Permissions, Object ownership and select \\"ACLs Enabled\\" and \\"Bucket owner preferred\\" radio buttons.\\n3. Go to bucket settings, Permissions, scroll down to Cross-origin resource sharing (CORS) and put in the following configuration:\\n\\n```json\\n[\\n    {\\n        \\"AllowedHeaders\\": [\\n            \\"*\\"\\n        ],\\n        \\"AllowedMethods\\": [\\n            \\"HEAD\\",\\n            \\"PUT\\"\\n        ],\\n        \\"AllowedOrigins\\": [\\n            \\"http://localhost:3500\\"\\n        ],\\n        \\"ExposeHeaders\\": []\\n    }\\n]\\n```\\n\\n> \u261d\ufe0f In AllowedOrigins add all your domains. For example if you will serve blog and admin on `https://blog.example.com/` you should add \\n> `\\"https://blog.example.com\\"` to AllowedOrigins:\\n>\\n> ```json\\n> [\\n>      \\"https://blog.example.com\\",\\n>      \\"http://localhost:3500\\"\\n> ]\\n> ```\\n> Every character matters, so don\'t forget to add `http://` or `https://` and don\'t add slashes at the end of the domain.\\n\\n4. Go to Services -> IAM and create a new user. Put in user name e.g. `my-ai-blog-bucket`.\\n5. Attach existing policies directly -> `AmazonS3FullAccess`. Go to your user -> `Add permissions` -> `Attach policies directly` -> `AmazonS3FullAccess`\\n6. Go to Security credentials and create a new access key. Save `Access key ID` and `Secret access key`.\\n\\n\\n### Edit .env file in project directory\\n\\nCreate `.env` file with the following content:\\n\\n```bash title=\\".env\\"\\nOPENAI_API_KEY=...\\nAWS_ACCESS_KEY_ID=your_access_key_id\\nAWS_SECRET_ACCESS_KEY=your_secret_access_key\\n```\\n\\nEdit `.env.local` file and add:\\n\\n```bash title=\\".env.local\\"\\nAWS_S3_BUCKET=my-ai-blog-bucket\\nAWS_S3_REGION=us-east-1\\n```\\n\\nIn same way edit `.env.prod` file and add:\\n\\n```bash title=\\".env.prod\\"\\nAWS_S3_BUCKET=my-ai-blog-bucket\\nAWS_S3_REGION=us-east-1\\n```\\n\\n\\n\\n## Step 3: Add prisma models\\n\\nOpen `./schema.prisma` and put next content there:\\n\\n\\n```yaml title=\\"./schema.prisma\\" \\ngenerator client {\\n  provider = \\"prisma-client-js\\"\\n}\\n\\ndatasource db {\\n  provider = \\"sqlite\\"\\n  url      = env(\\"PRISMA_DATABASE_URL\\")\\n}\\n\\nmodel adminuser {\\n  id            String     @id\\n  email         String     @unique\\n  password_hash String\\n  role          String\\n  created_at    DateTime\\n//diff-add\\n  avatar       String?\\n//diff-add\\n  public_name  String?\\n//diff-add\\n  posts        Post[]\\n}\\n\\n//diff-add\\nmodel Post {\\n//diff-add\\n  id          String     @id\\n//diff-add\\n  createdAt   DateTime \\n//diff-add\\n  title       String\\n//diff-add\\n  slug        String\\n//diff-add\\n  picture     String?\\n//diff-add\\n  content     String\\n//diff-add\\n  published   Boolean  \\n//diff-add\\n  author      adminuser?    @relation(fields: [authorId], references: [id])\\n//diff-add\\n  authorId    String?\\n//diff-add\\n  contentImages ContentImage[]\\n//diff-add\\n}\\n\\n//diff-add\\nmodel ContentImage {\\n//diff-add\\n  id         String     @id\\n//diff-add\\n  createdAt  DateTime \\n//diff-add\\n  img        String\\n//diff-add\\n  postId     String\\n//diff-add\\n  resourceId String\\n//diff-add\\n  post       Post      @relation(fields: [postId], references: [id])\\n//diff-add\\n}\\n```\\n\\nCreate a migration:\\n\\n```bash\\nnpm run makemigration -- --name add-posts ; npm run migrate:local\\n```\\n\\n\\n## Step 4: Setting up AdminForth\\n\\n\\n\\nOpen `index.ts` file in root directory and update it with the following content:\\n\\n```ts title=\\"./index.ts\\"\\nimport express from \'express\';\\nimport AdminForth, { Filters, Sorts } from \'adminforth\';\\nimport userResource from \'./resources/adminuser.js\';\\nimport postResource from \'./resources/posts.js\';\\nimport contentImageResource from \'./resources/content-image.js\';\\nimport httpProxy from \'http-proxy\';\\n\\ndeclare var process : {\\n  env: {\\n    DATABASE_URL: string\\n    NODE_ENV: string,\\n    AWS_S3_BUCKET: string,\\n    AWS_S3_REGION: string,\\n  }\\n  argv: string[]\\n}\\n\\nexport const admin = new AdminForth({\\n  baseUrl: \'/admin\',\\n  auth: {\\n    usersResourceId: \'adminuser\',  // resource to get user during login\\n    usernameField: \'email\',  // field where username is stored, should exist in resource\\n    passwordHashField: \'password_hash\',\\n  },\\n  customization: {\\n    brandName: \'My Admin\',\\n    datesFormat: \'D MMM\',\\n    timeFormat: \'HH:mm\',\\n    emptyFieldPlaceholder: \'-\',\\n    styles: {\\n      colors: {\\n        light: {\\n          // color for links, icons etc.\\n          primary: \'rgb(47 37 227)\',\\n          // color for sidebar and text\\n          sidebar: {main:\'#EFF5F7\', text:\'#333\'},\\n        },\\n      }\\n    }\\n  },\\n  dataSources: [{\\n    id: \'maindb\',\\n    url:  process.env.DATABASE_URL?.replace(\'file:\', \'sqlite://\'),\\n  }],\\n  resources: [\\n    userResource,\\n    postResource,\\n    contentImageResource,\\n  ],\\n  menu: [\\n    {\\n      homepage: true,\\n      label: \'Posts\',\\n      icon: \'flowbite:home-solid\',\\n      resourceId: \'post\',\\n    },\\n    { type: \'gap\' },\\n    { type: \'divider\' },\\n    { type: \'heading\', label: \'SYSTEM\' },\\n    {\\n      label: \'Users\',\\n      icon: \'flowbite:user-solid\',\\n      resourceId: \'adminuser\',\\n    }\\n  ],\\n});\\n\\n\\nif (import.meta.url === `file://${process.argv[1]}`) {\\n  // if script is executed directly e.g. node index.ts or npm start\\n\\n  const app = express()\\n  app.use(express.json());\\n  const port = 3500;\\n\\n  admin.bundleNow({ hotReload: process.env.NODE_ENV === \'development\' }).then(() => {\\n    console.log(\'Bundling AdminForth SPA done.\');\\n  });\\n\\n  // api to server recent posts\\n  app.get(\'/api/posts\', async (req, res) => {\\n    const offset = parseInt(req.query.offset as string) || 0;\\n    const limit = parseInt(req.query.limit as string) || 100;\\n    const slug = req.query.slug as string | null;\\n    const posts = await admin.resource(\'post\').list(\\n      [Filters.EQ(\'published\', true), ...(slug ? [Filters.LIKE(\'slug\', slug)] : [])],\\n      limit,\\n      offset,\\n      Sorts.DESC(\'createdAt\'),\\n    );\\n    const authorIds = [...new Set(posts.map((p: any) => p.authorId))];\\n    const authors = (await admin.resource(\'adminuser\').list(Filters.IN(\'id\', authorIds)))\\n      .reduce((acc: any, a: any) => {acc[a.id] = a; return acc;}, {});\\n    posts.forEach((p: any) => {\\n      const author = authors[p.authorId];\\n      p.author = { \\n        publicName: author.publicName, \\n        avatar: `https://${process.env.AWS_S3_BUCKET}.s3.${process.env.AWS_S3_REGION}.amazonaws.com/${author.avatar}`\\n      };\\n      p.picture = `https://${process.env.AWS_S3_BUCKET}.s3.${process.env.AWS_S3_REGION}.amazonaws.com/${p.picture}`;\\n    });\\n    res.json(posts);\\n  });\\n\\n  // here we proxy all non-/admin requests to nuxt instance http://localhost:3000\\n  // this is done for demo purposes, in production you should do this using high-performance reverse proxy like traefik or nginx\\n  app.use((req, res, next) => {\\n    if (!req.url.startsWith(\'/admin\')) {\\n      const proxy = httpProxy.createProxyServer();\\n      proxy.on(\'error\', function (err, req, res) {\\n        res.send(`No response from Nuxt at http://localhost:3000, did you start it? ${err}`)\\n      });\\n      proxy.web(req, res, { target: \'http://localhost:3000\' });\\n    } else {\\n      next();\\n    }\\n  });\\n\\n  // serve after you added all api\\n  admin.express.serve(app)\\n\\n  admin.discoverDatabases().then(async () => {\\n    if (!await admin.resource(\'adminuser\').get([Filters.EQ(\'email\', \'adminforth@adminforth.dev\')])) {\\n      await admin.resource(\'adminuser\').create({\\n        email: \'adminforth@adminforth.dev\',\\n        role: \'superadmin\',\\n        password_hash: await AdminForth.Utils.generatePasswordHash(\'adminforth\'),\\n      });\\n    }\\n  });\\n\\n  admin.express.listen(port, () => {\\n    console.log(`\\\\n\u26a1 AdminForth is available at http://localhost:${port}/admin\\\\n`)\\n  });\\n}\\n```\\n\\n## Step 5: Edit resources\\n\\nOpen `./resources/adminuser.ts` file with following content:\\n\\n```ts title=\\"./resources/adminuser.ts\\"\\nimport AdminForth, { AdminForthDataTypes } from \'adminforth\';\\nimport { randomUUID } from \'crypto\';\\nimport UploadPlugin from \'@adminforth/upload\';\\n\\nexport default {\\n  dataSource: \'maindb\',\\n  table: \'adminuser\',\\n  label: \'Users\',\\n  recordLabel: (r: any) => `\ud83d\udc64 ${r.email}`,\\n  columns: [\\n    {\\n      name: \'id\',\\n      primaryKey: true,\\n      fillOnCreate: () => randomUUID(),\\n      showIn: {\\n        edit: false,\\n        create: false,\\n      },\\n    },\\n    {\\n      name: \'email\',\\n      required: true,\\n      isUnique: true,\\n      enforceLowerCase: true,\\n      validation: [\\n        AdminForth.Utils.EMAIL_VALIDATOR,\\n      ],\\n      type: AdminForthDataTypes.STRING,\\n    },\\n    {\\n      name: \'role\',\\n      enum: [\\n        { value: \'superadmin\', label: \'Super Admin\' },\\n        { value: \'user\', label: \'User\' },\\n      ]\\n    },\\n    {\\n      name: \'created_at\',\\n      type: AdminForthDataTypes.DATETIME,\\n      showIn: {\\n        edit: false,\\n        create: false,\\n      },\\n      fillOnCreate: () => (new Date()).toISOString(),\\n    },\\n    {\\n      name: \'password\',\\n      virtual: true,\\n      required: { create: true },\\n      editingNote: { edit: \'Leave empty to keep password unchanged\' },\\n      minLength: 8,\\n      type: AdminForthDataTypes.STRING,\\n      showIn: {\\n        show: false,\\n        list: false,\\n        filter: false,\\n      },\\n      masked: true,\\n      validation: [\\n        // request to have at least 1 digit, 1 upper case, 1 lower case\\n        AdminForth.Utils.PASSWORD_VALIDATORS.UP_LOW_NUM,\\n      ],\\n    },\\n    { name: \'password_hash\', backendOnly: true, showIn: { all: false } },\\n    { \\n      name: \'public_name\',\\n      type: AdminForthDataTypes.STRING,\\n    },\\n    { name: \'avatar\' },\\n  ],\\n  hooks: {\\n    create: {\\n      beforeSave: async ({ record, adminUser, resource }) => {\\n        record.passwordHash = await AdminForth.Utils.generatePasswordHash(record.password);\\n        return { ok: true };\\n      }\\n    },\\n    edit: {\\n      beforeSave: async ({ record, adminUser, resource }) => {\\n        if (record.password) {\\n          record.passwordHash = await AdminForth.Utils.generatePasswordHash(record.password);\\n        }\\n        return { ok: true }\\n      },\\n    },\\n  },\\n  plugins: [\\n    new UploadPlugin({\\n      pathColumnName: \'avatar\',\\n      s3Bucket: process.env.AWS_S3_BUCKET,\\n      s3Region: process.env.AWS_S3_REGION,\\n      allowedFileExtensions: [\'jpg\', \'jpeg\', \'png\', \'gif\', \'webm\',\'webp\'],\\n      maxFileSize: 1024 * 1024 * 20, // 20MB\\n      s3AccessKeyId: process.env.AWS_ACCESS_KEY_ID,\\n      s3SecretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,\\n      s3ACL: \'public-read\', // ACL which will be set to uploaded file\\n      s3Path: (\\n        { originalFilename, originalExtension }: {originalFilename: string, originalExtension: string }\\n      ) => `user-avatars/${new Date().getFullYear()}/${randomUUID()}/${originalFilename}.${originalExtension}`,\\n      generation: {\\n        provider: \'openai\',\\n        countToGenerate: 2,\\n        openAiOptions: {\\n          model: \'gpt-4o\',\\n          apiKey: process.env.OPENAI_API_KEY,\\n        },\\n      },\\n    }),\\n  ],\\n}\\n```\\n\\n\\nCreate `posts.ts` file in res directory with following content:\\n\\n```ts title=\\"./resources/post.ts\\"\\nimport { AdminUser, AdminForthDataTypes } from \'adminforth\';\\nimport { randomUUID } from \'crypto\';\\nimport UploadPlugin from \'@adminforth/upload\';\\nimport RichEditorPlugin from \'@adminforth/rich-editor\';\\nimport ChatGptPlugin from \'@adminforth/chat-gpt\';\\nimport slugify from \'slugify\';\\nimport CompletionAdapterOpenAIChatGPT from \\"@adminforth/completion-adapter-open-ai-chat-gpt\\";\\nimport ImageGenerationAdapterOpenAI from \'@adminforth/image-generation-adapter-openai\';\\n\\nexport default {\\n  table: \'post\',\\n  dataSource: \'maindb\',\\n  label: \'Posts\',\\n  recordLabel: (r: any) => `\ud83d\udcdd ${r.title}`,\\n  columns: [\\n    {\\n      name: \'id\',\\n      primaryKey: true,\\n      fillOnCreate: () => randomUUID(),\\n      showIn: {\\n        list: false,\\n        edit: false,\\n        create: false,\\n      },\\n    },\\n    {\\n      name: \'title\',\\n      required: true,\\n      showIn: { all: true },\\n      maxLength: 255,\\n      minLength: 3,\\n      type: AdminForthDataTypes.STRING,\\n    },\\n    {\\n      name: \'picture\',\\n      showIn: { all: true },\\n    },\\n    {\\n      name: \'slug\',\\n      showIn: {\\n        list: false,\\n        edit: false,\\n        create: false,\\n      },\\n    },\\n    {\\n      name: \'content\',\\n      showIn: { list: false },\\n      type: AdminForthDataTypes.TEXT,\\n      components: {\\n        show: \\"@/renderers/RichText.vue\\",\\n      }\\n    },\\n    {\\n      name: \'createdAt\',\\n      showIn: {\\n        edit: false,\\n        create: false,\\n      },\\n      fillOnCreate: () => (new Date()).toISOString(),\\n    },\\n    {\\n      name: \'published\',\\n      required: true,\\n    },\\n    {\\n      name: \'authorId\',\\n      foreignResource: {\\n        resourceId: \'adminuser\',\\n      },\\n      showIn: {\\n        list: false,\\n        edit: false,\\n        create: false,\\n      },\\n      fillOnCreate: ({ adminUser }: { adminUser: AdminUser }) => {\\n        return adminUser.dbUser.id;\\n      }\\n    }\\n  ],\\n  hooks: {\\n    create: {\\n      beforeSave: async ({ record, adminUser }: { record: any, adminUser: AdminUser }) => {\\n        record.slug = slugify(record.title, { lower: true });\\n        return { ok: true };\\n      },\\n    },\\n    edit: {\\n      beforeSave: async ({ record, adminUser }: { record: any, adminUser: AdminUser }) => {\\n        if (record.title) {\\n          record.slug = slugify(record.title, { lower: true });\\n        }\\n        return { ok: true };\\n      },\\n    },\\n  },\\n  plugins: [\\n    new UploadPlugin({\\n      pathColumnName: \'picture\',\\n      s3Bucket: process.env.AWS_S3_BUCKET,\\n      s3Region: process.env.AWS_S3_REGION,\\n      allowedFileExtensions: [\'jpg\', \'jpeg\', \'png\', \'gif\', \'webm\',\'webp\'],\\n      maxFileSize: 1024 * 1024 * 20, // 20MB\\n      s3AccessKeyId: process.env.AWS_ACCESS_KEY_ID,\\n      s3SecretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,\\n      s3ACL: \'public-read\', // ACL which will be set to uploaded file\\n      s3Path: (\\n        { originalFilename, originalExtension }: {originalFilename: string, originalExtension: string }\\n      ) => `post-previews/${new Date().getFullYear()}/${randomUUID()}/${originalFilename}.${originalExtension}`,\\n      generation: {\\n        countToGenerate: 2,\\n        adapter: new ImageGenerationAdapterOpenAI({\\n          openAiApiKey: process.env.OPENAI_API_KEY as string,\\n          model: \'gpt-image-1\', \\n        }),\\n        fieldsForContext: [\'title\'],\\n        outputSize: \'1536x1024\'\\n      },\\n    }),\\n    new RichEditorPlugin({\\n      htmlFieldName: \'content\',\\n      completion: {\\n        adapter: new CompletionAdapterOpenAIChatGPT({\\n          openAiApiKey: process.env.OPENAI_API_KEY as string,\\n          model: \'gpt-4o\',\\n          expert: {\\n            temperature: 0.7\\n          }\\n        }),\\n        expert: {\\n          debounceTime: 250,\\n        }\\n      }, \\n      attachments: {\\n        attachmentResource: \'contentImage\',\\n        attachmentFieldName: \'img\',\\n        attachmentRecordIdFieldName: \'postId\',\\n        attachmentResourceIdFieldName: \'resourceId\',\\n      },\\n    }),\\n    new ChatGptPlugin({\\n      openAiApiKey: process.env.OPENAI_API_KEY,\\n      model: \'gpt-4o\',\\n      fieldName: \'title\',\\n      expert: {\\n        debounceTime: 250,\\n      }\\n    }),\\n  ]\\n}\\n```\\n\\nAlso create `content-image.ts` file in `res` directory with following content:\\n\\n```ts title=\\"./resources/content-image.ts\\"\\n\\nimport { AdminForthDataTypes } from \'adminforth\';\\nimport { randomUUID } from \'crypto\';\\nimport UploadPlugin from \'@adminforth/upload\';\\n\\nexport default {\\n  table: \'contentImage\',\\n  dataSource: \'maindb\',\\n  label: \'Content Images\',\\n  recordLabel: (r: any) => `\ud83d\uddbc\ufe0f ${r.img}`,\\n  columns: [\\n    {\\n      name: \'id\',\\n      primaryKey: true,\\n      showIn: {\\n        edit: false,\\n        create: false,\\n      },\\n      fillOnCreate: () => randomUUID(),\\n    },\\n    {\\n      name: \'createdAt\',\\n      type: AdminForthDataTypes.DATETIME,\\n      showIn: {\\n        edit: false,\\n        create: false,\\n      },\\n      fillOnCreate: () => (new Date()).toISOString(),\\n    },\\n    {\\n      name: \'img\',\\n      type: AdminForthDataTypes.STRING,\\n      required: true,\\n    },\\n    {\\n      name: \'postId\',\\n      foreignResource: {\\n        resourceId: \'post\',\\n      },\\n      showIn: {\\n        edit: false,\\n        create: false,\\n      },\\n    },\\n    {\\n      name: \'resourceId\',\\n    }\\n  ],\\n  plugins: [\\n    new UploadPlugin({\\n      pathColumnName: \'img\',\\n      s3Bucket: process.env.AWS_S3_BUCKET,\\n      s3Region: process.env.AWS_S3_REGION,\\n      allowedFileExtensions: [\'jpg\', \'jpeg\', \'png\', \'gif\', \'webm\',\'webp\'],\\n      maxFileSize: 1024 * 1024 * 20, // 20MB\\n      s3AccessKeyId: process.env.AWS_ACCESS_KEY_ID,\\n      s3SecretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,\\n      s3ACL: \'public-read\', // ACL which will be set to uploaded file\\n      s3Path: (\\n        { originalFilename, originalExtension }: {originalFilename: string, originalExtension: string }\\n      ) => `post-content/${new Date().getFullYear()}/${randomUUID()}/${originalFilename}.${originalExtension}`,\\n    }),\\n  ],\\n}\\n```\\n\\nNow you can start your admin panel:\\n\\n```bash\\nnpm run dev\\n```\\n\\nOpen `http://localhost:3500/admin` in your browser and login with `adminforth@adminforth.dev` and `adminforth` credentials.\\nSet up your avatar (you can generate it with AI) and public name in user settings.\\n\\n![alt text](aiblogpost.png)\\n\\n## Step 6: Create Nuxt project\\n\\nNow let\'s initialize our seo-facing frontend.\\nIn the root directory of your admin app (`ai-blog`) and create a new folder `seo` and run:\\n\\n```bash\\nnpx nuxi@latest init seo\\ncd seo\\nnpm install -D sass-embedded\\nnpm run dev\\n```\\n\\nEdit `app.vue`: \\n\\n```html title=\\"./seo/app.vue\\"\\n<template>\\n  <div id=\\"app\\">\\n    <NuxtPage />\\n  </div>\\n</template>\\n\\n\\n<style lang=\\"scss\\">\\n\\n$grColor1: #74E1FF;\\n$grColor2: #8580B4;\\n$grColor3: #5E53C3;\\n$grColor4: #4FC7E9;\\n$grColor5: #695BE9;\\n\\n  #app {\\n    font-family: Avenir, Helvetica, Arial, sans-serif;\\n    -webkit-font-smoothing: antialiased;\\n    -moz-osx-font-smoothing: grayscale;\\n    // gradient with color spots\\n    animation: gradient 15s ease infinite;\\n    min-height: 100dvh;\\n  }\\n  body {\\n    margin: 0;\\n    padding: 0;\\n    max-height: 100dvh;\\n    overflow: overlay;\\n    background-image: radial-gradient(\\n  circle farthest-corner at top left, $grColor1 0%, rgba(225, 243, 97,0) 50%),\\n      radial-gradient(\\n      circle farthest-side at top right, $grColor2 0%, rgba(181, 176, 177,0) 10%),\\n      radial-gradient(circle farthest-corner at bottom right, $grColor3 0%, rgba(204, 104, 119, 0) 33%),\\n      radial-gradient(\\n          circle farthest-corner at top right, $grColor4 0%, rgba(155, 221, 240,0) 50%),\\n      radial-gradient(ellipse at bottom center, $grColor5 0%, rgba(254, 43, 0, 0) 80%); \\n    background-attachment: fixed;\\n  }\\n</style>\\n```\\n\\n\\nAdd folder `pages` and create `index.vue`:\\n\\n```html title=\\"./seo/pages/index.vue\\"\\n<template>\\n  <div class=\\"container\\">\\n    <PostCard \\n      v-for=\\"post in posts\\" \\n      :key=\\"post.id\\" \\n      :post=\\"post\\"\\n    />\\n    <div class=\\"no-posts\\" v-if=\\"!posts.length\\">\\n      No posts added yet\\n      <a href=\\"/admin\\">Add a first one in admin</a>\\n    </div>\\n  </div>\\n</template>\\n\\n<style lang=\\"scss\\">\\n.container {\\n  display: flex;\\n  justify-content: center;\\n  align-items: center;\\n  flex-wrap: wrap;\\n  flex-direction: column;\\n  gap: 1rem;\\n  padding-top: 2rem;\\n}\\n\\n.no-posts {\\n  margin-top: 2rem;\\n  font-size: 1.5rem;\\n  text-align: center;\\n  background-color: rgba(255 244 255 / 0.43);\\n  padding: 2rem;\\n  border-radius: 0.5rem;\\n  border: 1px solid #FFFFFF;\\n  box-shadow: 0.2rem 0.3rem 2rem rgba(0, 0, 0, 0.1);\\n  color: #555;\\n  a {\\n    color: #333;\\n    text-decoration: underline;\\n    margin-top: 1rem;\\n    display: block;\\n    font-size: 1.2rem;\\n  }\\n\\n}\\n</style>\\n\\n<script lang=\\"ts\\" setup>\\n\\nimport PostCard from \'~/PostCard.vue\'\\n\\nconst posts = ref([])\\n\\nonMounted(async () => {\\n  const resp = await fetch(`/api/posts`);\\n  posts.value = await resp.json();\\n})\\n\\n<\/script>\\n```\\n\\nFinally, create `PostCard.vue` component:\\n\\n```html title=\\"./seo/PostCard.vue\\"\\n<template>\\n  <div class=\\"post-card\\">\\n    <img v-if=\\"props.post.picture\\" :src=\\"props.post.picture\\" alt=\\"post image\\" />\\n    <h2>{{ props.post.title }}</h2>\\n    <div class=\\"content\\" v-html=\\"props.post.content\\"></div>\\n    <div class=\\"posted-at\\">\\n      <div>{{ formatDate(props.post.createdAt) }}</div>\\n      <div class=\\"author\\">\\n        <img :src=\\"props.post.author.avatar\\" alt=\\"author avatar\\" />\\n        <div>\\n          {{ props.post.author.publicName }}\\n        </div>\\n      </div>\\n    </div>\\n  </div>\\n</template>\\n\\n<script setup lang=\\"ts\\">\\n\\nconst props = defineProps<{\\n  post: {\\n    title: string\\n    content: string\\n    createdAt: string // iso date\\n    picture?: string\\n    author: {\\n      publicName: string\\n      avatar: string\\n    }\\n  }\\n}>()\\n\\n\\nfunction formatDate(date: string) {\\n  // format to format MMM DD, YYYY using Intl.DateTimeFormat\\n  return new Intl.DateTimeFormat(\'en-US\', {\\n    month: \'short\',\\n    day: \'2-digit\',\\n    year: \'numeric\'\\n  }).format(new Date(date))\\n}\\n<\/script>\\n\\n<style lang=\\"scss\\">\\n\\n.post-card {\\n  background-color: rgba(255 244 255 / 0.43);\\n  padding: 2rem;\\n  border-radius: 0.5rem;\\n  border: 1px solid #FFFFFF;\\n  box-shadow: 0.2rem 0.3rem 2rem rgba(0, 0, 0, 0.1);\\n  max-width: calc(100vw - 4rem);\\n  width: 600px;\\n  color: #333;\\n  line-height: 1.8rem;\\n\\n  >img {\\n    width: 100%;\\n    border-radius: 0.5rem;\\n    margin-bottom: 2rem;\\n  }\\n  \\n  h2 {\\n    margin: 0 0 2rem 0;\\n    font-size: 1.5rem;\\n  }\\n\\n  .content {\\n    margin-top: 1rem;\\n  }\\n\\n  .posted-at {\\n    margin-top: 1rem;\\n    font-size: 0.8rem;\\n    color: #666;\\n    display: flex;\\n    justify-content: space-between;\\n    align-items: center;\\n  }\\n\\n  .author {\\n    display: flex;\\n    align-items: center;\\n\\n    img {\\n      width: 2rem;\\n      height: 2rem;\\n      border-radius: 50%;\\n      margin-right: 0.5rem;\\n    }\\n    div {\\n      // flash wire dot line effect\\n      position: relative;\\n      overflow: hidden;\\n      border-radius: 1rem;\\n      padding: 0.2rem 0.5rem;\\n      font-size: 1rem;\\n      background: linear-gradient(90deg, rgb(0 21 255) 0%, rgb(0 0 0) 100%);\\n      background-size: 200% auto;\\n      background-clip: text;\\n      -webkit-background-clip: text;\\n      color: transparent; /* Hide the original text color */\\n      animation: shimmer 2s infinite;\\n      @keyframes shimmer {\\n        0% {\\n          background-position: -200% center;\\n        }\\n        100% {\\n          background-position: 200% center;\\n        }\\n      }\\n\\n    }\\n  }\\n\\n}\\n\\n</style>\\n```\\n\\nNow you can start your Nuxt project:\\n\\n```bash\\nnpm run dev\\n```\\n\\nAnd run `npm start` if you did not run it previously:\\n\\n```bash\\nnpm start\\n```\\n\\nOpen `http://localhost:3500` in your browser and you will see your blog with posts from admin panel:\\n\\n![alt text](localhost_3500_.png)\\n\\nGo to `http://localhost:3500/admin` to add new posts.\\n\\n## Step 7: Deploy\\n\\nWe will use Docker to make it easy to deploy with many ways. We will wrap both Node.js adminforth app and Nuxt.js app into single container for simplicity using supervisor. However you can split them into two containers and deploy them separately e.g. using docker compose. \\n\\nPlease note that in this demo example we routing requests to Nuxt.js app from AdminForth app using http-proxy. \\nWhile this will work fine, it might give slower serving then if you would route traffik using dedicated reverse proxies like traefik or nginx.\\n\\n\\n### Dockerize AdminForth and Nuxt in single container\\n\\n\\nOpen `Dockerfile` in root project directory (`ai-blog`) and put in the following content:\\n\\n```dockerfile title=\\"./Dockerfile\\"\\nFROM node:20-slim\\nEXPOSE 3500\\nWORKDIR /app\\nRUN apt-get update && apt-get install -y supervisor\\nCOPY package.json package-lock.json ./\\nRUN npm ci\\nCOPY seo/package.json seo/package-lock.json seo/\\nRUN cd seo && npm ci\\nCOPY . .\\n\\nRUN npx adminforth bundle\\nRUN cd seo && npm run build\\n\\nRUN cat > /etc/supervisord.conf <<EOF\\n[supervisord]\\nnodaemon=true\\n\\n[program:app]\\ncommand=npm run prod\\ndirectory=/app\\nautostart=true\\nautorestart=true\\nstdout_logfile=/dev/stdout\\nstderr_logfile=/dev/stderr\\nstdout_logfile_maxbytes = 0\\nstderr_logfile_maxbytes = 0\\n\\n[program:seo]\\ncommand=sh -c \\"cd seo && node .output/server/index.mjs\\"\\ndirectory=/app\\nautostart=true\\nautorestart=true\\nstdout_logfile=/dev/stdout\\nstderr_logfile=/dev/stderr\\nstdout_logfile_maxbytes = 0\\nstderr_logfile_maxbytes = 0\\n\\n[program:prisma]\\ncommand=npm run migrate:prod\\ndirectory=/app\\nautostart=true\\nstdout_logfile=/dev/stdout\\nstderr_logfile=/dev/stderr\\nstdout_logfile_maxbytes = 0\\nstderr_logfile_maxbytes = 0\\n\\nEOF\\n\\nCMD [\\"supervisord\\", \\"-c\\", \\"/etc/supervisord.conf\\"]\\n```\\n\\nOpen `.dockerignore` file in root project directory (`ai-blog`) and put in the following content:\\n\\n```bash title=\\".dockerignore\\"\\n.env\\nnode_modules\\nseo/node_modules\\n.git\\ndb\\n*.tar\\n.terraform*\\nterraform*\\n*.tf\\n```\\n\\n\\nBuild and run your docker container locally:\\n\\n```bash\\ndocker build -t my-ai-blog .\\ndocker run -p80:3500 -v ./prodDb:/app/db --env-file .env -it --name my-ai-blog -d my-ai-blog\\n```\\n\\nNow you can open `http://localhost` in your browser and see your blog.\\n\\n### Deploy to EC2 with terraform\\n\\n\\nFirst of all install Terraform as described here [terraform installation](https://developer.hashicorp.com/terraform/install#linux).\\n\\nIf you are on Ubuntu(WSL2 or native) you can use the following commands:\\n\\n```bash\\nwget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\\necho \\"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\\nsudo apt update && sudo apt install terraform\\n```\\n\\n\\nCreate special AWS credentials for deployemnts by going to `AWS console` -> `IAM` -> `Users` -> `Add user` (e.g. my-ai-blog-user) -> Attach existing policies directly -> `AdministratorAccess` -> Create user. Save `Access key ID` and `Secret access key` into `~/.aws/credentials` file:\\n\\nCreate or open file:\\n\\n```bash\\ncode ~/.aws/credentials\\n```\\n\\n```bash\\n...\\n\\n[myaws]\\naws_access_key_id = YOUR_ACCESS_KEY\\naws_secret_access_key = YOUR_SECRET\\n```\\n\\n\\nCreate file `main.tf` in root project directory:\\n\\n```hcl title=\\"./main.tf\\"\\nprovider \\"aws\\" {\\n  region = \\"eu-central-1\\"\\n  profile = \\"myaws\\"\\n}\\n\\ndata \\"aws_ami\\" \\"amazon_linux\\" {\\n  most_recent = true\\n  owners      = [\\"amazon\\"]\\n\\n  filter {\\n    name   = \\"name\\"\\n    values = [\\"amzn2-ami-hvm-*-x86_64-gp2\\"]\\n  }\\n}\\n\\ndata \\"aws_vpc\\" \\"default\\" {\\n  default = true\\n}\\n\\ndata \\"aws_subnet\\" \\"default_subnet\\" {\\n  filter {\\n    name   = \\"vpc-id\\"\\n    values = [data.aws_vpc.default.id]\\n  }\\n\\n  filter {\\n    name   = \\"default-for-az\\"\\n    values = [\\"true\\"]\\n  }\\n\\n  filter {\\n    name   = \\"availability-zone\\"\\n    values = [\\"eu-central-1a\\"]\\n  }\\n}\\n\\nresource \\"aws_security_group\\" \\"instance_sg\\" {\\n  name   = \\"my-aiblog-instance-sg\\"\\n  vpc_id = data.aws_vpc.default.id\\n\\n  ingress {\\n    description = \\"Allow HTTP\\"\\n    from_port   = 80\\n    to_port     = 80\\n    protocol    = \\"tcp\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n\\n  # SSH\\n  ingress {\\n    description = \\"Allow SSH\\"\\n    from_port   = 22\\n    to_port     = 22\\n    protocol    = \\"tcp\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n\\n  egress {\\n    description = \\"Allow all outbound traffic\\"\\n    from_port   = 0\\n    to_port     = 0\\n    protocol    = \\"-1\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n}\\n\\nresource \\"aws_key_pair\\" \\"deployer\\" {\\n  key_name   = \\"terraform-deployer-my-aiblog-key\\"\\n  public_key = file(\\"~/.ssh/id_rsa.pub\\") # Path to your public SSH key\\n}\\n\\n\\nresource \\"aws_instance\\" \\"docker_instance\\" {\\n  ami                    = data.aws_ami.amazon_linux.id\\n  instance_type          = \\"t3a.small\\"\\n  subnet_id              = data.aws_subnet.default_subnet.id\\n  vpc_security_group_ids = [aws_security_group.instance_sg.id]\\n  key_name               = aws_key_pair.deployer.key_name\\n\\n  # prevent accidental termination of ec2 instance and data loss\\n  # if you will need to recreate the instance still (not sure why it can be?), you will need to remove this block manually by next command:\\n  # > terraform taint aws_instance.app_instance\\n  lifecycle {\\n    prevent_destroy = true\\n    ignore_changes = [ami]\\n  }\\n\\n  user_data = <<-EOF\\n    #!/bin/bash\\n    yum update -y\\n    amazon-linux-extras install docker -y\\n    systemctl start docker\\n    systemctl enable docker\\n    usermod -a -G docker ec2-user\\n\\n    echo \\"done\\" > /home/ec2-user/user-data-done\\n  EOF\\n\\n  tags = {\\n    Name = \\"my-ai-blog-instance\\"\\n  }\\n}\\n\\nresource \\"null_resource\\" \\"wait_for_user_data\\" {\\n  provisioner \\"remote-exec\\" {\\n    inline = [\\n      \\"echo \'Waiting for EC2 software install to finish...\'\\",\\n      \\"while [ ! -f /home/ec2-user/user-data-done ]; do sleep 2; done\\",\\n      \\"echo \'EC2 software install finished.\'\\"\\n    ]\\n\\n    connection {\\n      type        = \\"ssh\\"\\n      user        = \\"ec2-user\\"\\n      private_key = file(\\"~/.ssh/id_rsa\\")\\n      host        = aws_instance.docker_instance.public_ip\\n    }\\n  }\\n\\n  depends_on = [aws_instance.docker_instance]\\n}\\n\\n\\nresource \\"null_resource\\" \\"build_image\\" {\\n  provisioner \\"local-exec\\" {\\n    command = \\"docker build -t blogapp . && docker save blogapp:latest -o blogapp_image.tar\\"\\n  }\\n  triggers = {\\n    always_run = timestamp() # Force re-run if necessary\\n  }\\n}\\n\\nresource \\"null_resource\\" \\"remote_commands\\" {\\n  depends_on = [null_resource.wait_for_user_data, null_resource.build_image]\\n\\n  triggers = {\\n    always_run = timestamp()\\n  }\\n\\n\\n  provisioner \\"file\\" {\\n    source      = \\"${path.module}/blogapp_image.tar\\"\\n    destination = \\"/home/ec2-user/blogapp_image.tar\\"\\n    \\n    connection {\\n      type        = \\"ssh\\"\\n      user        = \\"ec2-user\\"\\n      private_key = file(\\"~/.ssh/id_rsa\\")\\n      host        = aws_instance.docker_instance.public_ip\\n    }\\n  }\\n\\n  provisioner \\"file\\" {\\n    source      = \\"${path.module}/.env\\"\\n    destination = \\"/home/ec2-user/.env\\"\\n    \\n    connection {\\n      type        = \\"ssh\\"\\n      user        = \\"ec2-user\\"\\n      private_key = file(\\"~/.ssh/id_rsa\\")\\n      host        = aws_instance.docker_instance.public_ip\\n    }\\n  }\\n\\n  provisioner \\"remote-exec\\" {\\n    inline = [\\n      \\"sudo docker system prune -af\\",\\n      \\"docker load -i /home/ec2-user/blogapp_image.tar\\",\\n      \\"sudo docker rm -f blogapp || true\\",\\n      \\"sudo docker run --env-file .env -d -p 80:3500 --name blogapp -v /home/ec2-user/db:/app/db blogapp\\"\\n    ]\\n\\n    connection {\\n      type        = \\"ssh\\"\\n      user        = \\"ec2-user\\"\\n      private_key = file(\\"~/.ssh/id_rsa\\")\\n      host        = aws_instance.docker_instance.public_ip\\n    }\\n  }\\n\\n  \\n}\\n\\noutput \\"instance_public_ip\\" {\\n  value = aws_instance.docker_instance.public_ip\\n}\\n\\n```\\n\\n\\nNow you can deploy your app to AWS EC2:\\n\\n```bash\\nterraform init\\nterraform apply -auto-approve\\n```\\n\\n> \u261d\ufe0f To destroy and  stop billing run `terraform destroy -auto-approve`\\n\\n> \u261d\ufe0f To check logs run `ssh -i ~/.ssh/id_rsa ec2-user@$(terraform output instance_public_ip)`, then `sudo docker logs -n100 -f aiblog`\\n\\nTerraform config will build the Docker image locally and then copy it to the EC2 instance. This approach saves build resources (CPU/RAM) on the EC2 instance, though it increases network traffic (the image might be around 200MB). If you prefer to build the image directly on the EC2 instance, you can slightly adjust the configuration: remove `null_resource.build_image` and modify `null_resource.remote_commands` to perform the build remotely. However, note that building the image on a `t3.small` instance may still consume significant resources and can interfere with running applications. To avoid potential downtime or performance issues, building the image locally remains the recommended approach.\\n\\n\\n### Add HTTPs and CDN\\n\\nFor adding HTTPS and CDN you will use free Cloudflare service (though you can use paid AWS Cloudfront or any different way e.g. add Traefik and Let\'s Encrypt). Go to https://cloudflare.com and create an account. Add your domain and follow instructions to change your domain nameservers to Cloudflare ones.\\n\\nGo to your domain settings and add A record with your server IP address, which was shown in output of `terraform apply` command.\\n\\n```\\nType: A\\nName: blog\\nValue: x.y.z.w\\nCloudflare proxy: orange (enabled)\\n```\\n\\n![alt text](image.png)\\n\\n\\n## Useful links\\n\\n* [Full source code of the project](https://github.com/devforth/adminforth-example-ai-blog)\\n* [Live demo of AI BLog](https://blog-demo.adminforth.dev/admin/resource/post)\\n* [AdminForth documentation](https://adminforth.dev/docs/tutorial/gettingStarted/)\\n* [AdminForth GitHub](https://github.com/devforth/adminforth)\\n* [Nuxt.js documentation](https://nuxt.com/docs/getting-started/introduction)"},{"id":"chatgpt-plugin","metadata":{"permalink":"/blog/chatgpt-plugin","source":"@site/blog/2024-08-05-chatgpt/index.md","title":"Chat-GPT plugin to co-write texts and strings","description":"Couple of days ago we released a plugin which allows you to co-write texts and strings with the AI.","date":"2024-08-05T00:00:00.000Z","tags":[{"inline":false,"label":"ChatGPT","permalink":"/blog/tags/chatgpt","description":"ChatGPT is a conversational AI model that can generate human-like responses to text inputs."},{"inline":false,"label":"Plugin","permalink":"/blog/tags/plugin","description":"Plugins are extensions that add new features or functionality to a AdminForth."}],"readingTime":3.78,"hasTruncateMarker":true,"authors":[{"name":"Ivan Borshchov","title":"Maintainer of AdminForth","url":"https://github.com/ivictbor","imageURL":"https://avatars.githubusercontent.com/u/1838656?v=4","key":"ivanb","page":null}],"frontMatter":{"slug":"chatgpt-plugin","title":"Chat-GPT plugin to co-write texts and strings","authors":"ivanb","tags":["chatgpt","plugin"]},"unlisted":false,"prevItem":{"title":"Build AI-Assisted blog with AdminForth and Nuxt in 20 minutes","permalink":"/blog/ai-blog"}},"content":"Couple of days ago we released a plugin which allows you to co-write texts and strings with the AI. \\n\\nToday LLM is already a must tool to speed-up writing, brainstorming, or generating ideas.\\n\\nHere is how it looks in action:\\n\\n![alt text](../../docs/tutorial/07-Plugins/demoChatGpt.gif)\\n\\n\x3c!-- truncate --\x3e\\n\\n# Simple controls\\n\\nTo control plugin we use our open-source [vue-suggestion-input](https://github.com/devforth/vue-suggestion-input).\\nIt allows to:\\n* Complete suggestion with `Tab`.\\n* Complete word with `Ctrl + Right`.\\n* Regenerate suggestion with `Ctrl + Down`.\\n* On mobile suggestion word is accepted with swipe right on the screen.\\n\\n## Want to try it out?\\n\\nGo to a [Live Demo](https://demo.adminforth.dev/resource/aparts/create) and start creating a new apartment record. Type in the `title` and `description` field and see how the plugin works.\\n\\nIf you want to try it out on your hello-wrold admin panel, then, first follow the instructions in the [Getting Started](/docs/tutorial/gettingStarted) tutorial to create a new project. To install the plugin, then, follow the instructions in the [Chat-GPT plugin page](/docs/tutorial/Plugins/text-complete).\\n\\n\\n## Context matters, but with sane limit!\\n\\nWhen the prompts are called, the plugin passes to LLM not only previous text in current field to complete, but also passes values of other fields in record edited. This allows to generate more relevant completions. \\nFor example if you have a record with fields `title` and `description`, and you are editing `description`, the plugin will pass `title` value to LLM as well.\\n\\nBut obviously longer prompts lead to higher LLM costs and longer response times. That is why we created mechanics to limit the length of prompts passed to LLM.\\n\\nLimitation is done on 2 levels:\\n- plugin settings have `expert.promptInputLimit` - limits length of edited field passed to LLM. If field is longer, it will pass only last `promptInputLimit` characters.\\n- plugin settings have `expert.recordContext` which defines limits for other fields in record. Each field can\'t be longer then `maxFieldLength` (default is 300). If field is longer then it is split to parts `splitParts` and they are joined with \'...\'. Also if there are more non-empty fields then `maxFields`, then plugin selects top longest `maxFields` fields to pass to LLM.\\n\\nIn the end, total number of characters passed to LLM is limited by formula:\\n\\n```\\npromptInputLimit + maxFields * maxFieldLength + <LLM request static part>\\n```\\n\\nWhere `<LLM request static part>` is a constant part of request to LLM which looks like this:\\n\\n```\\nContinue writing for text/string field \\"${this.options.fieldName}\\" in the table \\"${resLabel}\\"\\\\n\\nRecord has values for the context: ${inputContext}\\\\n\\nCurrent field value: ${currentVal}\\\\n\\nDon\'t talk to me. Just write text. No quotes. Don\'t repeat current field value, just write completion\\\\n\\n```\\n\\n## Model configuration\\n\\nOf course you can define which model to use for completion. By default plugin uses `gpt-4o-mini` model ($0.150 / 1M input tokens, $0.600 / 1M output tokens for Aug 2024). But you can change it to any other model available in OpenAI API. More powerful replacement is `gpt-4o` model ($5.00 / 1M input tokens, $15.00 / 1M output tokens for Aug 2024).\\n\\nAlso you can define other parameters for completion like:\\n- `maxTokens` - most likely you don\'t want to waste tokens on longer completions, so default is 50 tokens.\\n- `temperature` - model temperature, default is 0.7. You can increase it to get more creative completions (but with risk of getting nonsense). Or decrease it to get more conservative completions.\\n- `debounceTime` - debounce time in milliseconds, default is 300. After typing each character, plugin waits for `debounceTime` milliseconds before sending request to LLM. If new character is typed during this time, then timer is reset. This is done to prevent sending too many requests to LLM.\\n\\n## Frontend story\\n\\nWhen we were working on plugin, we wanted to make it as user-friendly as possible. \\n\\nMost frontend packages for completion have old-fashioned dropdowns, which are not very convenient to use.\\n\\nWe wanted to have something very similar to Copilot or Google doc. So we created our own package [vue-suggestion-input](https://github.com/devforth/vue-suggestion-input). It is also MIT and open-source so you can use it in your projects as well.\\n\\nUnder the hood vue-suggestion-input uses [quill](https://quilljs.com/) editor. Quill is one of the WYSIWYG editors which have really good\\nAPI to work with DOM inside of editor. Basically all pieces of content in editor are represented as so called blots. And best thing - you can create your own custom blot. So we created our own blot which is responsible for rendering completion suggestion. Then you just \\"dance\\" around positioning of selection, suggestion and text modification, and thees things are easy-peasy with quill API."}]}}')}}]);